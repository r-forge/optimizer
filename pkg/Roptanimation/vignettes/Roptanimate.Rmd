---
title: "Animating geometric optimization: small polygons"
author: "John C. Nash, Greg Snow"
date: "`r Sys.Date()`"
## output: pdf_document
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Animating geometric optimization: small polygons}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Abstract

**Roptanimation** is an experimental R package to display the progress of 
geometric animations. A classic example is the **largest small polygon** where
we aim to maximize the area of the polygon subject to the constraint that no
two vertices are separated by more than one unit of distance. This article discusses 
the problem and how the animation is created, but only shows snapshots of the 
animation.


## TODOS

  - make sure animation is running
  - sort out what is going on with tkrplot
  - comment and test the animation code
  - try nloptr
  - try other optimizers
  - try shiny for running the animation
  - Do we want to make VMPOLY files available?)

## Background

The **The Largest Small Hexagon** is the title of a paper by
Ron Graham (J. Combinatorial Theory (A), vol. 18, pp. 165-170, 1975). This did
not introduce this problem, but served to bring it to wider attention. One of
the authors (JCN) used this problem to illustrate constrained optimization using
the tools in J.C. Nash and M. Walker-Smith (1987, Nonlinear parameter estimation:
an integrated system in BASIC, New York: Marcel Dekker, now available online at 
https://archive.org/details/ost-engineering-jnmws2004).

To provide a visual presentation of the optimization, Nash coded a display for the
IBM PC family of MS DOS computers running GWBASIC. 
In May 2016, the discovery that
files for this example could still execute, albeit clumsily, under Linux using DosBox or 
PCBasic raised the possibility of bringing them up to date. R was a logical choice for such an 
implementation, given that the authors and many others work with this software system. 

## Parametrization of the polygon

For a polygon with `nv` vertices, we have `2*nv` cartesian (i.e., x, y)
coordinates. However, use of cartesian coordinates as parameters for this problem 
leads to a very complicated specification, since 3 parameters can be fixed right
away. That is, we can arbitrarily fix vertex 1 at the `(0, 0)` or origin of our [2D] space.
Moreover, we can put the second vertex at `(b[1], 0)` where `b` is a vector of
`(2*nv - 3)` parameters. Changing to a representation that uses a radius from the
origin for vertex `L` equal to `b[L-1]`, we could use the angle of this vertex from the
positive x axis as a parameter. Call this angle `alpha[L]`. Clearly `alpha[1]` for 
vertex 2 is 0, so the 2nd vertex is still at `(b[1], 0)`.  Note that there is no `b[0]`, 
which would be the distance of vertex 1 from itself. 

We could put the alpha angles in the parameter vector as `b[L+lshift]` where 
`lshift = nv - 3`. Thus the first non-zero angle is for vertex 3 and is parameter
`3 + nv - 3 = nv`. Check: there are `(nv - 1)` radius parameters, so the first angle
parameter is in position `nv`. 

There may be good implementations based on having
parameters `b[nv] ... b[2*nv-3]` equal to the angles for vertices `3...nv` from
the x axis. However, 
that then requires the angles to be monotonically increasing. By specifying that
  `b[L+lshift] = alpha[L-1] - alpha[L-2]` for `L=3 ... nv`, and noting that 
  `alpha[1]=0`
we automatically get the
angles alpha monotonic by imposing a lower bound of 0 on the parameters `b`. 

Note that the radii already cannot be negative (in fact, zero is a bad idea too), so a lower
bound of 0 can be applied to all the parameters `b`. An upper bound of 1 clearly applies to
the first `(nv - 1)` parameters. The other `(nv - 2)` parameters are angles in radians. If
we are to have the polygon in the positive y half-space in cartesian coordinates, 
then pi is an obvious (and likely conservative) bound on these angles. In fact,
`pi` is a bound on their sum.

We make no assertion that this is the only or best parametrization of this problem, and
welcome suggestions for other ways to prepare the optimization.

## Problem setup

The above parametrization is implemented in the function `polysetup(nv, defsize)`,
where `defsize` is the default "size" of a regular polygon for which initial parameters
are established. Generally we will begin our optimization with a polygon for which the 
size is smaller than 1. This ensures that our initial polygon is feasible, and some 
optimization methods such as `nmkb` require that. Note that for drawing the polygon, it is useful to think of a
vertex `L+1` which is at the same position as vertex 1.

```{r cache=FALSE, echo=FALSE}
library(knitr) # This is critical -- or you don't get read_chunk()
read_chunk('/home/john/rsvnall/optimizer/pkg/Roptanimation/R/smallpoly.R')
```


```{r polysetup}
```

The parameters of a regular hexagon of size 1 can be created as follows.

```{r}
# A regular hexagon of size 1
reghex1 <- polysetup(6, defsize=1)
cat("Parameters of the regular hexagon of unit size\n")
print(reghex1$par0)
```

## The polygon area

The parameterization of the problem allows the area to be computed as the sum of the areas of
the triangles made up from vertices `1`, `L` and `(L+1)`
where `L` runs from `2` to `(nv-1)`. That is, there are `nv - 2` triangles. For the 
hexagon, the 4 triangles are made up of the vertices (1   2   3), (1   3   4),
(1   4   5), and (1   5   6).


```{r polyarea}
```

For reference, let us compute the area of this hexagon.

```{r}
reg1area <- polyarea(reghex1$par)
cat("Reference area of regular hexagon of unit size=",reg1area,"\n")
```

This is in accord with R.L. Graham's paper (J. Combinatorial Theory (A), vol. 18, pp. 165-170, 1975).


## Conversion of radial to cartesian coordinates

For drawing the current polygon, we need cartesian coordinates rather than the specially 
organized radial coordinates defined by the optimization parameters. The R function
`polypar2XY` carries out this computation and puts the `x, y` coordinates
in a two-vector list `XY`. `XY\$x` gives the `x` coordinates and 
`XY\$y` gives the `y` coordinates. To simplify the plotting of the polygon
the first and last values of each list are both 0 so that a graph with joining lines 
automatically gives the closed figure polygon.

```{r polypar2XY}
```

## Distance between polygon vertices

To verify constraints and to construct penalty or barrier functions for the optimization
process for this problem, we also need vertex to vertex distances. These are computed by 
the function `polydistXY`. This function uses the cartesian coordinates for the 
current polygon that result from running the function `polypar2XY`

```{r polydistXY}
```

## Vertex distances from radial parameters

We could compute these vertex distances from the radial parameters of the polygon.
The following function calls the conversion from radial to cartesian coordinates,
then computes the distances.

```{r polypar2distXY}
```

Alternatively, and perhaps more efficiently or at least more elegantly, we can do a
one-step calculation. However, the following function ONLY computes the non-radial
inter-vertex differences. The first `nv - 1` parameters where `nv` is the number of 
vertices give the other distances. Moreover, the positions of these distances in 
the output of polypar2distXY are not obvious at first glance.

```{r polypardist2}
```

## Testing functions.

Note that we tested our functions to create the original polygon and compute its area. 
This is a step that we recommend. In fact, one of us (JN) refuses to look at user queries
about his optimization routines unless there is evidence that objective functions and gradients
have been checked. It is an important part of the solution of EVERY optimization problem
that users verify that they are solving the intended problem. Moreover, even in our own work,
the simple checks often reveal silly but critical errors.

An example of a test script follows.

```{r polyex0, fig.width=5, fig.height=5}
```

## Setup of the optimization

The constrained optimization to maximize the area actually minimizes the negative area.
However, we need to account for the constraints. Clearly since the radial parameters
start at one vertex of the polygon, they are bounded above by 1. And naturally, we 
cannot have a polygon with negative lengths, so 0 is an obvious lower bound, though
realistically, some modest positive value would likely be workable. This accounts for
constraints on distance from the first, or base, vertex. For the other distances, we 
will apply a penalty function which will be added to the negative area. We can also
put 0 as a lower bound on the angular parameters, and a reasonable upper bound as well.
pi serves as a conservative bound for these parameters.

## Quadratic penalty function

Our first try (which we will state in advance does not work well) is to add a multiple
of the sum of the distance violations. These are the pairwise squared distances for those
inter-vertex distances that are not given by the radial parameters. We assume the simple
bounds are in force for the radial and angular parameters. This results in the following
objective function.

```{r polyobjq}
```

Setting the penalty factor (`penfactor`) at 100, we use M. J. Powell's bobyqa minimizer
to try to find the solution. 

```{r polyexq}
```
The objective is the negative area PLUS the penalty, so (-1) times this value is a lower 
bound on the area. But we see that it is smaller than the reference value (approximately
0.64952) of the unit regular hexagon. We also display the computed area directly, and it
shows that the constraint penalty is not appreciably contributing to the objective.
We clearly have more work to do.

## Making objective very large on constraint violation

An old "trick" in optimization is to use a non-gradient direct search method that 
assigns the objective function its correct value when the parameters are feasible 
and a very large value when they are violated. We supply the value of `bignum` 
to the objective function via the call.

```{r polyobjbig}
```

To test several optimizers at once, we use the `opm()} function of the
R-forge package `optimrx`. 

```{r polyexbig}
```

The two Nelder-Mead inspired codes are the best of a bad lot here, though neither has
got really close to the solution for the small hexagon problem. Note how different the
solutions appear. Let us draw them.

```{r polyexbigplot, fig.height=6, fig.width=6}
```


Drawing these polygons so we can visually compare them involves transformations that
must align the polygons so one edge is on the x axis. But which edge to choose as the
first? Then we must note that a vertical reflection of the polygon about the mid-point
of the chosen edge will result in an equivalent solution. These options remind us that
the optimization problem has multiple solutions, all with equal optimal area, which
is part of the difficulty of the largest small polygon problem.

## Logarithmic barrier constraint

A different kind of penalty is provided by the logarithmic barrier. This aims to keep
the parameters feasible by adding a steeply increasing function to the objective (the 
negative area) as 
the constraint is approached. Let us first define the **slack** in a distance constraint
as `slack = (1 - squared.distance)`. We could use the distance itself, but might as well 
avoid the extra computation. As each slack goes to zero, then `- log(slack)` goes to 
infinity. We can scale this, as in the quadratic penalty, with `penfactor`, but the
actual numerical value will be much smaller now because `- log()` increases much more
quickly than the quadratic.

There is an annoying computational practicality that some optimization methods may take
steps in the parameter vector that push some distances into infeasible territory. This
will cause exceptions to be generated when the logarithm of a negative "slack" is 
attempted. To avoid this, we will simply make the objective function very large at
any time when there is a violation. However, this does cause grief for the evaluation
of numerical approximations to gradients, so we may want to revise this policy later
and seek more elegant (but likely more complicated) techniques to deal with this 
possibility. The large number for now will be ` bignum = 1e20 `.

Since the logarithmic barrier does not let the parameters end actually ON then bound,
we may wish to move the constraint an epsilon beyond 1 by redefining the slack as

``slack = (1 + epsilon - squared.distance)``

But what should `epsilon` be? We may revisit this later, but for the moment set the 
value at 0, and the resulting code is as follows.

```{r polyobj}
```

With Powell's bobyqa, we attempt to minimize this objective from the same start as
before. We set the penalty factor quite small, in fact 0.01, as the barrier is non-zero within
the feasible region.

```{r polyex2}
```

This is not too bad. Possibly another method could do better. We can adjust the objective 
so the radial parameters are constrained using a logarithmic barrier, then use an unconstrained 
optimization method. 

```{r polyobju}
```

And here is the gradient code. Note that we need to be careful about the indexing. We don't show
it here, but we did perform a quick check with package `numDeriv` that the gradient is 
correctly computed for the starting vector `x0`.

```{r polygradu}
```

There are a number of unconstrained optimization methods, and the `optimrx` package lets
us try them out all at once.

```{r polyexuall}        
```

We note that the only bounds constrained minimizer in `R`'s base function `optim()` is
that invoked by the method "L-BFGS-B", and this has done rather poorly. In fact, it does not even find
the regular hexagon above. Possibly we should use the bounds constraints directly, but we can first 
supply gradients.

```{r polygrad}
```


Let us try all the bounds-constrained methods in `opm` from package `optimrx`.

```{r polyexallb}        
```

The Limited Memory BFGS codes and the Truncated Newton -- at least in the variants used here -- have 
failed completely with this objective function. All have suffered some catastrophe in an internal 
computation, but we have not yet had time to see if a workaround is possible. 

On the other hand, the all-R routines Rcgmin and Rvmmin have done rather well. 
Graham reports largest area is approximately 0.674981. We are close. Can we reduce the 
penalty factor steadily and find a good approximation to the optimal area?

## Issues

- polyex3g shows that it is difficult to stay in feasible zone with gradients. We need to stabilize this calculation.
- Can we use a transfinite function approach to this problem? This transforms the range (a, b) into (-Inf, Inf) using an inverse hyperbolic tangent. However, this can destabilize the optimization.

## Animating the progress to an optimum

If we want to visualize the progress of our optimization, then we need to draw the polygons as the
relevant parameter vectors are tried. However, we probably only want to draw the polygon when we have 
found a feasible one that increases the area. Furthermore, as we progress, it is helpful to 
indicate the rank of the polygons in order of area. This can be accomplished by "fading out"
polygons that are already drawn, though that means keeping a record of all the feasible, larger
area polygons. 

We can achieve all these desiderata by means of the following function.

```{r PolyTrack}
```

Calling the PolyTrack function creates a new environment to hold all the sets of points. Each
time a new "better" polygon is found, all the polygons are redrawn using the vector of (fading)
colours. 

```{r drawpoly1, eval=FALSE}
```

As of Nov 10, 2016, replay of the stored points with `tkrplot` does not seem to work reliably. 

## Saved results

Because the code actually builds a list of the points where the polygon is "better", we can
play back the progress. There are tools in the `tkrplot` package that allow this to be
made interactive. Thus, when the optimization terminates, we can redraw the polygons at will using the 
following code.

```{r redrawpoly1, eval=FALSE}
```

## Acknowledgement

Email discussions with Prof. S. G. Nash of George Mason University have been helpful in 
developing some of the ideas in this article.
