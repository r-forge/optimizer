\documentclass{article}

%\VignetteIndexEntry{nlmrt Tutorial}
%\VignetteDepends{}
%\VignetteKeywords{nonlinear least squares, Levenberg-Marquardt method}
%\VignettePackage{nlmrt}

%% from Ross Ihaka doc.

\newcommand{\R}{{\sf R\ }}
\newcommand{\Splus}{{\sf S-PLUS}}
\newcommand{\fixme}[1]{\textbf{FIXME: #1}}
%\newcommand{\fixme}[1]{}
\newcommand{\code}[1]{{\tt #1}}
\title{nlmrt-vignette}
\author{John C. Nash}
\usepackage{Sweave}
\usepackage{fancyvrb}
\usepackage{chicago}

\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=1em}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=1em}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=1em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}
 
%%% \DefineVerbatimEnvironment{Soutput}{Verbatim}{fontsize=\small,fontshape=n}

\begin{document}
\maketitle

\section*{Background}

This vignette discusses the 
\R package \code{nlmrt}, that aims to provide computationally robust
tools for nonlinear least squares problems. Note that \R already has the
\code{nls()} function to solve nonlinear least squares problems, and this
function has a large repertoire of tools for such problems. However, it is
specifically NOT indicated for problems where the residuals are small or
zero. Furthermore, it frequently fails to find a solution if starting 
parameters are provided that are not close enough to a solution. The tools
of \code{nlmrt} are very much intended to cope with both these issues.

\code{nlmrt} tools generally do not return the large nls-style object.
However, we do provide a tool \code{wrapnls} that will run either
\code{nlxb} followed by a call to \code{nls}. The call to \code{nls} is
adjusted to use the \code{port} algorithm if there are bounds constraints.


\section{An example problem and its solution}

Let us try an example initially presented by \cite{Ratkowsky83} and 
developed by \cite{Huet1996}. This is a model for the regrowth of pasture.
We set up the computation by putting the data for the problem in a data
frame, and specifying the formula for the model. This can be as a formula
object, but I have found that saving it as a character string seems to give
fewer difficulties. Note the "~" that implies "is modeled by". There must
be such an element in the formula for this package (and for \code{nls()}).
We also specify two sets of starting parameters, that is, the \code{ones}
which is a trivial (but possibly unsuitable) start with all parameters 
set to 1, and \code{huetstart} which was suggested in \cite{Huet1996}.
Finally we load the routines in the package \code{nlmrt}.


%%Chunk01, 
<<chunk01, echo=TRUE>>=
options(width=60)
pastured <- data.frame(
time=c(9, 14, 21, 28, 42, 57, 63, 70, 79),
yield= c(8.93, 10.8, 18.59, 22.33, 39.35, 
         56.11, 61.73, 64.62, 67.08))
regmod<-"yield ~ t1 - t2*exp(-exp(t3+t4*log(time)))"
ones<-c(t1=1, t2=1, t3=1, t4=1) # all ones start
huetstart<-c(t1=70, t2=60, t3=0, t4=1)
require(nlmrt)
@

Let us now call the routine \code{nlsmnqb} (even though we are not 
specifying bounds). We try both starts.

<<chunk02, echo=TRUE>>=
anmrt<-nlxb(regmod, start=ones, trace=FALSE, data=pastured)
print(anmrt)
@

<<chunk03, echo=TRUE>>=
anmrtx<-try(nlxb(regmod, start=huetstart, trace=FALSE, data=pastured))
print(strwrap(anmrtx))
@

Note that the standard \code{nls()} of \R fails to find a solution 
from either start.

\RecustomVerbatimEnvironment{Soutput}{Verbatim}{fontsize=\scriptsize}

<<chunk04, echo=TRUE>>=
anls<-try(nls(regmod, start=ones, trace=FALSE, data=pastured))
print(strwrap(anls))
@


<<chunk05, echo=TRUE>>=
anlsx<-try(nls(regmod, start=huetstart, trace=FALSE, data=pastured))
print(strwrap(anlsx))
@

In both cases, the \code{nls()} failed with a 'singular gradient'. 
This implies the Jacobian is effectively singular at some point. The
Levenberg-Marquardt stabilization used in \code{nlxb} avoids this 
particular issue by augmenting the Jacobian until it is non-singular.
The details of this common approach may be found elsewhere \cite{cnm79}.
?? Do we want a page ref?

There are some other tools for \R that aim to solve nonlinear least 
squares problems. We have not yet been able to successfully use the INRA package 
\code{nls2}. This 
is a quite complicated package and is not installable as a regular \R package 
using \code{install.packages()}. Note that there is a very different package 
by the same name on CRAN by Gabor Grothendieck. 

\section{The \code{nls} solution}

We can call \code{nls} after getting a potential nonlinear least squares
solution using \code{nlxb}. Package \code{nlmrt} has function \code{wrapnls} 
to allow this to be carried out automatically. Thus,

<<chunk06, echo=TRUE>>=
awnls<-wrapnls(regmod, start=ones, data=pastured)
print(awnls)
cat("Note that the above is just the nls() summary result.\n")
@


\section{Problems specified by residual functions}

The model expressions in \R, such as 

\code{yield $\sim$ t1 - t2*exp(-exp(t3+t4*log(time)))}

are an extremely helpful feature of the language. Moreover, they are used
to compute symbolic or automatic derivatives, so we do not have to rely on 
numerical approximations for the Jacobian of the nonlinar least squares 
problem. However, there are many situations where the expression structure
is not flexible enough to allow us to define our residuals, or where the
construction of the residuals is simply too complicated. In such cases it
is helpful to have tools that work with \R functions. 

Once we have an \R function for the residuals, we can use the safeguarded 
Marquardt routine \code{nlfb} from package \code{nlmrt} or else the 
routine \code{nls.lm} from package \code{minpack.lm} \cite{minpacklm12}. The latter is 
built on the Minpack Fortran codes of \cite{more80} implemented by Kate
Mullen. \code{nlfb} is written entirely in \R, and is intended to be 
quite aggessive in ensuring it finds a good minimum. Thus these two approaches
have somewhat different characteristics.

Let us consider a slightly different problem, called WEEDS. Here the objective
is to model a set of 12 data points (density $y$ of weeds at annual time points $tt$)
versus the time index. (A minor note: use of \code{t} rather than \code{tt} in \R
may encourage confusion with the transpose function \code{t()}, so I tend to 
avoid plain \code{t}.) The model suggested was a 3-parameter logistic function,

$  y_{model}  =  b_1/(1 + b_2 exp(-b_3 tt) ) $

and while it is possible to use this formulation, a scaled version gives slightly
better results

$  y_{model} =  100  b_1/(1 + 10 b_2 exp(-0.1 b_3 tt) ) $

The residuals for this latter model (in form "model" minus "data") are coded in 
\R in the following code chunk in the function \code{shobbs.res}. We have also
coded the Jacobian for this model as \code{shobbs.jac}

<<chunk07, echo=TRUE>>=
shobbs.res<-function(x){ # scaled Hobbs weeds problem -- residual
# This variant uses looping
    if(length(x) != 3) stop("hobbs.res -- parameter vector n!=3")
    y<-c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443, 38.558, 50.156, 62.948,
         75.995, 91.972)
    tt<-1:12
    res<-100.0*x[1]/(1+x[2]*10.*exp(-0.1*x[3]*tt)) - y
}
 
shobbs.jac<-function(x) { # scaled Hobbs weeds problem -- Jacobian
    jj<-matrix(0.0, 12, 3)
    tt<-1:12
    yy<-exp(-0.1*x[3]*tt) # We don't need data for the Jacobian
    zz<-100.0/(1+10.*x[2]*yy)
    jj[tt,1] <- zz
    jj[tt,2] <- -0.1*x[1]*zz*zz*yy
    jj[tt,3] <- 0.01*x[1]*zz*zz*yy*x[2]*tt
    return(jj)
}
@

With package \code{nlmrt}, function \code{nlfb} can be used to estimate the 
parameters of the WEEDS problem as follows, where we use the naive starting 
point where all parameters are 1.

<<chunk08, echo=TRUE>>=
st<-c(b1=1, b2=1, b3=1)
ans1<-nlfb(st, shobbs.res, shobbs.jac, trace=FALSE)
print(ans1)
@

This works very well, with almost identical iterates as given by \code{nlxb}.
(Since the algorithms are the same, this should be the case.) Note that we 
turn off the \code{trace} output. There is also the possibility of interrupting
the iterations to \code{watch} the progress. Changing the value of \code{watch} in 
the call to \code{nlfb} below allows this. In this code chunk, we use an internal
numerical approximation to the Jacobian. 

<<chunk09, echo=TRUE>>=
cat("No jacobian function -- use internal approximation\n")
ans1n<-nlfb(st, shobbs.res, trace=FALSE, control=list(watch=FALSE)) # NO jacfn
print(ans1n)
@

Note that we could also form the sum of squares function and the gradient and
use a function minimization code. The next code block shows how this is done,
creating the sum of squares function and its gradient, then using the \code{optimx}
package to call a number of minimizers simultaneously.

<<chunk10, echo=TRUE>>=
shobbs.f<-function(x){
   res<-shobbs.res(x)
   as.numeric(crossprod(res))
}
shobbs.g<-function(x){
   res<-shobbs.res(x) # This is NOT efficient -- we generally have res already calculated
   JJ<-shobbs.jac(x)
   2.0*as.vector(crossprod(JJ,res))
}
require(optimx)
aopx<-optimx(st, shobbs.f, shobbs.g, control=list(all.methods=TRUE))
optansout(aopx, NULL) # no file output
cat("\nNow with numerical gradient approximation or derivative free methods\n")
aopxn<-optimx(st, shobbs.f, control=list(all.methods=TRUE))
optansout(aopxn, NULL) # no file output
@

We see that most of the minimizers work with either the analytic or approximated
gradient. The 'CG' option of function \code{optim()} does not do very well in 
either case. As the author of the original step and description and then Turbo
Pascal code, I can say I was never very happy with this method and replaced it
recently with \code{Rcgmin} from the package of the same name, in the process
adding the possibility of bounds or masks constraints.

\section{Converting an expression to a function}

Clearly if we have an expression, it would be nice to be able to automatically
convert this to a function, if possible also getting the derivatives. Indeed,
it is possible to convert an expression to a function, and there are
several ways to do this (references??). In package \code{nlmrt} we 
provide the tools 
\code{model2grfun.R}, \code{model2jacfun.R}, \code{model2resfun.R}, 
and \code{model2ssfun.R} to convert a model expression to a function to
compute the gradient, Jacobian, residuals or sum of squares functions respectively.
We do not provide any tool for converting a function for the residuals back
to an expression, as functions can use structures that are not easily expressed
as \R expressions. 

Below are code chunks to illustrate the generation of the residual, sum of squares,
Jacobian and gradient code for the Ratkowsky problem used earlier in the 
vignette. The commented-out first line shows how we would use one of these
function generators to output the function to a file named "testresfn.R". 
However, it is not necessary to generate the file.

First, let us generate the residuals. We must supply the names of the parameters,
and do this via the starting vector of parameters \code{ones}. The actual
values are not needed by \code{model2resfun}, just the names. Other names
are drawn from the variables used in the model expression \code{regmod}.

<<chunk12, echo=TRUE>>=
# jres<-model2resfun(regmod, ones, funname="myxres", file="testresfn.R")
jres<-model2resfun(regmod, ones)
print(jres)
valjres<-jres(ones, yield=pastured$yield, time=pastured$time)
cat("valjres:")
print(valjres)
@

Now let us also generate the Jacobian and test it using the numerical
approximations from package \code{numDeriv}. 

<<chunk13, echo=TRUE>>=
jjac<-model2jacfun(regmod, ones)
print(jjac)
# Note that we now need some data!
valjjac<-jjac(ones, yield=pastured$yield, time=pastured$time)
cat("valjac:")
print(valjjac)
# Now compute the numerical approximation
Jn<-jacobian(jres, ones, , yield=pastured$yield, time=pastured$time)
cat("maxabsdiff=",max(abs(Jn-valjjac)),"\n")
@

As with the WEEDS problem, we can compute the sum of squares function and
the gradient. 

<<chunk14, echo=TRUE>>=
ssfn<-model2ssfun(regmod, ones) # problem getting the data attached!
print(ssfn)
valss<-ssfn(ones, yield=pastured$yield, time=pastured$time)
cat("valss: ",valss,"\n")
grfn<-model2grfun(regmod, ones) # problem getting the data attached!
print(grfn)
valgr<-grfn(ones, yield=pastured$yield, time=pastured$time)
cat("valgr:")
print(valgr)
gn<-grad(ssfn, ones, yield=pastured$yield, time=pastured$time)
cat("maxabsdiff=",max(abs(gn-valgr)),"\n")
@

Moreover, we can use the Huet starting parameters as a double check
on our conversion of the expression to various optimization-style functions.

<<chunk15, echo=TRUE>>=
cat("\n\nHuetstart:")
print(huetstart)
valjres<-jres(huetstart, yield=pastured$yield, time=pastured$time)
cat("valjres:")
print(valjres)
valss<-ssfn(huetstart, yield=pastured$yield, time=pastured$time)
cat("valss:", valss, "\n")
valjjac<-jjac(huetstart, yield=pastured$yield, time=pastured$time)
cat("valjac:")
print(valjjac)
Jn<-jacobian(jres, huetstart, , yield=pastured$yield, time=pastured$time)
cat("maxabsdiff=",max(abs(Jn-valjjac)),"\n")
valgr<-grfn(huetstart, yield=pastured$yield, time=pastured$time)
cat("valgr:")
print(valgr)
gn<-grad(ssfn, huetstart, yield=pastured$yield, time=pastured$time)
cat("maxabsdiff=",max(abs(gn-valgr)),"\n")
@

Now that we have these functions, let us apply them with \code{nlfb}. 

<<chunk16, echo=TRUE>>=
cat("All ones to start\n")
anlfb<-nlfb(ones, jres, jjac, trace=FALSE, yield=pastured$yield, time=pastured$time)
print(strwrap(anlfb))
cat("Huet start\n")
anlfbh<-nlfb(huetstart, jres, jjac, trace=FALSE, yield=pastured$yield, time=pastured$time)
print(strwrap(anlfbh))
@

\section{Using bounds and masks}

See the examples in man/nlmrt-package.Rd. 


\section{Brief comparison with \code{minpack.lm}}

<<chunk17, echo=TRUE>>=
require(minpack.lm)
anlslm<-nls.lm(ones, lower=rep(-1000,4), upper=rep(1000,4), jres, jjac, yield=pastured$yield, time=pastured$time)
cat("anlslm from ones\n")
print(strwrap(anlslm))
anlslmh<-nls.lm(huetstart, lower=rep(-1000,4), upper=rep(1000,4), jres, jjac, yield=pastured$yield, time=pastured$time)
cat("anlslmh from huetstart\n")
print(strwrap(anlslmh))
@


\bibliography{nlpd}
\bibliographystyle{chicago}

\end{document}

