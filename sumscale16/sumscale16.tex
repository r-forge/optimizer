\documentclass[11pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{graphicx} %If you want to include postscript graphics
\usepackage{wrapfig} 
\usepackage[round]{natbib}
\usepackage{url}

\raggedbottom
\newcommand{\B}[1]{{\bf #1 \rm}}
\newcommand{\Splus}{{\sf S-PLUS}}
\newcommand{\Slang}{{\sf S}}
\newcommand{\R}{{\sf R}} 
\newcommand{\I}[1]{{\it#1\rm}}
\newcommand{\code}[1]{{\tt#1}}
\newcommand{\pkg}[1]{\bf{\tt#1}\rm }
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}










?? find new reference for RQ times
?? put in eigen
?? clean up warnings
?? try simple problem with different sum constraints e.g. sum vs sum of squares


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning in parse\_objects(paths[1L]): file cache/\_\_objects not found}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in parse\_objects(paths[2L]): file cache/\_\_globals not found}}\end{kframe}
\end{knitrout}


\title{Optimization problems constrained by parameter sums}
\author{Gabor Grothendieck, GKX Group,\\
John C. Nash, Telfer School of Management, University of Ottawa, and\\
Ravi Varadhan, Johns Hopkins University Medical School}
\date{November 2016}
\maketitle

\section*{Abstract}

This article presents a discussion of optimization problems where the 
objective function $f(\B{x})$ has parameters that are constrained by some
scaling, so that $q(\B{x}) = constant$, where this function $q()$ involves
a sum of the parameters, their squares, or similar simple function.

\section{Background}

We consider problems where we want to minimize or maximize a function subject to a constraint
that the sum of some function of the parameters, e.g., their sum of squares, must 
equal some constant.
We refer to these problems as \B{sumscale} optimization problems. We have observed questions 
about problems like this on the R-help mailing list: 

\begin{verbatim}
Jul 19, 2012 at 10:24 AM, Linh Tran <Tranlm@berkeley.edu> wrote:
> Hi fellow R users,
>
> I am desperately hoping there is an easy way to do this in R.
>
> Say I have three functions:
>
> f(x) = x^2
> f(y) = 2y^2
> f(z) = 3z^2
>
> constrained such that x+y+z=c (let c=1 for simplicity).
>
> I want to find the values of x,y,z that will minimize 
f(x) + f(y) + f(z).
\end{verbatim}

If the parameters $x$, $y$ and $z$ are non-negative, this problem can actually 
be solved as a Quadratic Program. We revisit this problem at the end of this
article.

Other examples of this type of objective function are:

\begin{itemize}
\item{The maximum volume  of a regular polyhedron where the sum of the lengths
of the sides is fixed.}
\item{The minimum negative log likelihood for a multinomial model.}
%% ?? May want to expand
%% this -- Gabor's example is not data dependent, but it would be nice to have one of these.
\item{The Rayleigh Quotient for the maximal or minimal eigensolutions of a matrix, where
the eigenvectors should be normalized so the square norm of the vector is 1.}
\end{itemize}

For the moment, let us consider a basic example, which is 
\vspace*{10pt}

\B{Problem A}: Minimize  $( - \prod{\B{x}})$ subject to $\sum{\B{x}}=1$
It is \textbf{assumed} for multinomial problems that the $x$ elements are positive.
\vspace*{10pt}

This is a very simplified version of the multinomial maximum likelihood problem. 

Because these problems all have an objective that is dependent on a scaled set of parameters 
where the scale is defined by a sum, sum of squares, or similar sum of the parameters, we will
refer to them as \B{sumscale} optimization problems. The condition that the parameters
must be positive is often implicit. In practice it can be important to have it imposed
explicitly if it is part of the actual problem. 

\section{Using general optimization with sumscale problems}

Let us use the basic example above to consider how we might formulate Problem A to try
to find a computational solution with \R. 

\subsection{A direct approach}

One possibility is to select one of the parameters and solve for it in 
terms of the others. Let this
be the last parameter $x_n$, so that the set of parameters to be 
optimized is $ \B{y} = (x_1, x_1, ..., x_{n-1})$ where 
$n$ is the original size of our problem. We now have the unconstrained problem
\vspace*{10pt}

$ minimize ( - (\prod{\B{y}}) * (1 - \sum{y} ) ) $

\vspace*{10pt}
This is easily coded and tried. We will use a very simple start, namely, the sequence $1,2, ...,
(n-1)$ scaled by $1/n^2$. We will also specify that the gradient is to be computed by a 
central approximation \citep{optextras}. At this point we are not requiring positive
parameters, but our methods do return solutions with all positive parameters when started
as in the example directly below.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{cat}\hlstd{(}\hlstr{"try loading optimrx\textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## try loading optimrx
\end{verbatim}
\begin{alltt}
\hlkwd{require}\hlstd{(optimrx,} \hlkwc{quietly}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\hlstd{pr} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{y}\hlstd{) \{}
\hlopt{-} \hlkwd{prod}\hlstd{(y)}\hlopt{*}\hlstd{(}\hlnum{1}\hlopt{-}\hlkwd{sum}\hlstd{(y))}
\hlstd{\}}
\hlkwd{cat}\hlstd{(}\hlstr{"test the simple product for n=5\textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## test the simple product for n=5
\end{verbatim}
\begin{alltt}
\hlstd{meth} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"Nelder-Mead"}\hlstd{,} \hlstr{"BFGS"}\hlstd{)}
\hlstd{n}\hlkwb{<-}\hlnum{5}
\hlstd{m}\hlkwb{<-}\hlstd{n}\hlopt{-}\hlnum{1}
  \hlstd{st}\hlkwb{<-}\hlnum{1}\hlopt{:}\hlstd{m}\hlopt{/}\hlstd{(m}\hlopt{*}\hlstd{m)}
   \hlstd{ans}\hlkwb{<-}\hlkwd{opm}\hlstd{(st, pr,} \hlkwc{gr}\hlstd{=}\hlstr{"grcentral"}\hlstd{,} \hlkwc{control}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{trace}\hlstd{=}\hlnum{0}\hlstd{))}
   \hlstd{ao}\hlkwb{<-}\hlkwd{summary}\hlstd{(ans,}\hlkwc{order}\hlstd{=value)}
\hlkwd{print}\hlstd{(ao)}
\end{alltt}
\begin{verbatim}
##                    p1        p2        p3        p4         value fevals
## Nelder-Mead 0.1999973 0.1999963 0.2000065 0.2000013 -0.0003200000    201
## BFGS        0.2000550 0.1999015 0.2000793 0.1999630 -0.0003199999     20
##             gevals convergence kkt1 kkt2 xtime
## Nelder-Mead     NA           0 TRUE TRUE 0.004
## BFGS            17           0 TRUE TRUE 0.000
\end{verbatim}
\begin{alltt}
\hlstd{par} \hlkwb{<-} \hlkwd{as.double}\hlstd{(ao[}\hlnum{1}\hlstd{,}\hlnum{1}\hlopt{:}\hlstd{m])}
\hlstd{par} \hlkwb{<-} \hlkwd{c}\hlstd{(par,} \hlnum{1}\hlopt{-}\hlkwd{sum}\hlstd{(par))}
\hlstd{par} \hlkwb{<-} \hlstd{par}\hlopt{/}\hlkwd{sum}\hlstd{(par)}
\hlkwd{cat}\hlstd{(}\hlstr{"Best parameters:"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Best parameters:
\end{verbatim}
\begin{alltt}
\hlkwd{print}\hlstd{(par)}
\end{alltt}
\begin{verbatim}
## [1] 0.1999973 0.1999963 0.2000065 0.2000013 0.1999986
\end{verbatim}
\end{kframe}
\end{knitrout}


While these codes work fine for small $n$, it is fairly easy to see that there will be 
computational difficulties as the size of the problem increases. Since the sum of the 
parameters is constrained to be equal to 1, the parameters are of the order of $1/n$,
and the function therefore of the order of $1/(n^n)$, which underflows around $n=144$ in 
\R. 

We do need to think about the positivity of the parameters. Let us start with 
a different set of values, some of which are negative. If pairs are negative, the 
product is still positive.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{stm} \hlkwb{<-} \hlstd{st}\hlopt{*}\hlstd{((}\hlopt{-}\hlnum{1}\hlstd{)}\hlopt{^}\hlstd{(}\hlnum{1}\hlopt{:}\hlnum{4}\hlstd{))}
\hlkwd{print}\hlstd{(stm)}
\end{alltt}
\begin{verbatim}
## [1] -0.0625  0.1250 -0.1875  0.2500
\end{verbatim}
\begin{alltt}
\hlstd{ansm}\hlkwb{<-}\hlkwd{opm}\hlstd{(stm, pr,} \hlkwc{gr}\hlstd{=}\hlstr{"grcentral"}\hlstd{,} \hlkwc{control}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{trace}\hlstd{=}\hlnum{0}\hlstd{))}
\hlstd{aom} \hlkwb{<-} \hlkwd{summary}\hlstd{(ansm,} \hlkwc{order}\hlstd{=value)}
\hlkwd{print}\hlstd{(aom)}
\end{alltt}
\begin{verbatim}
##                        p1           p2            p3           p4
## BFGS        -2.954862e+57 2.003069e+57 -2.955447e+57 1.953545e+57
## Nelder-Mead -5.411717e+47 3.177738e+47 -2.292733e+46 2.395192e+47
##                      value fevals gevals convergence kkt1  kkt2 xtime
## BFGS        -6.676311e+286    300     30           0 TRUE FALSE 0.004
## Nelder-Mead -6.427455e+234   1001     NA           1 TRUE FALSE 0.004
\end{verbatim}
\end{kframe}
\end{knitrout}

Clearly this is not what we intended.

\subsection{A log-likelihood approach}

Traditionally, statisticians solve maximum likelihood problems by \B{minimizing} the 
negative log-likelihood. That is, the objective function is formed as (-1) times the
logarithm of the likelihood. Using this idea, we convert our product to a sum. Choosing the first
parameter to be the one determined by the summation constraint, we can write the 
function and gradient quite easily. Programs that try to find the minimum may try sets of 
parameters where some are zero or negative, so that logarithms of non-positive numbers 
are attempted, so we have put some safeguards in the function \code{nll} below. At this 
point we have assumed the gradient calculation is only attempted if the function can be 
computed satisfactorily, so we have
not put similar safeguards in the gradient. Note that we work with $n-1$ parameters
in the optimization, and expand to the full set for reporting.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{nll} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{y}\hlstd{) \{} \hlcom{# large result if near zero arguments to log()}
  \hlkwa{if} \hlstd{((}\hlkwd{any}\hlstd{(y} \hlopt{<=} \hlnum{10}\hlopt{*}\hlstd{.Machine}\hlopt{$}\hlstd{double.xmin))} \hlopt{||} \hlstd{(}\hlkwd{sum}\hlstd{(y)}\hlopt{>}\hlnum{1}\hlopt{-}\hlstd{.Machine}\hlopt{$}\hlstd{double.eps))}
         \hlstd{.Machine}\hlopt{$}\hlstd{double.xmax}
  \hlkwa{else}   \hlopt{-} \hlkwd{sum}\hlstd{(}\hlkwd{log}\hlstd{(y))} \hlopt{-} \hlkwd{log}\hlstd{(}\hlnum{1}\hlopt{-}\hlkwd{sum}\hlstd{(y))}
\hlstd{\}}
\hlstd{nll.g} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{y}\hlstd{) \{} \hlopt{-} \hlnum{1}\hlopt{/}\hlstd{y} \hlopt{+} \hlnum{1}\hlopt{/}\hlstd{(}\hlnum{1}\hlopt{-}\hlkwd{sum}\hlstd{(y))\}} \hlcom{# so far not safeguarded}
\end{alltt}
\end{kframe}
\end{knitrout}

We can easily try several optimization methods using the \code{opm()} function
of \pkg{optimrx} package. Here are the
calls, which overall did not perform as well as we would like. Note that we do not ask for
\code{method="ALL"}. For one thing, this can take a lot of computing effort. Also, 
we found that some of the methods, in particular those using Powell's
quadratic approximation methods, seem to get "stuck". The reasons for this have not been
sufficiently understood to report at this time. 

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{require}\hlstd{(optimrx,} \hlkwc{quietly}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\hlstd{n}\hlkwb{<-}\hlnum{5}
\hlcom{## mset<-c("L-BFGS-B", "BFGS", "CG", "spg", "ucminf", "nlm", "nlminb", "Rvmmin", "Rcgmin")}
\hlstd{mset}\hlkwb{<-}\hlkwd{c}\hlstd{(}\hlstr{"L-BFGS-B"}\hlstd{,} \hlstr{"spg"}\hlstd{,} \hlstr{"nlm"}\hlstd{,} \hlstr{"nlminb"}\hlstd{,} \hlstr{"Rvmmin"}\hlstd{,} \hlstr{"Rcgmin"}\hlstd{)}
\hlstd{a5}\hlkwb{<-}\hlkwd{opm}\hlstd{(}\hlnum{2}\hlopt{:}\hlstd{n}\hlopt{/}\hlstd{n}\hlopt{^}\hlnum{2}\hlstd{, nll,} \hlkwc{gr}\hlstd{=}\hlstr{"grfwd"}\hlstd{,} \hlkwc{method}\hlstd{=mset,} \hlkwc{control}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{dowarn}\hlstd{=}\hlnum{FALSE}\hlstd{))}
\hlstd{a5g}\hlkwb{<-}\hlkwd{opm}\hlstd{(}\hlnum{2}\hlopt{:}\hlstd{n}\hlopt{/}\hlstd{n}\hlopt{^}\hlnum{2}\hlstd{, nll, nll.g,} \hlkwc{method}\hlstd{=mset,} \hlkwc{control}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{dowarn}\hlstd{=}\hlnum{FALSE}\hlstd{))}
\hlstd{a5gb}\hlkwb{<-}\hlkwd{opm}\hlstd{(}\hlnum{2}\hlopt{:}\hlstd{n}\hlopt{/}\hlstd{n}\hlopt{^}\hlnum{2}\hlstd{, nll, nll.g,} \hlkwc{lower}\hlstd{=}\hlnum{0}\hlstd{,} \hlkwc{upper}\hlstd{=}\hlnum{1}\hlstd{,} \hlkwc{method}\hlstd{=mset,} \hlkwc{control}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{dowarn}\hlstd{=}\hlnum{FALSE}\hlstd{))}
\hlcom{#- a5x <- opm(2:n/n^2, nll, nll.g, method="ALL", control=list(dowarn=FALSE))}
\hlkwd{summary}\hlstd{(a5,}\hlkwc{order}\hlstd{=value)}
\end{alltt}
\begin{verbatim}
##                 p1        p2        p3        p4         value fevals
## Rcgmin   0.2000000 0.2000000 0.2000000 0.2000000  8.047190e+00     51
## Rvmmin   0.2000000 0.2000000 0.2000000 0.2000000  8.047190e+00     46
## spg      0.2000000 0.2000000 0.2000000 0.2000000  8.047190e+00     17
## nlm      0.2000005 0.1999995 0.2000000 0.2000000  8.047190e+00     NA
## nlminb   0.2000004 0.1999990 0.1999989 0.1999992  8.047190e+00     23
## L-BFGS-B        NA        NA        NA        NA 8.988466e+307     NA
##          gevals convergence kkt1 kkt2 xtime
## Rcgmin       18           0 TRUE TRUE 0.004
## Rvmmin       13           0 TRUE TRUE 0.004
## spg          13           0 TRUE TRUE 0.048
## nlm          11           0 TRUE TRUE 0.000
## nlminb       12           0 TRUE TRUE 0.004
## L-BFGS-B     NA        9999   NA   NA 0.000
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(a5g,}\hlkwc{order}\hlstd{=value)}
\end{alltt}
\begin{verbatim}
##                 p1        p2        p3        p4         value fevals
## Rvmmin   0.2000000 0.2000000 0.2000000 0.2000000  8.047190e+00     43
## spg      0.2000000 0.2000000 0.2000000 0.2000000  8.047190e+00     17
## Rcgmin   0.2000000 0.2000000 0.2000000 0.2000000  8.047190e+00     28
## nlm      0.2000006 0.1999995 0.2000000 0.2000000  8.047190e+00     NA
## nlminb   0.2000004 0.1999990 0.1999989 0.1999992  8.047190e+00     23
## L-BFGS-B        NA        NA        NA        NA 8.988466e+307     NA
##          gevals convergence kkt1 kkt2 xtime
## Rvmmin       12           0 TRUE TRUE 0.000
## spg          13           0 TRUE TRUE 0.040
## Rcgmin       12           0 TRUE TRUE 0.000
## nlm          11           0 TRUE TRUE 0.000
## nlminb       12           0 TRUE TRUE 0.004
## L-BFGS-B     NA        9999   NA   NA 0.000
\end{verbatim}
\begin{alltt}
\hlkwd{summary}\hlstd{(a5gb,}\hlkwc{order}\hlstd{=value)}
\end{alltt}
\begin{verbatim}
##                 p1       p2        p3        p4         value fevals
## Rvmmin   0.2000000 0.200000 0.2000000 0.2000000  8.047190e+00     38
## Rcgmin   0.2000000 0.200000 0.2000000 0.2000000  8.047190e+00     18
## spg      0.2000000 0.200000 0.2000000 0.2000000  8.047190e+00     18
## nlminb   0.2000004 0.199999 0.1999989 0.1999992  8.047190e+00     23
## L-BFGS-B        NA       NA        NA        NA 8.988466e+307     NA
## nlm             NA       NA        NA        NA 8.988466e+307     NA
##          gevals convergence kkt1 kkt2 xtime
## Rvmmin       14           0 TRUE TRUE 0.004
## Rcgmin       10           0 TRUE TRUE 0.000
## spg          13           0 TRUE TRUE 0.040
## nlminb       12           0 TRUE TRUE 0.000
## L-BFGS-B     NA        9999   NA   NA 0.000
## nlm          NA        9999   NA   NA 0.000
\end{verbatim}
\begin{alltt}
\hlcom{#- summary(a5x,order=value)}
\end{alltt}
\end{kframe}
\end{knitrout}

Most, but not all, of the methods find the solution for the $n=5$ case. 
The exception (L-BFGS-B) is due to the optimization method trying to 
compute the gradient where sum(x) is greater than 1. We 
have not tried to determine the source of this particular issue. However, 
it is almost certainly 
a consequence of too large a step. This method uses a quite sophisticated
line search and its ability to use quite large search steps often results in
very good performance. Here, however, the particular form of $log(1-sum(x))$ 
is undefined once the argument of
the logarithm is negative. Indeed, this is the basis of 
logarithmic barrier functions for constraints. There
is a similar issue if any of the $n-1$ parameters approach or pass zero. Negative 
parameter values are inadmissible in this formulation. 

Numerical gradient approximations can similarly fail, 
particularly as step sizes are often of the order
of 1E-7 in size. There is generally no special check within numerical 
gradient routines to apply bounds. 
Note also that a lower bound of 0 on parameters is not adequate, 
since $log(0)$ is undefined. Choosing a
bound large enough to avoid the logarithm of a zero or negative argument 
while still being small enough 
to allow for parameter optimization is non-trivial.

\subsection{Projected search directions}

Objective functions defined by $(-1)*\prod{\B{x}}$ or $(-1)*\sum{log(\B{x})}$ will change 
with the scale of the parameters. Moreover, the constraint $\sum{\B{x}}=1$ 
effectively imposes the scaling $\B{x_{scaled}} = \B{x}/\sum{\B{x}}$. The 
optimizer \code{spg} from package \pkg{BB} allows us to project our search 
direction to satisfy constraints. Thus, we could use the following approach.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{require}\hlstd{(BB,} \hlkwc{quietly}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\hlstd{nllrv} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{) \{}\hlopt{-} \hlkwd{sum}\hlstd{(}\hlkwd{log}\hlstd{(x))\}}
\hlstd{nllrv.g} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{) \{}\hlopt{-} \hlnum{1}\hlopt{/}\hlstd{x \}}
\hlstd{proj} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{) \{x}\hlopt{/}\hlkwd{sum}\hlstd{(x)\}}
\hlstd{n} \hlkwb{<-} \hlnum{5}
\hlstd{tspg}\hlkwb{<-}\hlkwd{system.time}\hlstd{(aspg} \hlkwb{<-} \hlkwd{spg}\hlstd{(}\hlkwc{par}\hlstd{=(}\hlnum{1}\hlopt{:}\hlstd{n)}\hlopt{/}\hlstd{n}\hlopt{^}\hlnum{2}\hlstd{,} \hlkwc{fn}\hlstd{=nllrv,} \hlkwc{gr}\hlstd{=nllrv.g,} \hlkwc{project}\hlstd{=proj))[[}\hlnum{3}\hlstd{]]}
\end{alltt}
\begin{verbatim}
## iter:  0  f-value:  11.30689  pgrad:  0.3607565
\end{verbatim}
\begin{alltt}
\hlstd{tspgn}\hlkwb{<-}\hlkwd{system.time}\hlstd{(aspgn} \hlkwb{<-} \hlkwd{spg}\hlstd{(}\hlkwc{par}\hlstd{=(}\hlnum{1}\hlopt{:}\hlstd{n)}\hlopt{/}\hlstd{n}\hlopt{^}\hlnum{2}\hlstd{,} \hlkwc{fn}\hlstd{=nllrv,} \hlkwc{gr}\hlstd{=}\hlkwa{NULL}\hlstd{,} \hlkwc{project}\hlstd{=proj))[[}\hlnum{3}\hlstd{]]}
\end{alltt}
\begin{verbatim}
## iter:  0  f-value:  11.30689  pgrad:  0.1333334
\end{verbatim}
\begin{alltt}
\hlkwd{cat}\hlstd{(}\hlstr{"Times: with gradient ="}\hlstd{,tspg,}\hlstr{"   using numerical approx.="}\hlstd{, tspgn,}\hlstr{"\textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Times: with gradient = 0.043    using numerical approx.= 0.041
\end{verbatim}
\begin{alltt}
\hlkwd{cat}\hlstd{(}\hlstr{"F_optimal: with gradient="}\hlstd{,aspg}\hlopt{$}\hlstd{value,}\hlstr{"  num. approx.="}\hlstd{,aspgn}\hlopt{$}\hlstd{value,}\hlstr{"\textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## F_optimal: with gradient= 8.04719   num. approx.= 8.04719
\end{verbatim}
\begin{alltt}
\hlstd{pbest}\hlkwb{<-}\hlkwd{rep}\hlstd{(}\hlnum{1}\hlopt{/}\hlstd{n, n)}
\hlkwd{cat}\hlstd{(}\hlstr{"fbest = "}\hlstd{,}\hlkwd{nllrv}\hlstd{(pbest),}\hlstr{"  when all parameters = "}\hlstd{, pbest[}\hlnum{1}\hlstd{],}\hlstr{"\textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## fbest =  8.04719   when all parameters =  0.2
\end{verbatim}
\begin{alltt}
\hlkwd{cat}\hlstd{(}\hlstr{"deviations:  with gradient="}\hlstd{,}\hlkwd{max}\hlstd{(}\hlkwd{abs}\hlstd{(aspg}\hlopt{$}\hlstd{par}\hlopt{-}\hlstd{pbest)),}\hlstr{"   num. approx.="}\hlstd{,}\hlkwd{max}\hlstd{(}\hlkwd{abs}\hlstd{(aspgn}\hlopt{$}\hlstd{par}\hlopt{-}\hlstd{pbest)),}\hlstr{"\textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## deviations:  with gradient= 3.81244e-06    num. approx.= 6.367897e-08
\end{verbatim}
\end{kframe}
\end{knitrout}

Here the projection \code{proj} is the key to success of method 
\code{spg}. The near-equality of timings with and without analytic gradient is 
because the approximation attempt only uses one iteration and two function evaluations
to finish. In fact, the solution with approximate gradient is actually better, and
this seems to carry over to cases with more parameters, e.g., 100 of them.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{n}\hlkwb{<-}\hlnum{100}
\hlstd{tspgh}\hlkwb{<-}\hlkwd{system.time}\hlstd{(aspgh} \hlkwb{<-} \hlkwd{spg}\hlstd{(}\hlkwc{par}\hlstd{=(}\hlnum{1}\hlopt{:}\hlstd{n)}\hlopt{/}\hlstd{n}\hlopt{^}\hlnum{2}\hlstd{,} \hlkwc{fn}\hlstd{=nllrv,} \hlkwc{gr}\hlstd{=nllrv.g,} \hlkwc{project}\hlstd{=proj))[[}\hlnum{3}\hlstd{]]}
\end{alltt}
\begin{verbatim}
## iter:  0  f-value:  557.2947  pgrad:  0.1925703
\end{verbatim}
\begin{alltt}
\hlstd{tspgnh}\hlkwb{<-}\hlkwd{system.time}\hlstd{(aspgnh} \hlkwb{<-} \hlkwd{spg}\hlstd{(}\hlkwc{par}\hlstd{=(}\hlnum{1}\hlopt{:}\hlstd{n)}\hlopt{/}\hlstd{n}\hlopt{^}\hlnum{2}\hlstd{,} \hlkwc{fn}\hlstd{=nllrv,} \hlkwc{gr}\hlstd{=}\hlkwa{NULL}\hlstd{,} \hlkwc{project}\hlstd{=proj))[[}\hlnum{3}\hlstd{]]}
\end{alltt}
\begin{verbatim}
## iter:  0  f-value:  557.2947  pgrad:  0.00980205
\end{verbatim}
\begin{alltt}
\hlkwd{cat}\hlstd{(}\hlstr{"Times: with gradient ="}\hlstd{,tspgh,}\hlstr{"   using numerical approx.="}\hlstd{, tspgnh,}\hlstr{"\textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Times: with gradient = 0.054    using numerical approx.= 0.041
\end{verbatim}
\begin{alltt}
\hlkwd{cat}\hlstd{(}\hlstr{"F_optimal: with gradient="}\hlstd{,aspgh}\hlopt{$}\hlstd{value,}\hlstr{"  num. approx.="}\hlstd{,aspgnh}\hlopt{$}\hlstd{value,}\hlstr{"\textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## F_optimal: with gradient= 460.517   num. approx.= 460.517
\end{verbatim}
\begin{alltt}
\hlstd{pbesth}\hlkwb{<-}\hlkwd{rep}\hlstd{(}\hlnum{1}\hlopt{/}\hlstd{n, n)}
\hlkwd{cat}\hlstd{(}\hlstr{"fbest = "}\hlstd{,}\hlkwd{nllrv}\hlstd{(pbesth),}\hlstr{"  when all parameters = "}\hlstd{, pbesth[}\hlnum{1}\hlstd{],}\hlstr{"\textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## fbest =  460.517   when all parameters =  0.01
\end{verbatim}
\begin{alltt}
\hlkwd{cat}\hlstd{(}\hlstr{"deviations:  with gradient="}\hlstd{,}\hlkwd{max}\hlstd{(}\hlkwd{abs}\hlstd{(aspgh}\hlopt{$}\hlstd{par}\hlopt{-}\hlstd{pbesth)),}\hlstr{"   num. approx.="}\hlstd{,}\hlkwd{max}\hlstd{(}\hlkwd{abs}\hlstd{(aspgnh}\hlopt{$}\hlstd{par}\hlopt{-}\hlstd{pbesth)),}\hlstr{"\textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## deviations:  with gradient= 5.055715e-07    num. approx.= 7.006446e-08
\end{verbatim}
\end{kframe}
\end{knitrout}

Larger $n$ values eventually give difficulties as non-positive parameters are produced at
intermediate stages of the optimization. 

%% <<label=C13ravi2, echo=TRUE>>=
%% n<-1000
%% tspgt<-system.time(aspgt <- spg(par=(1:n)/n^2, fn=nllrv, gr=nllrv.g, project=proj))[[3]]
%% tspgnt<-system.time(aspgnt <- spg(par=(1:n)/n^2, fn=nllrv, gr=NULL, project=proj))[[3]]
%% cat("Times: with gradient =",tspgt,"   using numerical approx.=", tspgnt,"\n")
%% cat("F_optimal: with gradient=",aspgt$value,"  num. approx.=",aspgnt$value,"\n")
%% pbestt<-rep(1/n, n)
%% cat("fbest = ",nllrv(pbestt),"  when all parameters = ", pbestt[1],"\n")
%% cat("deviations:  with gradient=",max(abs(aspgt$par-pbestt)),"   num. approx.=",max(abs(aspgnt$par-pbestt)),"\n")
%% @


Minimization methods other than \texttt{spg} do not have the flexibility to impose the projection directly. 
We would need to carefully build the projection into
the function(s) and/or the method codes. 
This was done by \cite{Geradin71} for the Rayleigh quotient 
problem, but requires a number of changes to the program code.

\subsection{$log()$ transformation of parameters}

When problems give difficulties, it is common to re-formulate them 
by transformations of the function
or the parameters. 
A common method to ensure parameters are positive is to use a log transform. 
In the present case, optimizing over
parameters that are the logarithms of the parameters above 
ensures we have positive arguments to most of the
elements of the negative log likelihood. Here is the code. 
Note that the parameters used in optimization
are "lx" and not x.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{enll} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{lx}\hlstd{) \{}
    \hlstd{x}\hlkwb{<-}\hlkwd{exp}\hlstd{(lx)}
    \hlstd{fval}\hlkwb{<-}  \hlopt{-} \hlkwd{sum}\hlstd{(} \hlkwd{log}\hlstd{( x}\hlopt{/}\hlkwd{sum}\hlstd{(x) ) )}
\hlstd{\}}
\hlstd{enll.g} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{lx}\hlstd{)\{}
    \hlstd{x}\hlkwb{<-}\hlkwd{exp}\hlstd{(lx)}
    \hlstd{g}\hlkwb{<-}\hlkwd{length}\hlstd{(x)}\hlopt{/}\hlkwd{sum}\hlstd{(x)} \hlopt{-} \hlnum{1}\hlopt{/}\hlstd{x}
    \hlstd{gval}\hlkwb{<-}\hlstd{g}\hlopt{*}\hlkwd{exp}\hlstd{(lx)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

But where is our constraint that the sum of parameters must be 1? 
Here we have noted that we could define the objective 
function only to within the scaling  $\B{x}/\sum(\B{x})$. There is a minor 
nuisance, in that we need to re-scale our 
parameters after solution to have them in a standard form. 
This is most noticeable if one uses \pkg{optimrx} function \code{opm()}
and displays the results of \code{method = mset}, a collection of six
gradient-based minimizers. In the following, we
extract the best solution for the 5-parameter problem.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{require}\hlstd{(optimrx,} \hlkwc{quietly}\hlstd{=}\hlnum{TRUE}\hlstd{)} \hlcom{# just to be sure}
\hlstd{st}\hlkwb{<-}\hlnum{1}\hlopt{:}\hlnum{5}\hlopt{/}\hlnum{10} \hlcom{# 5 parameters, crude scaling to start}
\hlstd{st}\hlkwb{<-}\hlkwd{log}\hlstd{(st)}
\hlstd{n} \hlkwb{<-} \hlnum{5}
\hlstd{mset}\hlkwb{<-}\hlkwd{c}\hlstd{(}\hlstr{"L-BFGS-B"}\hlstd{,} \hlstr{"spg"}\hlstd{,} \hlstr{"nlm"}\hlstd{,} \hlstr{"nlminb"}\hlstd{,} \hlstr{"Rvmmin"}\hlstd{,} \hlstr{"Rcgmin"}\hlstd{)}
\hlstd{a5x}\hlkwb{<-}\hlkwd{opm}\hlstd{(st, enll, enll.g,} \hlkwc{method}\hlstd{=mset,} \hlkwc{control}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{trace}\hlstd{=}\hlnum{0}\hlstd{))}
\hlstd{a5xbyvalue}\hlkwb{<-}\hlkwd{summary}\hlstd{(a5x,} \hlkwc{order}\hlstd{=value)}
\hlkwd{print}\hlstd{(a5xbyvalue)[(n}\hlopt{+}\hlnum{1}\hlstd{)}\hlopt{:}\hlstd{(n}\hlopt{+}\hlnum{7}\hlstd{)]}
\end{alltt}
\begin{verbatim}
##                 p1        p2        p3        p4        p5   value fevals
## spg      -1.345087 -1.345087 -1.345087 -1.345087 -1.345087 8.04719      6
## Rvmmin   -1.345087 -1.345087 -1.345087 -1.345087 -1.345087 8.04719     26
## Rcgmin   -1.345087 -1.345087 -1.345087 -1.345087 -1.345087 8.04719     11
## nlminb   -1.345087 -1.345087 -1.345087 -1.345087 -1.345087 8.04719      8
## nlm      -1.345084 -1.345085 -1.345086 -1.345088 -1.345091 8.04719     NA
## L-BFGS-B -1.345080 -1.345087 -1.345085 -1.345088 -1.345093 8.04719      7
##          gevals convergence kkt1  kkt2 xtime
## spg           5           0 TRUE FALSE 0.044
## Rvmmin        8           0 TRUE FALSE 0.000
## Rcgmin        8           0 TRUE FALSE 0.000
## nlminb        8           0 TRUE FALSE 0.004
## nlm           6           0 TRUE FALSE 0.000
## L-BFGS-B      7           0 TRUE FALSE 0.000
##            value fevals gevals convergence kkt1  kkt2 xtime
## spg      8.04719      6      5           0 TRUE FALSE 0.044
## Rvmmin   8.04719     26      8           0 TRUE FALSE 0.000
## Rcgmin   8.04719     11      8           0 TRUE FALSE 0.000
## nlminb   8.04719      8      8           0 TRUE FALSE 0.004
## nlm      8.04719     NA      6           0 TRUE FALSE 0.000
## L-BFGS-B 8.04719      7      7           0 TRUE FALSE 0.000
\end{verbatim}
\begin{alltt}
\hlstd{xnor}\hlkwb{<-}\hlkwd{exp}\hlstd{(a5xbyvalue[}\hlnum{1}\hlstd{,} \hlnum{1}\hlopt{:}\hlnum{5}\hlstd{])} \hlcom{# get the 5 parameters of "best" solution, exponentiate}
\hlstd{xnor}\hlkwb{<-}\hlstd{xnor}\hlopt{/}\hlkwd{sum}\hlstd{(xnor)}
\hlkwd{cat}\hlstd{(}\hlstr{"best normalized parameters:"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## best normalized parameters:
\end{verbatim}
\begin{alltt}
\hlkwd{print}\hlstd{(xnor)}
\end{alltt}
\begin{verbatim}
##      p1  p2  p3  p4  p5
## spg 0.2 0.2 0.2 0.2 0.2
\end{verbatim}
\end{kframe}
\end{knitrout}


While there are reasons to think that the indeterminacy
might upset the optimization codes, in practice, the objective 
and gradient above are generally
well-behaved, though they did reveal that tests of the size 
of the gradient used, in particular, to
decide to terminate iterations in \pkg{Rcgmin} were too 
hasty in stopping progress for problems
with larger numbers of parameters. A user-specified tolerance is now allowed; for
example \code{control=list(tol=1e-32)}, the rather extreme setting we have used
below. 

Let us try a larger problem in 100 parameters. We emply the conjugate gradient
algorithm in package \code{Rcgmin}.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## Initial function value = 460.5587
## Time =  0.001   fval= 460.517
## Average parameter is  0.01  with max deviation 6.290762e-08
\end{verbatim}
\end{kframe}
\end{knitrout}

We have a solution. However, a worrying aspect of this 
solution is that the objective function 
at the start and end differ by a tiny amount. 

\subsection{A transformation inspired by the n-sphere}

A slightly different transformation or projection is inspired by spherical coordinates.
See \url{https://en.wikipedia.org/wiki/N-sphere}.

The idea here is to transform a set of $n$ parameters by specifying $n-1$ values and
letting a special projection transform this set of $n-1$ numbers into a set of $n$ parameters
that always sum to 1. 

The first such transformation uses the trigonometric identity that 

$sin^2(theta) + cos^2(theta) =1$.

This identity is extended to $n$ dimensions. We can do this via the projection

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{proj1} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{) \{}
   \hlstd{s2} \hlkwb{<-} \hlkwd{sin}\hlstd{(theta)}\hlopt{^}\hlnum{2}
    \hlkwd{cumprod}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{, s2))} \hlopt{*} \hlkwd{c}\hlstd{(}\hlnum{1}\hlopt{-}\hlstd{s2,} \hlnum{1}\hlstd{)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

You can easily verify that this produces a set of parameters that sum to 1 by setting
$theta = (a,b)$ for a problem in 3 parameters. However, we do not need to use the 
$sin()$ function, as any transformation onto the unit line segment [0,1] will work.
\code{proj2} below works fine, and can be verified for 3 parameters as with \code{proj1},
though the parameters are different. (Caution!)

We solve problems in 5 and 100 parameters using the \code{spg()} function from package
\code{BB}, and by not specifying a gradient function use an internal approximation.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## ?? need to explain this better}
\hlstd{proj2} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{) \{}
    \hlstd{theta2} \hlkwb{<-} \hlstd{theta}\hlopt{^}\hlnum{2}
    \hlstd{s2} \hlkwb{<-} \hlstd{theta2} \hlopt{/} \hlstd{(}\hlnum{1} \hlopt{+} \hlstd{theta2)}
    \hlkwd{cumprod}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{, s2))} \hlopt{*} \hlkwd{c}\hlstd{(}\hlnum{1}\hlopt{-}\hlstd{s2,} \hlnum{1}\hlstd{)}
\hlstd{\}}
\hlstd{obj} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{) \{} \hlopt{-} \hlkwd{sum}\hlstd{(}\hlkwd{log}\hlstd{(}\hlkwd{proj2}\hlstd{(theta))) \}}
\hlstd{n} \hlkwb{<-} \hlnum{5}
\hlstd{ans} \hlkwb{<-} \hlkwd{spg}\hlstd{(}\hlkwd{seq}\hlstd{(n}\hlopt{-}\hlnum{1}\hlstd{), obj)}
\end{alltt}
\begin{verbatim}
## iter:  0  f-value:  11.15175  pgrad:  3 
## iter:  10  f-value:  8.78015  pgrad:  0.5806909 
## iter:  20  f-value:  8.04719  pgrad:  3.925749e-06
\end{verbatim}
\begin{alltt}
\hlkwd{proj2}\hlstd{(ans}\hlopt{$}\hlstd{par)} \hlcom{# The parameters}
\end{alltt}
\begin{verbatim}
## [1] 0.2000000 0.2000007 0.2000002 0.1999996 0.1999995
\end{verbatim}
\end{kframe}
\end{knitrout}


\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{n}\hlkwb{<-}\hlnum{100}
\hlstd{ans100} \hlkwb{<-} \hlkwd{spg}\hlstd{(}\hlkwd{seq}\hlstd{(n}\hlopt{-}\hlnum{1}\hlstd{), obj,} \hlkwc{control}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{trace}\hlstd{=}\hlnum{FALSE}\hlstd{),} \hlkwc{quiet}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\hlkwd{proj2}\hlstd{(ans100}\hlopt{$}\hlstd{par)[}\hlnum{1}\hlopt{:}\hlnum{5}\hlstd{]} \hlcom{# Display only 1st 5 parameters}
\end{alltt}
\begin{verbatim}
## [1] 0.009999999 0.010000001 0.010000000 0.010000000 0.009999999
\end{verbatim}
\end{kframe}
\end{knitrout}

In the above, we note that the transformation is embedded into the objective function, 
so we could run any of the optimizers in \pkg{optimrx} as follows. This can take some time, 
and the derivative-free
methods do an awful lot of work with this formulation, though they do seem to get the
best solution. We have omitted the results for these, as they make the rendering of
this document unacceptably slow with \code{knitr}. Moreover, \code{Rcgmin}
and \code{Rvmmin} are not recommended when an analytic gradient is not provided. Here
we have specified that a simple forward difference approximation to the gradient
should be used. 

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sans}\hlkwb{<-} \hlkwd{opm}\hlstd{(}\hlkwd{seq}\hlstd{(n}\hlopt{-}\hlnum{1}\hlstd{), obj,} \hlkwc{gr}\hlstd{=}\hlstr{"grfwd"}\hlstd{,} \hlkwc{method}\hlstd{=mset,} \hlkwc{control}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{dowarn}\hlstd{=}\hlnum{FALSE}\hlstd{))}
\hlcom{## summary(allans, order = "list(round(value, 3), fevals)", par.select = FALSE)}
\hlkwd{summary}\hlstd{(sans,} \hlkwc{order} \hlstd{= value,} \hlkwc{par.select} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\end{alltt}
\begin{verbatim}
##             value fevals gevals convergence  kkt1  kkt2 xtime
## Rvmmin   460.5170    398    272           2  TRUE  TRUE 0.660
## spg      460.5170    230    212           0  TRUE  TRUE 0.436
## Rcgmin   460.5170   1612   1023           0  TRUE  TRUE 1.872
## nlm      460.5170     NA    203           0  TRUE  TRUE 0.644
## L-BFGS-B 460.5170    280    280           0  TRUE  TRUE 0.488
## nlminb   482.2372    189    151           1 FALSE FALSE 0.332
\end{verbatim}
\end{kframe}
\end{knitrout}

\subsection{Fixing parameters}

Some function minimizers can specify that some parameters are fixed. \code{Rvmmin} and
\code{Rcgmin} are two such methods. Let us work with the \code{enll} objective function
that works with the logarithms of the parameters.
We need to rescale the parameters after solution and recompute the objective if we need it.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{n}\hlkwb{<-}\hlnum{5}
\hlstd{mmth} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"Rvmmin"}\hlstd{,} \hlstr{"Rcgmin"}\hlstd{)}
\hlstd{strt} \hlkwb{<-} \hlstd{(}\hlnum{1}\hlopt{:}\hlstd{n)}\hlopt{/}\hlstd{n}
\hlstd{lo} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlopt{-}\hlnum{100}\hlstd{, (n}\hlopt{-}\hlnum{1}\hlstd{)),strt[n])}
\hlstd{up} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{100}\hlstd{, (n}\hlopt{-}\hlnum{1}\hlstd{)),strt[n])}
\hlstd{amsk1} \hlkwb{<-} \hlkwd{opm}\hlstd{(strt, enll, enll.g,} \hlkwc{lower}\hlstd{=lo,} \hlkwc{upper}\hlstd{=up,} \hlkwc{method}\hlstd{=mmth)}
\end{alltt}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning in bmchk(par, lower = lower, upper = upper): Masks (fixed parameters) set by bmchk due to tight bounds. CAUTION!!}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in bmchk(par, lower = lower, upper = upper): Masks (fixed parameters) set by bmchk due to tight bounds. CAUTION!!}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in bmchk(par, lower = lower, upper = upper): Masks (fixed parameters) set by bmchk due to tight bounds. CAUTION!!}}\begin{alltt}
\hlkwd{print}\hlstd{(amsk1)}
\end{alltt}
\begin{verbatim}
##               p1        p2        p3        p4 p5   value fevals gevals
## Rvmmin 1.0000000 1.0000000 1.0000000 1.0000000  1 8.04719     32     10
## Rcgmin 0.9999998 0.9999998 0.9999998 0.9999997  1 8.04719     29     13
##        convergence kkt1  kkt2 xtime
## Rvmmin           0 TRUE FALSE 0.004
## Rcgmin           0 TRUE FALSE 0.004
\end{verbatim}
\begin{alltt}
\hlstd{amsk1} \hlkwb{<-} \hlkwd{summary}\hlstd{(amsk1,} \hlkwc{order}\hlstd{=value)}
\hlstd{parmsk} \hlkwb{<-} \hlstd{amsk1[}\hlnum{1}\hlstd{,} \hlnum{1}\hlopt{:}\hlstd{n]}
\hlstd{parmsk} \hlkwb{<-} \hlstd{parmsk}\hlopt{/}\hlkwd{sum}\hlstd{(parmsk)}
\hlkwd{print}\hlstd{(parmsk)}
\end{alltt}
\begin{verbatim}
##         p1  p2  p3  p4  p5
## Rvmmin 0.2 0.2 0.2 0.2 0.2
\end{verbatim}
\end{kframe}
\end{knitrout}

This also works well for $n=100$ with both methods that allow fixed parameters.

Note that for the product problem, no parameter can be zero or the product is zero.
In the case of the Rayleigh Quotient below, we may have to be concerned that the
parameter we have chosen to fix may have zero as its optimal value, in which case
the approach of fixing one parameter is not useful unless we are prepared to 
indulge some trial and error. 

\subsection{Use the gradient equations}

Another approach is to "solve" the gradient equations as a nonlinear equations
problem. We could do this with 
a sum of squares minimizer, though the \code{nls} function in \R\ is 
specifically NOT useful as it cannot deal
with small or zero residuals. However, \code{nlfb} 
from package \pkg{nlmrt} is capable of dealing
with such problems. Unfortunately, it will be slow as it has to 
generate the Jacobian by numerical
approximation unless we can provide a function to prepare the 
Jacobian analytically. Moreover,
the determination of the Jacobian is still subject to 
the unfortunate scaling issues we have
been confronting throughout this article. A better approach
would likely be via package \code{nleqslv}, which is constructed
to attempt solutions of nonlinear equations problems. 

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## Problem of order 5 using nll.g
## nleqslv: rescaled parameter mean= 0.2  with SD= 0.09352086
## Problem of order 5 using enll.g
## nleqslv: rescaled parameter mean= 0.2  with SD= 9.573944e-10
## nlmrt: rescaled parameter mean= 0.2  with SD= 1.962616e-17
## 
## Now 100 parameters using enll.g
## nleqslv: rescaled parameter mean= 0.01  with SD= 0.00574485
## nlmrt: rescaled parameter mean= 0.01  with SD= 0.005999938
\end{verbatim}
\end{kframe}
\end{knitrout}

The results with the larger problem are not acceptable. 

\section{The Rayleigh Quotient}

This is another typical sumscale problem. 
The maximal and minimal eigensolutions of a symmetric matrix $A$ 
are extrema of the Rayleigh Quotient

$ R(x) =  (x' A x)  / (x' x) $

We can also deal with generalized eigenproblems of the form 

$A x = e B x$

where B is symmetric and positive definite by using the Rayleigh Quotient

$ R_g(x) =  (x' A x)  / (x' B x) $

Once again, the objective is scaled by the parameters, this time by their 
sum of squares. Alternatively, 
we may think of requiring the \B{normalized} eigensolution, which is given as 

$ x_{normalized} = x/sqrt(x' x) $

We will first try the projected gradient method \code{spg} from \pkg{BB}. 
Below is the code, where our test uses
a matrix called the Moler matrix \cite[Appendix 1]{cnm79}. We caution that there
are faster ways to compute this matrix in \R\, \citep{RQtimes12}, where different
approaches to speed up \R\ computations are discussed. Here we are concerned 
with getting the solutions correctly rather than the speed of so doing. Note
that to get the solution with the most-positive eigenvalue, we minimize the
Rayleigh quotient of the matrix multiplied by -1. This is solution \code{tmax}.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{molerbuild}\hlkwb{<-}\hlkwa{function}\hlstd{(}\hlkwc{n}\hlstd{)\{} \hlcom{# Create the moler matrix of order n}
   \hlcom{# A[i,j] = i for i=j, min(i,j)-2 otherwise}
   \hlstd{A} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwc{nrow} \hlstd{= n,} \hlkwc{ncol} \hlstd{= n)}
   \hlstd{j} \hlkwb{<-} \hlnum{1}\hlopt{:}\hlstd{n}
   \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{n) \{}
      \hlstd{A[i,} \hlnum{1}\hlopt{:}\hlstd{i]} \hlkwb{<-} \hlkwd{pmin}\hlstd{(i,} \hlnum{1}\hlopt{:}\hlstd{i)} \hlopt{-} \hlnum{2}
   \hlstd{\}}
   \hlstd{A} \hlkwb{<-} \hlstd{A} \hlopt{+} \hlkwd{t}\hlstd{(A)}
   \hlkwd{diag}\hlstd{(A)} \hlkwb{<-} \hlnum{1}\hlopt{:}\hlstd{n}
   \hlstd{A}
\hlstd{\}}

\hlstd{raynum}\hlkwb{<-}\hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,} \hlkwc{A}\hlstd{)\{}
   \hlstd{rayquo}\hlkwb{<-}\hlkwd{as.numeric}\hlstd{((}\hlkwd{t}\hlstd{(x)}\hlopt{%*%}\hlstd{A)}\hlopt{%*%}\hlstd{x)}
\hlstd{\}}

\hlstd{proj}\hlkwb{<-}\hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{) \{ x}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlkwd{crossprod}\hlstd{(x)) \}}

\hlkwd{require}\hlstd{(BB,} \hlkwc{quietly}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\hlstd{n}\hlkwb{<-}\hlnum{10}
\hlstd{x}\hlkwb{<-}\hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,n)}
\hlstd{A}\hlkwb{<-}\hlkwd{molerbuild}\hlstd{(n)}
\hlstd{tmin}\hlkwb{<-}\hlkwd{system.time}\hlstd{(asprqmin}\hlkwb{<-}\hlkwd{spg}\hlstd{(x,} \hlkwc{fn}\hlstd{=raynum,} \hlkwc{project}\hlstd{=proj,} \hlkwc{A}\hlstd{=A))[[}\hlnum{3}\hlstd{]]}
\end{alltt}
\begin{verbatim}
## iter:  0  f-value:  205  pgrad:  3.089431e-09
\end{verbatim}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning in spg(x, fn = raynum, project = proj, A = A): convergence tolerance satisified at intial parameter values.}}\begin{alltt}
\hlstd{tmax}\hlkwb{<-}\hlkwd{system.time}\hlstd{(asprqmax}\hlkwb{<-}\hlkwd{spg}\hlstd{(x,} \hlkwc{fn}\hlstd{=raynum,} \hlkwc{project}\hlstd{=proj,} \hlkwc{A}\hlstd{=}\hlopt{-}\hlstd{A))[[}\hlnum{3}\hlstd{]]}
\end{alltt}
\begin{verbatim}
## iter:  0  f-value:  -205  pgrad:  0.6324555
\end{verbatim}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning in spg(x, fn = raynum, project = proj, A = -A): Unsuccessful convergence.}}\begin{alltt}
\hlkwd{cat}\hlstd{(}\hlstr{"maximal eigensolution: Value="}\hlstd{,asprqmax}\hlopt{$}\hlstd{value,}\hlstr{"in time "}\hlstd{,tmax,}\hlstr{"\textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## maximal eigensolution: Value= -205 in time  0.284
\end{verbatim}
\begin{alltt}
\hlkwd{print}\hlstd{(asprqmax}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
##  [1] 0.3162278 0.3162278 0.3162278 0.3162278 0.3162278 0.3162278 0.3162278
##  [8] 0.3162278 0.3162278 0.3162278
\end{verbatim}
\begin{alltt}
\hlkwd{cat}\hlstd{(}\hlstr{"minimal eigensolution: Value="}\hlstd{,asprqmin}\hlopt{$}\hlstd{value,}\hlstr{"in time "}\hlstd{,tmin,}\hlstr{"\textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## minimal eigensolution: Value= 205 in time  0.042
\end{verbatim}
\begin{alltt}
\hlkwd{print}\hlstd{(asprqmin}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
##  [1] 0.3162278 0.3162278 0.3162278 0.3162278 0.3162278 0.3162278 0.3162278
##  [8] 0.3162278 0.3162278 0.3162278
\end{verbatim}
\end{kframe}
\end{knitrout}

For the record, these results compare well with eigenvalues from eigen().

If we ignore the constraint, and simply perform the optimization, we can
get satisfactory solutions, though comparisons require that we normalize 
the parameters (the eigenvector components) post-optimization. 
We can check if the scale of the eigenvectors 
is becoming large by computing the norm of the final parameter vector. In 
tests on the Moler matrix up to dimension 100, none grew to a worrying size. 

For comparison, we also ran a specialized Geradin routine as implemented in \R\ by one of 
us (JN). This gave equivalent answers, albeit more efficiently. For those interested, the
Geradin routine is available as referenced in \citep{RQtimes12}.


\section{The R-help example}

As a final example, let us use our present techniques to solve the 
problem posed by Lanh Tran on R-help. We will use
only a method that scales the parameters directly inside the objective function and 
not bother with gradients for this small problem. 

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{ssums}\hlkwb{<-}\hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)\{}
  \hlstd{n}\hlkwb{<-}\hlkwd{length}\hlstd{(x)}
  \hlstd{tt}\hlkwb{<-}\hlkwd{sum}\hlstd{(x)}
  \hlstd{ss}\hlkwb{<-}\hlnum{1}\hlopt{:}\hlstd{n}
  \hlstd{xx}\hlkwb{<-}\hlstd{(x}\hlopt{/}\hlstd{tt)}\hlopt{*}\hlstd{(x}\hlopt{/}\hlstd{tt)}
  \hlkwd{sum}\hlstd{(ss}\hlopt{*}\hlstd{xx)}
\hlstd{\}}

\hlkwd{cat}\hlstd{(}\hlstr{"Try penalized sum\textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Try penalized sum
\end{verbatim}
\begin{alltt}
\hlstd{st}\hlkwb{<-}\hlkwd{runif}\hlstd{(}\hlnum{3}\hlstd{)}
\hlstd{aos}\hlkwb{<-}\hlkwd{opm}\hlstd{(st, ssums,} \hlkwc{gr}\hlstd{=}\hlstr{"grcentral"}\hlstd{,} \hlkwc{method}\hlstd{=mset)}
\hlcom{# rescale the parameters}
\hlstd{nsol}\hlkwb{<-}\hlkwd{dim}\hlstd{(aos)[}\hlnum{1}\hlstd{]}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{nsol)\{}
  \hlstd{tpar}\hlkwb{<-}\hlstd{aos[i,}\hlnum{1}\hlopt{:}\hlnum{3}\hlstd{]}
  \hlstd{ntpar}\hlkwb{<-}\hlkwd{sum}\hlstd{(tpar)}
  \hlstd{tpar}\hlkwb{<-}\hlstd{tpar}\hlopt{/}\hlstd{ntpar}
\hlcom{#  cat("Method ",aos[i, "meth"]," gives fval =", ssums(tpar))}
  \hlstd{aos[i,} \hlnum{1}\hlopt{:}\hlnum{3}\hlstd{]}\hlkwb{<-}\hlstd{tpar}

\hlstd{\}}

\hlkwd{summary}\hlstd{(aos,}\hlkwc{order}\hlstd{=value)[}\hlnum{1}\hlopt{:}\hlnum{5}\hlstd{,]}
\end{alltt}
\begin{verbatim}
##                 p1        p2        p3     value fevals gevals convergence
## Rvmmin   0.5454546 0.2727273 0.1818182 0.5454545     24      9           0
## nlminb   0.5454545 0.2727273 0.1818182 0.5454545      8      8           0
## Rcgmin   0.5454545 0.2727273 0.1818182 0.5454545     18      8           0
## nlm      0.5454545 0.2727273 0.1818181 0.5454545     NA      6           0
## L-BFGS-B 0.5454546 0.2727271 0.1818182 0.5454545      8      8           0
##          kkt1  kkt2 xtime
## Rvmmin   TRUE FALSE 0.000
## nlminb   TRUE FALSE 0.000
## Rcgmin   TRUE FALSE 0.000
## nlm      TRUE FALSE 0.004
## L-BFGS-B TRUE FALSE 0.000
\end{verbatim}
\end{kframe}
\end{knitrout}



\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{ssum}\hlkwb{<-}\hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)\{}
  \hlstd{n}\hlkwb{<-}\hlkwd{length}\hlstd{(x)}
  \hlstd{ss}\hlkwb{<-}\hlnum{1}\hlopt{:}\hlstd{n}
  \hlstd{xx}\hlkwb{<-}\hlstd{x}\hlopt{*}\hlstd{x}
  \hlkwd{sum}\hlstd{(ss}\hlopt{*}\hlstd{xx)}
\hlstd{\}}
\hlstd{proj.simplex} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{y}\hlstd{) \{}
\hlcom{# project an n-dim vector y to the simplex Dn}
\hlcom{# Dn = \{ x : x n-dim, 1 >= x >= 0, sum(x) = 1\}}
\hlcom{# Ravi Varadhan, Johns Hopkins University}
\hlcom{# August 8, 2012}

\hlstd{n} \hlkwb{<-} \hlkwd{length}\hlstd{(y)}
\hlstd{sy} \hlkwb{<-} \hlkwd{sort}\hlstd{(y,} \hlkwc{decreasing}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\hlstd{csy} \hlkwb{<-} \hlkwd{cumsum}\hlstd{(sy)}
\hlstd{rho} \hlkwb{<-} \hlkwd{max}\hlstd{(}\hlkwd{which}\hlstd{(sy} \hlopt{>} \hlstd{(csy} \hlopt{-} \hlnum{1}\hlstd{)}\hlopt{/}\hlstd{(}\hlnum{1}\hlopt{:}\hlstd{n)))}
\hlstd{theta} \hlkwb{<-} \hlstd{(csy[rho]} \hlopt{-} \hlnum{1}\hlstd{)} \hlopt{/} \hlstd{rho}
\hlkwd{return}\hlstd{(}\hlkwd{pmax}\hlstd{(}\hlnum{0}\hlstd{, y} \hlopt{-} \hlstd{theta))}
\hlstd{\}}
\hlstd{as}\hlkwb{<-}\hlkwd{spg}\hlstd{(st, ssum,} \hlkwc{project}\hlstd{=proj.simplex)}
\end{alltt}
\begin{verbatim}
## iter:  0  f-value:  2.067073  pgrad:  0.48634
\end{verbatim}
\begin{alltt}
\hlkwd{cat}\hlstd{(}\hlstr{"Using project.simplex with spg: fmin="}\hlstd{,as}\hlopt{$}\hlstd{value,}\hlstr{" at \textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Using project.simplex with spg: fmin= 0.5454545  at
\end{verbatim}
\begin{alltt}
\hlkwd{print}\hlstd{(as}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
## [1] 0.5454579 0.2727254 0.1818168
\end{verbatim}
\end{kframe}
\end{knitrout}

Apart from the parameter rescaling, this is an entirely "doable" problem. 
Note that we can also solve the problem as a Quadratic Program using
the \pkg{quadprog} package.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(quadprog)}
\hlstd{Dmat}\hlkwb{<-}\hlkwd{diag}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{,}\hlnum{3}\hlstd{))}
\hlstd{Amat}\hlkwb{<-}\hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{1}\hlstd{),} \hlkwc{ncol}\hlstd{=}\hlnum{1}\hlstd{)}
\hlstd{bvec}\hlkwb{<-}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{)}
\hlstd{meq}\hlkwb{=}\hlnum{1}
\hlstd{dvec}\hlkwb{<-}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{)}
\hlstd{ans}\hlkwb{<-}\hlkwd{solve.QP}\hlstd{(Dmat, dvec, Amat, bvec,} \hlkwc{meq}\hlstd{=}\hlnum{0}\hlstd{,} \hlkwc{factorized}\hlstd{=}\hlnum{FALSE}\hlstd{)}
\hlstd{ans}
\end{alltt}
\begin{verbatim}
## $solution
## [1] 0.5454545 0.2727273 0.1818182
## 
## $value
## [1] 0.2727273
## 
## $unconstrained.solution
## [1] 0 0 0
## 
## $iterations
## [1] 2 0
## 
## $Lagrangian
## [1] 0.5454545
## 
## $iact
## [1] 1
\end{verbatim}
\end{kframe}
\end{knitrout}

\section{Recommendations}

Despite the relatively limited experience above, it is nevertheless incumbent on us to 
recommend to \R users how to approach these problems. 

\subsection{Application of the ideas to a new example}

Let us try to solve the following problem:

Maximize the product of $n$ parameters such that the scaled sum of squares is fixed to value $A$.
That is, 

$\sum_{i=1}^n{i*{x_i}^2} = A $

We will set $A = 1$, but other values will simply scale the objective function.
Defining 

$ scale({\B{x})} = \sqrt{(\sum_{i=1}^n{i*{x_i}^2})}  $

we want to maximize 

$ \prod{\B{x}} / scale({\B{x})} $

As before, we can \B{minimize}

$  - \sum_{i=1}^n{log(x_i)} + log(scale({\B{x})}) $

Moreover we could work with logarithms of parameters to avoid non-positive inputs to the objective function.


\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{scale} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{) \{}\hlkwd{sum}\hlstd{( (}\hlnum{1}\hlopt{:}\hlstd{n)} \hlopt{*} \hlstd{x}\hlopt{^}\hlnum{2}\hlstd{)\}}
\hlstd{enew} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)\{}
  \hlstd{n} \hlkwb{<-} \hlkwd{length}\hlstd{(x)}
\hlcom{##  x <- exp(lx)}
  \hlstd{sc} \hlkwb{<-} \hlkwd{sqrt}\hlstd{(}\hlkwd{scale}\hlstd{(x))}
  \hlstd{obj} \hlkwb{<-} \hlopt{-} \hlkwd{sum}\hlstd{(}\hlkwd{log}\hlstd{(x}\hlopt{/}\hlstd{sc))}
\hlcom{#  obj <- n*log(sc)-sum(log(x))}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(optimrx)}
\hlstd{n} \hlkwb{<-} \hlnum{5}
\hlstd{mmth}\hlkwb{<-}\hlkwd{c}\hlstd{(}\hlstr{"Rvmmin"}\hlstd{,} \hlstr{"Rcgmin"}\hlstd{)}
\hlstd{mset}\hlkwb{<-}\hlkwd{c}\hlstd{(}\hlstr{"Rcgmin"}\hlstd{,} \hlstr{"ucminf"}\hlstd{,} \hlstr{"Nelder-Mead"}\hlstd{)}
\hlstd{strt} \hlkwb{<-} \hlstd{(}\hlnum{1}\hlopt{:}\hlstd{n)}\hlopt{/}\hlstd{(}\hlnum{2}\hlopt{*}\hlstd{n)}
 \hlstd{lo} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlopt{-}\hlnum{100}\hlstd{, (n}\hlopt{-}\hlnum{1}\hlstd{)),strt[n])}
 \hlstd{up} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{100}\hlstd{, (n}\hlopt{-}\hlnum{1}\hlstd{)),strt[n])}
 \hlstd{amsk2} \hlkwb{<-} \hlkwd{opm}\hlstd{(strt, enew,} \hlkwc{gr}\hlstd{=}\hlstr{"grcentral"}\hlstd{,} \hlkwc{lower}\hlstd{=lo,} \hlkwc{upper}\hlstd{=up,} \hlkwc{method}\hlstd{=mmth)}
\end{alltt}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning in bmchk(par, lower = lower, upper = upper): Masks (fixed parameters) set by bmchk due to tight bounds. CAUTION!!}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in bmchk(par, lower = lower, upper = upper): Masks (fixed parameters) set by bmchk due to tight bounds. CAUTION!!}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in log(x/sc): NaNs produced}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in bmchk(par, lower = lower, upper = upper): Masks (fixed parameters) set by bmchk due to tight bounds. CAUTION!!}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in log(x/sc): NaNs produced}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in Rcgminb(par = spar, fn = efn, gr = egr, lower = slower, upper = supper, : Rcgmin - undefined function}}\begin{alltt}
\hlcom{## amsk2 <- opm(strt, enew, gr="grcentral", method=mmth)}
\hlstd{amsk2} \hlkwb{<-} \hlkwd{summary}\hlstd{(amsk2,} \hlkwc{order}\hlstd{=value)}
\hlkwd{print}\hlstd{(amsk2)}
\end{alltt}
\begin{verbatim}
##              p1        p2        p3       p4  p5    value fevals gevals
## Rcgmin 1.118034 0.7905694 0.6454971 0.559017 0.5 6.417341     39     18
## Rvmmin 0.100000 0.2000000 0.3000000 0.400000 0.5 8.752759      2      1
##        convergence  kkt1  kkt2 xtime
## Rcgmin           0  TRUE FALSE 0.004
## Rvmmin          21 FALSE FALSE 0.004
\end{verbatim}
\begin{alltt}
\hlstd{parm2} \hlkwb{<-} \hlstd{amsk2[}\hlnum{1}\hlstd{,} \hlnum{1}\hlopt{:}\hlstd{n]}
\hlkwd{print}\hlstd{(parm2)}
\end{alltt}
\begin{verbatim}
##              p1        p2        p3       p4  p5
## Rcgmin 1.118034 0.7905694 0.6454971 0.559017 0.5
\end{verbatim}
\begin{alltt}
\hlstd{parm2} \hlkwb{<-} \hlkwd{as.numeric}\hlstd{(parm2}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlkwd{scale}\hlstd{(parm2)))}
\hlcom{## parm2e<-as.numeric(exp(parm2))}
\hlkwd{print}\hlstd{(parm2)}
\end{alltt}
\begin{verbatim}
## [1] 0.4472136 0.3162278 0.2581989 0.2236068 0.2000000
\end{verbatim}
\begin{alltt}
\hlkwd{cat}\hlstd{(}\hlstr{"enew(parm2)="}\hlstd{,} \hlkwd{enew}\hlstd{(parm2),}\hlstr{"\textbackslash{}n"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## enew(parm2)= 6.417341
\end{verbatim}
\begin{alltt}
\hlkwd{print}\hlstd{(}\hlkwd{scale}\hlstd{(parm2))}
\end{alltt}
\begin{verbatim}
## [1] 1
\end{verbatim}
\begin{alltt}
\hlstd{areg}\hlkwb{<-} \hlkwd{opm}\hlstd{(strt, enew,} \hlkwc{gr}\hlstd{=}\hlstr{"grnd"}\hlstd{,} \hlkwc{method}\hlstd{=mset)}
\end{alltt}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning in log(x/sc): NaNs produced}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in Rcgminu(par = spar, fn = efn, gr = egr, control = mcontrol, ...): Rcgmin - undefined function}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in log(x/sc): NaNs produced}}

{\ttfamily\noindent\color{warningcolor}{\#\# Warning in Rcgminu(par = spar, fn = efn, gr = egr, control = mcontrol, ...): Rcgmin - undefined function}}\begin{alltt}
\hlstd{areg}
\end{alltt}
\begin{verbatim}
##                    p1        p2        p3        p4        p5    value
## Rcgmin      0.5707154 0.4035568 0.3295027 0.2853577 0.2552317 6.417341
## ucminf      1.1141731 0.7878393 0.6432681 0.5570865 0.4982733 6.417341
## Nelder-Mead 0.6300243 0.4449968 0.3629333 0.3145654 0.2812454 6.417344
##             fevals gevals convergence kkt1  kkt2 xtime
## Rcgmin          32     15           0 TRUE FALSE 0.012
## ucminf          15     15           0 TRUE FALSE 0.008
## Nelder-Mead    216     NA           0 TRUE FALSE 0.004
\end{verbatim}
\begin{alltt}
\hlkwa{for} \hlstd{(ii} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{dim}\hlstd{(areg)[}\hlnum{1}\hlstd{])\{}
    \hlstd{prm} \hlkwb{<-} \hlstd{areg[ii,}\hlnum{1}\hlopt{:}\hlnum{5}\hlstd{]}
    \hlkwd{print}\hlstd{(prm)}
    \hlstd{prm}\hlkwb{<-}\hlkwd{as.numeric}\hlstd{(prm}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlkwd{scale}\hlstd{(prm)))}
    \hlkwd{cat}\hlstd{(}\hlstr{"scaled parameters:"}\hlstd{)}
    \hlkwd{print}\hlstd{(prm)}
    \hlkwd{cat}\hlstd{(}\hlstr{"new scale="}\hlstd{,} \hlkwd{scale}\hlstd{(prm),}\hlstr{"\textbackslash{}n"}\hlstd{)}

\hlstd{\}}
\end{alltt}
\begin{verbatim}
##               p1        p2        p3        p4        p5
## Rcgmin 0.5707154 0.4035568 0.3295027 0.2853577 0.2552317
## scaled parameters:[1] 0.4472136 0.3162278 0.2581989 0.2236068 0.2000000
## new scale= 1 
##              p1        p2        p3        p4        p5
## ucminf 1.114173 0.7878393 0.6432681 0.5570865 0.4982733
## scaled parameters:[1] 0.4472136 0.3162278 0.2581989 0.2236068 0.2000000
## new scale= 1 
##                    p1        p2        p3        p4        p5
## Nelder-Mead 0.6300243 0.4449968 0.3629333 0.3145654 0.2812454
## scaled parameters:[1] 0.4478024 0.3162904 0.2579621 0.2235837 0.1999008
## new scale= 1
\end{verbatim}
\end{kframe}
\end{knitrout}




\section{Conclusion}

Sumscale problems can present difficulties for optimization (or function minimization)
codes. These difficulties are by no means insurmountable, but they do require some 
attention.

While specialized approaches are "best" for speed and correctness, a general user is more
likely to benefit from a simpler approach of embedding the scaling in the objective function
and rescaling the parameters before reporting them. Another choice is to use the projected
gradient via \code{spg} from package \pkg{BB}.



%%\bibliographystyle{chicago} %The style you want to use for references.
\bibliographystyle{abbrvnat}
\bibliography{sumscale} %The files containing all the articles and 


\end{document}

