---
title: "lbfgsb3x: Using the 2011 version of L-BFGSB"
author: "John C Nash
    Telfer School of Management,
    University of Ottawa,
    nashjc@uottawa.ca"
date: "February 20, 2019"
output: 
    pdf_document
bibliography: lbfgsb3x.bib
vignette: >
  %\VignetteIndexEntry{lbfgsb3x}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract

In 2011 the authors of the L-BFGSB program published a correction
and update to their 1995 code. The latter is the basis of the L-BFGS-B
method of the `optim()` function in base-R. The package `lbfgsb3x` is
a merging of CRAN packages `lfgsb3` and `lbfgsb3c`. This vignette 
gives a brief explanation of the offerings of this package.


## Provenance of the R optim::L-BFGS-B and related solvers

The base-R code lbfgsb.c (at writing in R-3.5.2/src/appl/) is commented:

```
/* l-bfgs-b.f -- translated by f2c (version 19991025).

  From ?optim:
  The code for method ‘"L-BFGS-B"’ is based on Fortran code by Zhu,
  Byrd, Lu-Chen and Nocedal obtained from Netlib (file 'opt/lbfgs_bcm.shar')

  The Fortran files contained no copyright information.

  Byrd, R. H., Lu, P., Nocedal, J. and Zhu, C.  (1995) A limited
  memory algorithm for bound constrained optimization.
  \emph{SIAM J. Scientific Computing}, \bold{16}, 1190--1208.
*/
```
The paper @Byrd95 builds on @Lu94limitedmemory. There have been a number
of other workers who have followed-up on this work, but **R** code and
packages seem to have largely stayed with codes derived from these original
papers. Though the date of the paper is 1995, the ideas it embodies were
around for a decade and a half at least, in particular in Nocedal80 and
LiuN89. The definitive Fortran code was published as @Zhu1997LBFGS. This
is available as `toms/778.zip` on www.netlib.org. A side-by-side comparison of the
main subroutines in the two downloads from Netlib unfortunately shows a lot of
differences. I have not tried to determine if these affect performance or are
simply cosmetic. 

More seriously perhaps, there were some deficiencies in the code(s), and in 2011 
Nocedal's team published a Fortran code with some corrections (@Morales2011).
Since the **R** code predates this, I prepared package `lbfgsb3` (@lbfgsb3JN) to wrap
the Fortran code. However, I did not discover any test cases where the
`optim::L-BFGS-B` and `lbfgsb3` were different, though I confess to only
running some limited tests. There are, in fact, more in this vignette.

In 2016, I was at a Fields Institute optimization conference in Toronto
for the 70th birthday of Andy Conn. By sheer serendipity, Nocedal did not attend 
the conference, but
sat down next to me at the conference dinner. When I asked him about the key changes,
he said that the most important one was to fix the computation of the machine 
precision, which was not always correct in the 1995 code. Since **R** gets this 
number as `.Machine$double.eps`, the offending code is irrelevant. 

Within @Morales2011, there is also reported an improvement in the subspace
minimization that is applied in cases of bounds constraints. Since few of the
tests I have applied imporse such constraints, it is reasonable that I will 
not have observed performance differences between the base-R `optim` code
and my `lbfsgb3` package. More appropriate tests are welcome, and on my agenda.

Besides the ACM TOMS code, there are two related codes from the Northwestern team on NETLIB:
http://netlib.org/opt/lbfgs_um.shar
is for unconstrained minimization, while http://netlib.org/opt/lbfgs_bcm.shar handles bounds
constrained problems. To these are attached references @LiuN89 and @Byrd1995 respectively,
most likely reflecting the effort required to implement the constraints.

The unconstrained code has been converted to **C** under the leadership of 
Naoaki Okazaki (see http://www.chokkan.org/software/liblbfgs/, or the fork at  https://github.com/MIRTK/LBFGS). This has been wrapped for **R** as @Coppola2014 as the
`lbfgs` package. This can be called from `optimx::optimr()`. 

Using Rcpp (see @RCppDERF2011) and the Fortran code in package `lbfgs3`, Matthew Fidler 
developed package `lbfgsb3c` (@lbfgsb3cMF). As this provides a more standard call and 
return than `lbfgsb3` Fidler and I are unified the two packages as `lbfgsb3x`.

## Functions in package `lbfgsb3x`

There are four optimizer functions in the package:

- `lbfgsb3`, which uses a `.Fortran` call of the compiled 2011 Fortran code. The 
object returned by this routine is NOT equivalent to the object returned by 
base-R `optim()` or by `optimx::optimr()`. Instead, it includes a structure `info`
which contains the detailed diagnostic information of the Fortran code. For most
users, this is not of interest, and I only recommend use of this function for those
needing to examine how the optimization has been carried out.
- `lbfgsb3f()` wraps `lbfsgb3()` to produce a returned object of the same form as
`optim()`.
- `lbfgsb3c()` uses Rcpp (@RCppDE2013, @RCppDERF2011, @RCppDEJJB2017) to streamline the call to the underlying Fortran.
- `lbfgsb3x()` is an alias of `lbfgsb3c()`

We recommend using the `lbfsgb3c()` call for most uses.

## Comparison with optim::L-BFGS-B

The new Fortran package claims better performance on bounds-constrained problems. Below we
present two fairly simple tests, which unfortunately do not show any advantage. We welcome 
examples showing differences, either better or not. 

```{r, bt, echo=TRUE}
# ref BT.RES in Nash and Walker-Smith (1987)

bt.f<-function(x){
 sum(x*x)
}

bt.g<-function(x){
  gg<-2.0*x
}

bt.badsetup<-function(n){
   x<-rep(0,n)
   lower<-rep(0,n)
   upper<-lower # to get arrays set
   bdmsk<-rep(1,n)
   bdmsk[(trunc(n/2)+1)]<-0
   for (i in 1:n) { 
      x[i]<-2.2*i-n
      lower[i]<-1.0*(i-1)*(n-1)/n
      upper[i]<-1.0*i*(n+1)/n
   }
   result<-list(x=x, lower=lower, upper=upper, bdmsk=bdmsk)
}

bt.setup0<-function(n){
   x<-rep(0,n)
   lower<-rep(0,n)
   upper<-lower # to get arrays set
   bdmsk<-rep(1,n)
   bdmsk[(trunc(n/2)+1)]<-0
   for (i in 1:n) { 
      lower[i]<-1.0*(i-1)*(n-1)/n
      upper[i]<-1.0*i*(n+1)/n
   }
   x<-0.5*(lower+upper)
   result<-list(x=x, lower=lower, upper=upper, bdmsk=bdmsk)
}

nn <- 4
baddy <- bt.badsetup(nn)
lo <- baddy$lower
up <- baddy$up
x0 <- baddy$x
baddy
## optim()
solbad0 <- optim(x0, bt.f, bt.g, lower=lo, upper=up, method="L-BFGS-B", control=list(trace=3))
library(lbfgsb3x)
sol3c <- lbfgsb3x(x0, bt.f, bt.g, lower=lo, upper=up, control=list(trace=3))
library(microbenchmark)
tbad0 <- microbenchmark(optim(x0, bt.f, bt.g, lower=lo, upper=up, method="L-BFGS-B"))
t3c <- microbenchmark(lbfgsb3x(x0, bt.f, bt.g, lower=lo, upper=up))
tbad0
t3c
library(optimx)
meths <- c("L-BFGS-B", "lbfgsb3c") # Note: lbfgsb3x not yet in optimx
allbt0 <- opm(x0, bt.f, bt.g, lower=lo, upper=up, method=meths)
summary(allbt0, order=value)

```

```{r, candlestick}
# candlestick function
# J C Nash 2011-2-3
cstick.f<-function(x,alpha=100){
  x<-as.vector(x)
  r2<-crossprod(x)
  f<-as.double(r2+alpha/r2)
  return(f)
}

cstick.g<-function(x,alpha=100){
  x<-as.vector(x)
  r2<-as.numeric(crossprod(x))
  g1<-2*x
  g2 <- (-alpha)*2*x/(r2*r2)
  g<-as.double(g1+g2)
  return(g)
}

nn <- 2
x0 <- c(10,10)
lo <- c(1, 1)
up <- c(10,10)

meths <- c("L-BFGS-B", "lbfgsb3c", "Rvmmin", "Rcgmin", "Rtnmin")
require(optimx)
print(x0)
cstick2a <- opm(x0, cstick.f, cstick.g, method=meths, upper=up, lower=lo, control=list(kkt=FALSE))
print(summary(cstick2a, par.select=1:2, order=value))
lo <- c(4, 4)
cstick2b <- opm(x0, cstick.f, cstick.g, method=meths, upper=up, lower=lo, control=list(kkt=FALSE))
print(summary(cstick2b, par.select=1:2, order=value))

nn <- 100
x0 <- rep(10, nn)
up <- rep(10, nn)
lo <- rep(1e-4, nn)
cstickc0 <- opm(x0, cstick.f, cstick.g, method=meths, upper=up, lower=lo, control=list(kkt=FALSE))
print(summary(cstickc0, par.select=1:5, order=value))
lo <- rep(1, nn)
cstickca <- opm(x0, cstick.f, cstick.g, method=meths, upper=up, lower=lo, control=list(kkt=FALSE))
print(summary(cstickca, par.select=1:5, order=value))
lo <- rep(4, nn)
cstickcb <- opm(x0, cstick.f, cstick.g, method=meths, upper=up, lower=lo, control=list(kkt=FALSE))
print(summary(cstickcb, par.select=1:5, order=value))
```

```{r,exrosen}
# require(funconstrain) ## not in CRAN, so explicit inclusion of this function
# exrosen <- ex_rosen()
# exrosenf <- exrosen$fn
exrosenf <- function (par) {
    n <- length(par)
    if (n%%2 != 0) {
        stop("Extended Rosenbrock: n must be even")
    }
    fsum <- 0
    for (i in 1:(n/2)) {
        p2 <- 2 * i
        p1 <- p2 - 1
        f_p1 <- 10 * (par[p2] - par[p1]^2)
        f_p2 <- 1 - par[p1]
        fsum <- fsum + f_p1 * f_p1 + f_p2 * f_p2
    }
    fsum
}
# exroseng <- exrosen$gr
exroseng <- function (par) {
    n <- length(par)
    if (n%%2 != 0) {
        stop("Extended Rosenbrock: n must be even")
    }
    grad <- rep(0, n)
    for (i in 1:(n/2)) {
        p2 <- 2 * i
        p1 <- p2 - 1
        xx <- par[p1] * par[p1]
        yx <- par[p2] - xx
        f_p1 <- 10 * yx
        f_p2 <- 1 - par[p1]
        grad[p1] <- grad[p1] - 400 * par[p1] * yx - 2 * f_p2
        grad[p2] <- grad[p2] + 200 * yx
    }
    grad
}

exrosenx0 <- function (n = 20) {
    if (n%%2 != 0) {
        stop("Extended Rosenbrock: n must be even")
    }
    rep(c(-1.2, 1), n/2)
}

meths <- c("L-BFGS-B", "lbfgsb3c", "Rvmmin", "Rcgmin", "Rtnmin")
require(optimx)
for (n in seq(2,12, by=2)) {
  cat("ex_rosen try for n=",n,"\n")
  x0 <- exrosenx0(n)
  lo <- rep(.5, n)
  up <- rep(3, n)
  print(x0)
  erfg <- opm(x0, exrosenf, exroseng, method=meths, lower=lo, upper=up)
  print(summary(erfg, par.select=1:2, order=value))
}


```



## References
