{
    "collab_server" : "",
    "contents" : "gradminu<-function(x0, fn, gr, hess, lower = NULL, upper = NULL, \n      control=list(),...) {\n## General (unconstrained) gradient minimizer \n##\n##Input\n##       - fn is the function we wish to minimize\n##?? fixup documentation here??\n##       - x0 is the initial value\n##       - ... is data used in the function fn\n##Output (list) -- need to match optim() output!! ???\n##       - xs is the value at the minimum\n##       - fv is the fn evaluated at xs\n##       - grd is the gradient\n##       - Hess is the Hessian\n##       - niter is the number of interations needed (gradient and Hessian evals).\n##       - add fevals??, other reports\n\n# Need a way to pass any object defined at this level into routines called\n# But not necessarily to define them WITHIN this routine\n  \n  \nnpar <- length(x0)\nnf <- ng <- nh <- niter <- 0 # counters\n\n# set up workspace\nws <- list(\n  lsmeth = \"default\",\n  solver = \"default\",\n  trace = 0,\n  maxit = 500,\n  maxfevals = npar*500,\n  laminc = 10,\n  lamdec = 0.4,\n  lamstart = 0.01,\n  acctol = 0.0001,\n  epstol = .Machine$double.eps,\n  svmin = 0.0,\n  stepdec = 0.2, \n  stepmax = 5,\n  stepmin = 0,\n  smult = 1.5,\n  fmult = -0.25,\n  offset = 100.0,\n  defstep=1,\n  bigval = .Machine$double.xmax*0.01\n)  \n\nncontrol <- names(control)\nnws <- names(ws)\nfor (onename in ncontrol) {\n  if (! (onename %in% nws)) {\n    ws[onename]<-control[onename]\n  }\n}\n\ncat(\"Workspace ws:\")\nprint(ws)\n\nlnsrch <- lsnone # default to unit step\nif (ws$lsmeth == \"lsback\") {lnsrch <- lsback}\nelse {stop(\"Undefined lsmeth = \",lsmeth)}\n\nlnsrch <- pracma::fminbnd\nprint(lnsrch)\ntmp <- readline(\"done\")\n\nif (ws$lsmeth == \"default\") {\n  lnsrch<-function(fn, fbest, xc, d, grv, ...) { # Line search using internal optimize()\n    cat(\"fn:\\n\")\n    print(fn)\n    ## Uses Brent's method to find the best stepsize in interval\n    # fbest is best function value so far. NOT used.\n    # grv is numeric gradient vector -- NOT used\n    # ?? more documentation\n    flsch<-function(st) {\n      # computes the function value at stepsize st on line (xc + gm*d)\n      # Essentially flsch(st)\n      # gm: step size\n      # fn: objective function\n      # xc: base set of parameters\n      # d : search direction\n#      nf <- nf +1\n      fval<-fn(xc+st*d,...)\n      fval\n    }\n    cat(\"function at ends of interval\\n\")\n    sta <- ws$stepmin\n    cat(\"f(\",sta,\")=\", flsch(sta),\"\\n\")\n    stb <- ws$stepmax\n    cat(\"f(\",stb,\")=\", flsch(stb),\"\\n\")\n    \n    #  lout<-optimize(flsch,interval=c(ws$stepmin, ws$stepmax),\n    #                  lower=ws$stepmin, upper=ws$stepmax,...)\n    # note fmin rather than objective in return  \n    lout<-pracma::fminbnd(flsch,ws$stepmin, ws$stepmax, ...)\n    cat(\"lnsrch lout:\")\n    print(lout)\n    rlout <- lout$xmin\n    #  cat(\"structure of rlout\")\n    #  print(str(rlout))\n    attr(rlout, \"Fval\") <- lout$fmin\n    attr(rlout, \"fcount\") <- (lout$niter + 1) # fevals is iterations + 1\n    rlout # Note: returns stepsize, not x\n  } # end default line search\n} else if (ws$lsmeth == \"backtrack\") {\n} else if (ws$lsmeth == \"none\") { \n   lnsrch <- function(fn, fbest, xc, d, grv, ...) {\n      rlout <- 1 # Does nothing! \n      attr(rlout, \"Fval\") <- fbest\n      rlout\n   }\n}\n  lambda<-ws$lamstart ## ?? do better\n  niter <- 1\n  xb <- x0 # best so far\n  fbest <- fn(xb, ...)\n  nf <- nf + 1\n  newH <- TRUE\n#  while (niter < ws$maxit) { # main loop\n  repeat {\n    if (newH) {\n        if (ws$trace > 0) {cat(\"Iteration \",niter,\":\")}\n        grd<-gr(xb,...)\n        ng <- ng + 1\n        H<-hess(xb,...)\n        nh <- nh + 1\n    }\n    cat(\"Termination test:\")    \n    halt <- FALSE # default is keep going\n    # tests on too many counts??\n    if (niter > ws$maxit) {\n        if (ws$trace > 0) cat(\"Too many (\",niter,\" iterations\\n\")\n        halt <- TRUE\n        convcode <- 1\n        break\n    }\n    cat(\"tt nf=\",nf,\"\\n\")\n    if (nf > ws$maxfevals){\n    if (ws$trace > 0) cat(\"Too many (\",nf,\" function evaluations\\n\")\n        halt <- TRUE\n        convcode <- 91 # ?? value\n        break\n    }\n    #    if (ng > ws$maxgevals){} # not implemented\n    #    if (nh > ws$maxhevals){} # not implemented\n    gmax <- max(abs(grd))\n    if (gmax <= ws$epstol) {\n      if (ws$trace > 0) cat(\"Small gradient norm\",gmax,\"\\n\")\n      halt <- TRUE\n      convcode <- 0 # OK\n      break\n    }\n    if (ws$solver == \"default\") {\n      stp<-try(solve(H, -grd))\n      if (class(stp) == \"class-error\") {\n          stop(\"Failure of default solve of Newton equations\")\n      }\n    } else if (ws$solver == \"marquardt\") {\n       Haug<-H + (diag(H)+1.0)*lambda # To avoid singularity\n       stp <- solve(Haug, -grd)\n    }\n    if (ws$trace > 0) {\n         cat(\"Search vector:\")\n         print(stp)\n    }\n    gprj <- as.numeric(crossprod(stp, grd))\n    cat(\"Gradient projection = \",gprj)\n    tmp <- readline(\"   continue?\")\n    ## Do line search\n    gvl<-lnsrch(fn,fbest, xb, stp, grv=NULL, ...)\n# lnsrch<-function(fn, fbest, xc,d,grv, ...)\n    print(str(gvl))\n    fval <- attr(gvl,\"Fval\")\n    nf <- nf + attr(gvl, \"fcount\")\n    if (ws$trace > 0) {cat(\" step =\", gvl,\"  fval=\",fval ,\" nf=\",nf,\"\\n\")}\n    xn<-xb+gvl*stp\n    if (niter >= ws$maxit) {\n      print(\"NewtonR: Failed to converge!\")\n      return(0)\n    }\n    if (ws$solver == \"marquardt\"){\n       if (fval <= fbest) {\n          xb <- xn\n          fbest <- fval\n          lambda <- lambda * ws$lamdec\n          newH <- TRUE # ensure we start over\n       } else {\n          newH <- FALSE # don't want new H, grd\n          lambda <- lambda * ws*laminc\n       }\n    }\n    xb <- xn\n    fbest <- fval\n    niter <- niter + 1\n    tmp <- readline(\"end iteration\")   \n  } # end repeat\n  out<-NULL\n  out$xs<-xn\n  out$fv<-fval\n  out$grd<-grd\n  out$Hess<-H\n  out$niter<-niter\n  out \n}\n",
    "created" : 1492373204297.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2527228084",
    "id" : "766942F",
    "lastKnownWriteTime" : 1492380011,
    "last_content_update" : 1492380011661,
    "path" : "~/rsvnall/optimizer/pkg/gradmin/R/gradminu.R",
    "project_path" : "R/gradminu.R",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}