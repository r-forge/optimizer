---
title: "Optimization Objects"
author: "Duncan Murdoch"
output:
  rmarkdown::html_vignette:
    toc: yes
---

# Introduction

These are notes describing a proposal for "optimization objects", a format
to unify the inputs and outputs from optimization functions.

Currently base R has many different optimizers (`optim()`, `nls()`, `nlm()`, etc.)
each with different strengths, but also each with different requirements for 
inputs.  The `optimx` and `optimz` packages are an attempt to unify these
(and others) into a common interface, to make comparison of the results easier.
This proposal carries that unification further.

# General Idea

The `optimx()` function should accept as input an optimization object.  The
object should contain sufficient information about the optimization problem
to allow choice of an optimizer and construction of defaults for all necessary 
arguments for that optimizer.

The result should also be an optimization object.  In addition to the inputs, 
the result should contain information about the outcome of the optimization 
attempt, and perhaps a history of previous attempts.  (There needs to be 
some control over the level of detail here, but it should at least be possible
to contain full trace information from previous attempts.)

# Details

I am thinking about this as if the objects are implemented in S3, but
this proposal is preliminary, so all of the suggestions are tentative,
including the form of the objects.

The current idea is that there should be two kinds of objects:  
`objectiveFn` objects which represent an objective function to optimize,
and `optimization` objects which represent an optimization problem, including
the objective function, strategy for optimizing, and a record of what
has been attempted so far.

We distinguish between two views of the objective function:  the user's view, and
the optimizer's view.  For example, the user may have a function of 5 variables,
but wish to fix two of them, so the optimizer's view is of a function of
3 variables.  Or the user may have a partially linear model, so the 
nonlinear optimizer only needs to modify the nonlinear parameters, and the 
linear parameters can then by estimated by linear regression.

## The optimization object

Fields in the optimization object.  These are modelled on the arguments
to the `stats::optim` function; other optimizers may need others, so 
these can change.

* `par`:  A vector of parameter values, used to initialize the optimizer.
* `fn`:   An objective function object.
* `methods`:  A vector of names of optimization methods to try.
* `lower`, `upper`:  Bounds on the allowable range of the parameters.
* `control`:  A list of control parameters.
* `hessian`:  An indicator of whether to compute the Hessian matrix.
* `dots`:  A list of other parameters to pass to the objective function.
* `results`:  A list of results of previous attempts.

The `lower`, `upper`, `control`, and `hessian` arguments can be lists, 
with entries corresponding to the matching entry in `methods`.  Currently
these are named with matching names to methods, but perhaps this should
be done as a vector, so the same method can be included multiple times
with different `control` values.  


## The objectiveFn object

This object contains the following functions.  Typically they would share
an environment where other data could be stored.

* `fn0`:  A `function(par, ...)` which is the
   user's view of the function.
* `fn1`:  A `function(par1, ...)` which is the 
   optimizer's view of the function.
* `gr0` and `gr1`:  Corresponding functions to compute the gradients.
* `forward`: A `function(par)` that converts `par` to `par1`.
* `inverse`: A `function(par1)` that converts `par1` to `par`.


# Prototype

This is the kind of thing I am thinking of.  This only supports `optim`; the
real thing needs to be much more elaborate.

```{r}

simpleObjective <- function(fn, gr = NULL) {
  identity <- function(par) par
  structure(list(fn0 = fn, fn1 = fn, gr0 = gr, gr1 = gr, 
  	         forward = identity, inverse = identity),
  	    class = "objectiveFn")
}

optimization <- function(par, fn, gr = NULL, ...,
      methods = knownMethods,
      lower = -Inf, upper = Inf,
      control = list(), hessian = FALSE) {
	
    if (!inherits(fn, "objectiveFn"))
      fn <- simpleObjective(fn, gr)
    
    knownMethods <- c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN",
                 "Brent") 
    
    result <- list(par = par, fn = fn,  
    	           dots = list(...), lower = lower, upper = upper,
    	           control = control, hessian = hessian)
    
    if (!missing(methods))
      result$methods <- match.arg(methods, knownMethods, several.ok = TRUE)
    else
      result$methods <- knownMethods[1]
    
    structure(result, class = "optimization")
}

print.optimization <- function(x, ...) {
  cat("Optimization object.")
  if (!is.null(x$value)) {
    cat("  fn(", paste(format(x$par), collapse=","), ") = ", 
                 format(x$value),"\n", sep="")
  } else
    cat("  Not run yet.\n")
}

run.optimization <- function (x, verbose = TRUE) {
  if (is.null(x$runs))
    x$runs <- list()
  
  if (is.null(x$value))
    x$value <- do.call(x$fn$fn0, c(list(x$par), x$dots))
  
  for (method in x$methods) {
    args <- unclass(x)
    args$method <- method
    args$methods <- NULL
    args$runs <- NULL
    args$value <- NULL
    args$dots <- NULL
    args$par <- x$fn$forward(args$par)
    args$gr <- x$fn$gr1
    args$fn <- x$fn$fn1
    
    if (is.list(args$lower)) {
    	args$lower <- x$lower[[method]]
        if (is.null(args$lower)) 
          args$lower <- -Inf
    }
    if (is.list(args$upper)) {
    	args$upper <- x$upper[[method]]
    	if (is.null(args$upper))
    	  args$upper <- Inf
    }
    if (is.list(args$control) && all(names(args$control) %in% x$methods)) {
    	args$control <- x$control[[method]]
    	if (is.null(args$control))
    	  args$control <- list()
    }
    if (is.list(args$hessian)) {
    	args$hessian <- x$hessian[[method]]
    	if (is.null(args$hessian))
    	  args$hessian <- FALSE
    }
    if (verbose)
        cat("Trying method = '", method, "'\n", sep = "")
    run <- do.call("optim", c(args, x$dots))
    x$runs <- c(x$runs, list(run))
    if (run$value < x$value) {
    	x$value <- run$value
    	x$par <- x$fn$inverse(run$par)
    	if (verbose) {
    	  cat("  ")
    	  print(x)
    	}
    } else 
    	if (verbose)
    	  cat("  No improvement.\n")
  }
  if (verbose) invisible(x)
  else x
}
```

Here we try it to optimize the function $(x-1)^2 + (x-y-3)^4 + 0.0001*(y-4)^4 + 1$.  I use methods SANN followed by Nelder-Mead with a limit of 20 iterations,
just so that the problem doesn't get optimized in the very first try.
```{r}
fn <- function(arg) {
  x <- arg[1]
  y <- arg[2]
  (x-1)^2 + (x-y-3)^4 + 0.0001*(y-4)^4 + 1
}
opt <- optimization(par = c(x=0, y=10), 
		    fn = fn, methods=c("SANN", "Nelder"), 
		    control=list(maxit=20))
opt <- run.optimization(opt)
opt <- run.optimization(opt)
run.optimization(opt)
```

Here's the same example, with a transform that fixes $y$ at the value 10.

```{r}
fixparms <- function(parms, fn, gr = NULL) {
  if (!inherits(fn, "objectiveFn"))
    fn <- simpleObjective(fn, gr)
  
  fn0 <- fn$fn0
  gr0 <- fn$gr0
  
  unfixed <- is.na(parms)
  
  forward <- function(par) {
    par[unfixed]
  }
  
  inverse <- function(par1) {
    result <- parms
    result[unfixed] <- par1
    result
  }
  
  fn1 <- function(par1, ...) fn0(inverse(par1), ...)
  if (is.null(gr0)) 
    gr1 <- NULL
  else
    gr1 <- function(par1, ...) gr0(inverse(par1), ...)[unfixed]
  
  structure(list(fn0 = fn0, gr0 = gr0, fn1 = fn1, gr1 = gr1, 
       forward = forward, inverse = inverse),
            class = "objectiveFn")
}

opt <- optimization(par = c(x = -2, y = 10), 
		    fn = fixparms(c(NA, 10), fn), 
		    methods=c("SANN", "Brent", "BFGS"), 
		    lower = list(Brent = -5), 
		    upper = list(Brent = 100),
		    control=list(maxit=20))
run.optimization(opt)
```

# What else?

* We should have more examples of transformations
* More optimization methods should be supported