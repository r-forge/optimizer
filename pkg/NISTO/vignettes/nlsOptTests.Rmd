---
title: "A structure for building and running optimization and nonlinear least squares tests for **R**"
author: "John C. Nash and Paul Gilbert"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: nlsOptTests.bib
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract

This article discusses how tests for nonlinear equations, least squares or equations 
may be set up, executed and analyzed for **R**. This activity is sensitive to the 
manner in which the test functions are computed, the choice of starting values, and
the choice of measures of success. They may also be modified by changes in the solvers
for the problems in question. Indeed, it is always a concern that seemingly minor 
adjustments intended to improve such solvers may cause them to fail on particular problems
or starting values. Thus it is desirable to be able to 

- be able to quickly and easily run a specific test on all available solvers;

- be able to quickly and easily run all available tests on an updated or new solver;

- be able to report successes and failures in a manner that allows for rapid detection
of potential issues with solvers.

The last criterion requires that there be a way to save and access the results of test runs.
Because each test instance can generate quite a lot of data, and the combinatorial possibilities
of tests, solvers, and solver options such as choice of gradient approximation, saving test
information could demand considerable storage space and data management effort. We outline 
some possible approaches to the data management aspect of optimization testing.

#

## Motivation

http://www.itl.nist.gov/div898/strd/nls/nls_info.shtml presents a number of nonlinear
regression (nonlinear least squares) problems that are more or less difficult to solve
numerically. Doug Bates prepared an **R** package `NISTnls` which adapted these problems
to **R** and tested them with the `nls()` function of which he was a major author.
There are a number of other test function collections (some referenced below), 
usually with overlap in their
coverage, for nonlinear equations, nonlinear least squares and function minimization.
In some cases, such tests include bounds constraints or fixed parameters. 

In 2009, I built the incomplete package `NISTopt` to allow the same problems as in `NISTnls`
to be approached as unconstrained function minimization problems. Similarly, the Google 
Summer of Code project OptimGUI by Yixuan Qiu in 2011 attempted to provide a point-and-click
interface to aid in building and running function minimization problems in **R**, and 
developed a structure for storing such problems in a consistent manner. This vignette
is an attempt to better document such ideas. It is especially concerned with how to 
**efficiently** add, modify, run and review tests of the **R** tools to solve such problems. 

Note that the test problems used are **NOT** always good illustrations of the tasks to 
which the various
function minimization tools should be applied by general **R** users. 
However, they do suggest limits of performance of these tools. 

We note that in **R** circles, function minimization
is often called "optimization", though that generally implies that there
are also constraints. 

## Approach

We want to avoid having to write specific code to run each test. 
The package **optimrx** (@optimrx16) can call most of the function 
minimization tools in **R** and can be extended to include others, and the
functions within this package standardize the call to each of the minimization
solvers. Thus we can simplify the code to test such solvers, using only their 
method name within the calls to either `optimr()` or `opm()`. Furthermore, these
functions allow a consistent syntax for specifying analytic gradients (if available),
the built-in gradient approximation (if this is part of the solver), else one of 
several pre-defined gradient approximations callable in the **optextras** package.
At the time of writing, an effort is in process to try to similarly unify the 
nonlinear least squares tools for **R**, namely, the `nls()` function in the **stats**
package, and the tools in packages **nlmrt**, **minpack.lm** and **nls2** as well as some
new efforts. 

### Specification of problems

Code developed in the trial package **NISTopt** suggested that a given problem could be
specified by a particular name. Thus, the problem of @danwood1980, is called **DanielWood**.
If we wish to solve it using one of the nonlinear least squares solvers that use a 
syntax like that of `nls()`, then we need to provide a model formula and some data that
is consistent with this model formula. By naming the formula as `DanielWood.formula`, and
putting the data in a dataframe named `DanielWood.df`, we then need only a starting
vector of parameters to have a well-specified formula-based nonlinear least squares 
problem. Besides `nls()`, `nlxb()` from **nlmrt**, `nls2()` from **nls2** (CRAN version),
and `nls.lm` from **minpack.lm**.

Similarly, a function-based nonlinear least squares problem solvable by `nlfb()` from 
package **nlmrt** or `nlsLM` from **minpack.lm** can be specified by an **R** function
that computes the residual vector from a particular set of parameters. These functions
can also use a Jacobian function if it is available. At the time of writing, an approximate
Jacobian will be provided by these solvers if the provided argument is NULL. Work is 
in process to try to provide particular derivative approximations by a mechanism similar
to the one used in package **optimrx**. We can provide for use of such solvers by using
the name `DanielWood.res` for the residual function and `DanielWood.jac` for the Jacobian.

Finally, for function minimization, we can specify `DanielWood.f` for a general function
to be minimized, with the (optional) `DanielWood.g` for the gradient. 

Many optimization problems, including some test problems, need data other than the parameters.
This can be provided by `DanielWood.setup`. This function will, if required, create the
dataframe for nonlinear least squares solvers.

There are often several sets of starting parameters for given test problems, as well as
the possibility of generated pseudo-random vectors, especially for those problems which have
a variable number of parameters. These can be provided in a matrix called `starts` which has
named columns, since the nonlinear least squares solvers using a model formula require 
parameter names. When the problem has no model formula, we can and do provide parameter names
as `p1, p2, ..., pn`. (?? not done yet, but it is in opm() already) The specification of 
starting parameters will be part of the `.setup` function.


Note that the `.res` function can be used to specify nonlinear equations problems if the 
solution has all the residuals at zero. At the time of writing, we have not yet included
this possibility.



### Restrictions on the problems

We do not wish (at the moment) to 
include general optimization problems with general constraints.
In fact, we will limit our attention to at most bounds constrained nonlinear optimization and
nonlinear least squares problems. There are, however, a lot of 
these. By considering lower and
upper bounds that are equal, we can extend the constraints to fixed or **masked** parameters.

## A sample design

Building on the ideas of the above section, we create the example **problem file** 
`DanielWood.prb`. In this case, we provide all the elements above. Thus we have a
**set** of problems, all of which can be accessed through a single structure. Through 
some experiments, detailed in *Appendix A: Use of a data frame*, we found that one could,
if desired, store such problems in a data frame, but this seems overly complicated.
Our flat, plain text files, one per named problem, should suffice. 
Other structures are, of course, equally possible. However, we will experiment with the
current approach until we discover it cannot support our requirements. 

### The **DanielWood** problem

Let us implement, in **R**, a possible file for the 
problem of @danwood1980.

```{r cache=FALSE, echo=FALSE}
require(knitr)
# read chunk (does not run code)
read_chunk('../inst/extdata/DanielWood.prb')
```

```{r}
<<DanielWood.prb>>
```

## Reporting experience running problems

### The computing environment

The computing environment is an important aspect of computational tests. However, it is
often reported in summary and incomplete form. If we are to be thorough we should report

- the computing hardware and particular settings (clock rates, extra features, etc.)

- the operating system version and any add-ons

- the choice of floating point and related computational libraries used

- the version of the computing language processor (in our case, the **R** version)

- the versions of any packages used, and packages loaded at the time of the test

- how storage hardware is connected. While buffering may take care of this, writing to
external storage, or sending information over communications links (these may be equivalent)
may alter timings or other performance measures

- any parallelism that could alter performance

- other running processes that could alter performance.

Note that some of the above environmental factors should not alter the computed optima, nor
the counts that report the computational effort e.g., the number of function or gradient
evaluations. However, some choices will affect both these computed values and the timings to
arrive at them.

In the past, the computational environment has been reported largely as a commentary to 
published articles and reports. However, we believe that -- as far as possible -- the 
information should be gathered automatically and included in log files of test runs. 
**R** already has a useful `sessionInfo()` command that reports quite well at the level
of the programming language. 

### Reproducible tests

It is well-known that timing measures for real-world computers have variability, which 
may be surprisingly large depending on the particular context within which test computations
are performed. It is also possible that there may be non-timing variability if optimization
methods use "random" search. We will presume (possibly heroically) that efforts have been
made to eliminate factors that could result in differences in counts and other measures
of performance excepting time.

We also believe that for some tests it is useful to employ tools such as **microbenchmark**
to try to gauge the degree of variability in performance. This tool is almost certainly a
key choice in measuring the impact of any instrumentation code we embed in our tests, such
as the function, gradient, residual or jacobian counts mentioned below.

### Measures of computational effort

Running the function `opm()` with `method = "ALL"` very quickly reveals that different solvers
return different measures of effort. `opm()` is set up to report counts of function and 
gradient evaluations. For some methods, these are not returned. Sometimes other measures, 
such as "iterations" are reported. This makes it difficult to compare reported outcomes across
methods.

In response to this issue, we could insert counts into the various test function components.
This will, of course, have an effect on reported timings, so some measure of that effort 
should be taken from time to time. The effect should be small. It may be sensitive, however,
to the computational environment of the previous section. At the time of writing, we have
assigned a low priority to investigating the effect of instrumentation code on the overall
performance, as we believe it is relatively small.

### What information should be saved?

Ideally, a test run should record very complete information. This includes:

- the hardware computational environment

- the operating environment, that is, operating system and relevane computational libraries

- the R version and package versions and other packages loaded

- the test function and derivative calculations used

- the solver and any settings or tolerances

- measures of effort from the solver

- measures of effort by direct count

- the trajectory of the computations, including all evaluations of the function used
for derivatives (sometimes the parameter inputs to derivative computations violate constraints,
giving failures for which the reported errors may be difficult to interpret)

- the reported solution or termination point

### What information should be reported?

The above information will be far too voluminous to report. We therefore need tools to 
render summaries at various levels. 

For many situations, it will be sufficient and desirable to have a "yes/no" or "red/yellow/green"
indicator of success or failure for a specified method and given test problem. This may, in
fact, be the most common report.

In the numerical analysis literature, it has become quite popular to present **performance 
profiles** of various types. 

### Automated collection from diverse sources

Assuming there is an established set of test functions along with a similarly prescribed 
testing code, it is desirable to allow diverse workers to carry out computations and submit
reports to a (likely centralized) data repository. This requires:

- a standardized report format, which hopefully is embedded in the tool that runs the
test computations

- a secure method to accept reports into the data repository;

- a streamlined approach to allow new contributors to join the group submitting test results;

- a mechanism to test the submissions for acceptable quality;

- tools to extract results, and organize, analyze, summarize or visualize them; and

- tools to automatically report unexpected or unwanted results. For example we would like
to know if a new version of a package causes failures on some test functions.

### Managing the test output

The previous sub-section has listed some desiderata for handling test output. Details that
will be the burden of the work remain. What are key obstacles to success?

First, we will need to deal with volume of data, particularly if trajectories are important.
There are, of course, many choices for how to store the data. A database could be used, but
imposes the choice of software to run it, as well as potential operating system issues if the
overall project needs to be replicated or moved. A simpler arrangement is a tree structure
of files. As a suggestion, we could use

```   optimization_tests
         probname1
             probname1.prb - the problem file
             probname1.runlist - a list of test runs pointing to files in the runs directory
             runs
                 probname1.instance1 - output of a give run
                 ...         
         probname2
             probname2.prb - the problem file
             probname2.runlist - a list of test runs pointing to files in the runs directory
             runs
                 probname2.instance1 - output of a give run
                 ...

```

Given that the instance files will be plain text and quite large, as well as unlikely to 
be accessed frequently, we could compress them using one of the standard tools.


## Tools for preparing and using problem files

### Access to problem files

In order to use problem files, we need a way to list them, display their content in
both summary and detail, and attempt to use them with various nonlinear least squares
and/or function minimization software.

#### Command line file lister and display tool

We need a tool that 

- sets the directory where problems are found

- lists the files

- allows one file to be selected

- displays information in a structured way (possibly according to some profile)

- allows the profile for display to be edited somehow

?? code here

#### GUI file lister and display tool

Likely tied into command line tool by allowing command line to be executed when
mouse or pointer clicks are executed. Check existing GUI tools to see what is 
possible.

?? code here

### Building problem files

#### Data frame or matrix to source vector

If problem files are to be self contained, we do not want to need to access external
files. The simplest way to present a variable to **R** is to input data as a simple
list and then name it. To prepare the problem file, we therefore want to be able to
get data into the local **R** environment and prepare the text for an assignment
statement that contains a source vector.

?? code

```{r, eval=FALSE}

# Use paste() to build the c(...) structure

```


## Running problems

`runoptprob` -- ?? how it should be used

- provide 

    -- filename (at least the root)

    -- which tool to use (optimr, nls, nlmrt, nlsr tools, minpack.LM tools)

    -- choice of gradient function or approximation (gr= (gr, "grfwd", etc.)) ?? optional?

    -- controls -- as per the control list in programs

    -- other arguments ?? how to provide? dotargs as below?

    -- xdata or dotargs (how to specify might be interesting)
    
    -- timing control (e.g., microbenchmark or simple timing)

- read output control profile (initially just use sink())

- read the file and execute it (make sure it has **R** commands so we can
actually source() it)

- analyze the call to runprob and do the appropriate call

- format output and extract and store summaries

  -- this may be multilayerd and take a lot of work
  
  -- start with no formatting, and gradually add features
  
  -- need to save conditions
  
  -- make sure we have time/date stamp on all runs
  
Issues to address?? :
 - sink() will save information, but there is a cost in time and space
 - how to do ok/not or red/yellow/green nicely | Do we want on params and value?
 - dealing with multiple solutions -- linear combination defined
 - where to put setting of FUZZ, reporting of deviations
  

```{r cache=FALSE, echo=FALSE}
require(knitr)
# read chunk (does not run code)
read_chunk('../R/runoptprob.R')
```

```{r}
require(nlmrt, quietly=TRUE)
require(optimrx, quietly=TRUE)
<<runoptprob.R>>
```

## Running multiple problems

- prepare "problem list", like a play list

- run them in sequence

- try to figure out how to save the output 
    
    -- sink
    
    -- summaries
    
    -- test against expected output
    
    -- timings
    
    -- measures of success


For the moment, let us just call runoptprob(). Note that it creates a sinkfile for each
call.

```{r}
pfname <- "DanielWood"
# try it
test1 <- runoptprob(pfilename=pfname, minmeth="nls")
test1
tmp <- readline("continue")

test2 <- runoptprob(pfilename=pfname, minmeth="nlxb")
test2
tmp <- readline("continue")

test3 <- runoptprob(pfilename=pfname, minmeth="optimr", submeth="L-BFGS-B")
test3
```


## Appendix A: Use of a data frame

The following script shows that we can store problem materials, including functions,
**INSIDE** an **R** data frame.

```{r, eval=FALSE}
require(NISTO)
ls(package:NISTO)
NISTO::DanielWood.res
NISTO:::DanielWood.res
f1 <- y ~ (b1+x*(b2+x*(b3+b4*x))) / (1+x*(b5+x*(b6+x*b7)))
str(f1)
?NISTO
?DanielWood
data(DanielWood)
DanielWood
dwprob <- list(mform = f1, mdata=DanielWood, pname="DanielWood", ptype="nls")
dwprob
data()
f2 <- y ~ b1 / (1+exp(b2-b3*x))
r2data <-Ratkowsky2
r2data
r2prob <- list(mform = f2, mdata=Ratkowsky2, pname="Ratkowsky2", ptype="nls")
r2prob
myprobs <- rbind(dwprob, r2prob)
myprobs
str(myprobs)
myprobs.d <- as.data.frame(myprobs)
str(myprobs.d)
f3 <- y ~ exp(-b1*x)/(b2+b3*x)
c1data <- Chwirut1
c1name <- "Chwirut1"
c1prob <- list(mform = f3, mdata=Chwirut1, pname=c1name, ptype="nls")
myprobs.d[3,] <- c1prob
myprobs.d <- as.dataframe(rbind(myprobs.d, c1prob))
myprobs.d <- as.data.frame(rbind(myprobs.d, c1prob))
myprobs.d <- as.data.frame(rbind(myprobs, c1prob))
myprobs.d
str(myprobs.d)
myprobs.d[,5] <- NA
str(myprobs.d)
colnames(myprobs.d[,5] <- "resfn")
colnames(myprobs.d[,5]) <- "resfn"
colnames(myprobs.d)[5] <- "resfn"
myprobs.d
DanielWood.res <- function(b) {
   xx<-DanielWood$x # case !!
   yy<-DanielWood$y
   res <- rep(NA, length(xx))
   b1<-b[1]
   b2<-b[2]
   res<-b1*(xx**b2) - yy
   return(res)
}
myprobs.d[1,5] <- DanielWood.res
myprobs.d[1,5] <- quote(DanielWood.res)
dwreschr <- "DanielWood.res <- function(b) {;xx<-DanielWood$x; yy<-DanielWood$y; res <- rep(NA, length(xx)); b1<-b[1];  b2<-b[2];   res<-b1*(xx**b2) - yy;   return(res);}"
myprobs.d[1,5] <- dwreschr
ls()
rm DanielWood.res
rm( DanielWood.res)
ls
ls()
source("myprobs.d[1,5])
"
)
source("myprobs.d[1,5]")
source(text="myprobs.d[1,5]")
?source
parse(text=myprobs.d[1,5])
ls()
eval(parse(text=myprobs.d[1,5]))
ls()
savehistory("NISTOx1.txt")
```

```{r, eval=FALSE}
# NISTOx2.txt
test <- "
# Chwirut1 - Jacobian
Chwirut1.jac <- function(b) {
   xx<-Chwirut1$x
   yy<-Chwirut1$y
   n<-length(b)
   m<-length(xx)
   b1<-b[1]
   b2<-b[2]
   b3<-b[3]
   J<-matrix(0,m,n) # define the size of the Jacobian
   expr3 <- exp(-b1 * xx)
   expr5 <- b2 + b3 * xx
   expr7 <- expr3 * xx
   expr10 <- expr5*expr5
   value <- expr3/expr5
   J[,1] <- -(expr7/expr5)
   J[,2] <- -(expr3/expr10)
   J[,3] <- -(expr7/expr10)
   return(J)
}
"
ftest <- eval(parse(test))
ftest <- eval(parse(text=test))
bstart <- c(1,1,1)
print(ftest(bstart))
savehistory("NISTOx2.txt")
```