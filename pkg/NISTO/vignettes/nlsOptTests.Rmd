---
title: "A structure for building optimization and nonlinear least squares tests for **R**"
author: "John C. Nash"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


# Motivation


http://www.itl.nist.gov/div898/strd/nls/nls_info.shtml presents a number of nonlinear
regression (nonlinear least squares) problems that are more or less difficult to solve
numerically. Doug Bates prepared an **R** package `NISTnls` which adapted these problems
to **R** and tested them with the `nls()` function of which he was a major author.
I built a related package to allow the same problems to be approached as unconstrained function 
minimization problems, which I named `NISTopt`. However, this package was, when I prepared
it in 2009, incomplete, particularly in respect to the manuals (Rd files). This vignette
is an attempt to better document that effort and in the process force a review of the
package. Furthermore, it raises the question of how to **efficiently** add and run
new test problems. 

Note that the problems here are **NOT** always good illustrations of tasks to which the various
function minimization (also called "optimization", though that generally implies that there
are also constraints) tools should be applied. However, they do suggest limits 
of performance of these tools. 

# Approach

We need to be able to set things up so that a separate command is not needed for each start
or example. Such a structure needs a lot of careful thought to avoid awkwardness. In particular,
the existing trial (from 2008/9) of package `NISTopt` shows that its structure has a lot of
code, much of which is common in nature. Moreover, we want to be able to add new problems 
easily. Also the "Name.setup" functions do not seem very helpful for the long term.

## Restrictions on the problems

We do not wish to (at the moment) include general optimization problems with general constraints.
In fact, we will limit our attention to at most bounds constrained nonlinear optimization and
nonlinear least squares problems. There are, however, a lot of these. By considering lower and
upper bounds that are equal, we can extend the constraints to fixed or **masked** parameters.

# Design I

We may be willing to have a single file to describe a single (but more or less complete)
problem, but we ultimately want
a **set** of problems, all of which can be accessed through a single structure. Through 
some experiments, detailed in *Appendix A: Use of a data frame*. 

This design is not, of course, the only possibility. However, it is likely that something
based on the "single problem" **R** code file illustrated in the next section will prove
useful.

## A single problem

Let us consider how we might, in **R**, specify a nonlinear least squares problem that
also can be presented as an optimization problem. Here is a possible file for the 
problem of Daniel and Wood (??citation).

```{r}
probname <- "DanielWood"
probtype <- "nls" # nls is more restrictive than function minimization
mformula <- ( y ~ (b1+x*(b2+x*(b3+b4*x))) / (1+x*(b5+x*(b6+x*b7)) ) )
# The above is a "formula". But we could also possibl also use a character string or expression,
# but need to know how to do so carefully. ??
## ?? mdata=DanielWood # This would work if we have a data package, but not useful independently
y <- c( 2.138, 3.421, 3.597, 4.340, 4.882, 5.660)
x <- c( 1.309, 1.471, 1.490, 1.565, 1.611, 1.680)
dframe <- data.frame(x,y)
# This is the local data frame. We need to rename this when we put it into a global structure
# Now add various functions for optimization or nls-by-function
# ?? We could have problems with quotation marks WITHIN the code.
fn <- "
DanielWood.f <- function(x) {
   res<-DanielWood.res(x)
   f<-sum(res*res)
}"

resfn <- "DanielWood.res <- function(b) {
   xx<-DanielWood$x # case !!
   yy<-DanielWood$y
   res <- rep(NA, length(xx))
   b1<-b[1]
   b2<-b[2]
   res<-b1*(xx**b2) - yy
   return(res)
}"

jacfn <- "# DanielWood - Jacobian
DanielWood.jac <- function(b) {
   xx<-DanielWood$x
   yy<-DanielWood$y
   n<-length(b)
   m<-length(xx)
   b1<-b[1]
   b2<-b[2]
   J<-matrix(0,m,n) # define the size of the Jacobian
   expr1 <- xx^b2
   J[, 1] <- expr1
   J[, 2] <- b1 * (expr1 * log(xx))
   return(J)
}"

hessfn <- "DanielWood.h <- function(x) {
   JJ<-DanielWood.jac(x)
   H <- t(JJ) %*% JJ
   res<-DanielWood.res(x)
}"

gr <- "DanielWood.g<-function(x) {
#   stop('not defined')
   JJ<-DanielWood.jac(x)
   res<-DanielWood.res(x)
   gg<-as.vector(2.0*t(JJ) %*% res)
   return(gg)
}"


psetup <- "DanielWood.setup<-function() {
   data(DanielWood) # and load up the data into x and y
   start1 = c( 1, 5)
   start2 = c( 0.7, 4)
   out<-list(start1=start1, start2=start2)
   return(out)
}"

ptest <- "DanielWood.test<-function() {
}"   

# We can "install" these functions via eval(parse(name))

```


## Combining single problems


## Running problems


## Extracting single problems





# Appendix A: Use of a data frame