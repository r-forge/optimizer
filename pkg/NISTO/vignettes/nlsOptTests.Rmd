---
title: "A structure for building and running optimization and nonlinear least squares tests for **R**"
author: "John C. Nash and Paul Gilbert"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: nlsOptTests.bib
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract

This article discusses how tests for nonlinear equations, least squares or equations 
may be set up, executed and analyzed for **R**. This activity is sensitive to the 
manner in which the test functions are computed, the choice of starting values, and
the choice of measures of success. They may also be modified by changes in the solvers
for the problems in question. Indeed, it is always a concern that seemingly minor 
adjustments intended to improve such solvers may cause them to fail on particular problems
or starting values. Thus it is desirable to be able to 

- be able to quickly and easily run a specific test on a specific solver;

- be able to quickly and easily run a specific test on all available solvers;

- be able to quickly and easily run all available tests on an updated or new solver;

- be able to compare performance of solvers on a single or a group of problems;

- be able to report successes and failures in a manner that allows for rapid detection
of potential issues with solvers.


The last two criteria, and especially the last, require that there be a way to save 
and access the results of test runs.
Because each test instance can generate quite a lot of data, and the combinatorial possibilities
of tests, solvers, and solver options such as choice of gradient approximation, saving test
information could demand considerable storage space and data management effort. We outline 
some possible approaches to the data management aspect of optimization testing.

#

## Motivation

http://www.itl.nist.gov/div898/strd/nls/nls_info.shtml presents a number of nonlinear
regression (nonlinear least squares) problems that are more or less difficult to solve
numerically. Doug Bates prepared an **R** package `NISTnls` which adapted these problems
to **R** and tested them with the `nls()` function of which he was a major author.
There are a number of other test function collections (some referenced below), 
usually with overlap in their
coverage, for nonlinear equations, nonlinear least squares and function minimization.
In some cases, such tests include bounds constraints or fixed parameters. 

In 2009, I built the incomplete package `NISTopt` to allow the same problems as in `NISTnls`
to be approached as unconstrained function minimization problems. Similarly, the Google 
Summer of Code project OptimGUI by Yixuan Qiu in 2011 attempted to provide a point-and-click
interface to aid in building and running function minimization problems in **R**, and 
developed a structure for storing such problems in a consistent manner. This vignette
is an attempt to better document such ideas. It is especially concerned with how to 
**efficiently** add, modify, run and review tests of the **R** tools to solve such problems. 

Note that the test problems used are **NOT** always good illustrations of the tasks to 
which the various
function minimization tools should be applied by general **R** users. 
However, they do suggest limits of performance of these tools. 

We note that in **R** circles, function minimization
is often called "optimization", though that generally implies that there
are also constraints. 

## Approach

We want to avoid having to write specific code to run each test. 
The package **optimrx** (@optimrx16) can call most of the function 
minimization tools in **R** and can be extended to include others, and the
functions within this package standardize the call to each of the minimization
solvers. Thus we can simplify the code to test such solvers, using only their 
method name within the calls to either `optimr()` or `opm()`. Furthermore, these
functions allow a consistent syntax for specifying analytic gradients (if available),
the built-in gradient approximation (if this is part of the solver), else one of 
several pre-defined gradient approximations callable in the **optextras** package.
At the time of writing, an effort is in process to try to similarly unify the 
nonlinear least squares tools for **R**, namely, the `nls()` function in the **stats**
package, and the tools in packages **nlmrt**, **minpack.lm** and **nls2** as well as some
new efforts. 

### Specification of problems

Code developed in the trial package **NISTopt** suggested that a given problem could be
specified by a particular name. Thus, the problem of @danwood1980, is called **DanielWood**.

If we wish to solve this problem using one of the nonlinear least squares solvers that use a 
syntax like that of `nls()`, then we need to provide a model formula and some data that
is consistent with this model formula. By naming the formula as `DanielWood.formula`, and
putting the data in a dataframe named `DanielWood.df`, we then need only a starting
vector of parameters to have a well-specified formula-based nonlinear least squares 
problem. Besides `nls()`, `nlxb()` from **nlmrt**, `nls2()` from **nls2** (CRAN version),
and `nls.lm` from **minpack.lm**. We assign a **problem class** `formula` to this 
specification of the DanielWood problem.

Similarly, a function-based nonlinear least squares problem solvable by `nlfb()` from 
package **nlmrt** or `nlsLM` from **minpack.lm** can be specified by an **R** function
that computes the residual vector from a particular set of parameters. These functions
can also use a Jacobian function if it is available. At the time of writing, an approximate
Jacobian will be provided by these solvers if the provided argument is NULL. Work is 
in process to try to provide particular derivative approximations by a mechanism similar
to the one used in package **optimrx**. We can provide for use of such solvers by using
the name `DanielWood.res` for the residual function and `DanielWood.jac` for the Jacobian.
We assign the problem class `sumsquares` to this approach to the problem.

Generally, for function minimization, we can specify `DanielWood.f` for a general function
to be minimized, with the (optional) `DanielWood.g` for the gradient. This is given problem
class `uncopt` for "unconstrained optimization".

Finally, specifying lower or upper bounds in the vectors `lower` and/or `upper` results in
a bounded function minimization problem that we assign problem class `boundopt`. Note that
the solution of a bounded problem is usually different from its unconstrained version. We
will use separate problem file names. For example, if there were a bounded version of the 
DanielWood problem, we might name it `DanielWoodBounded`. (At the time of writing there
is no such problem file.)

Many optimization problems, including some test problems, need data other than the parameters.
This can be provided in a data frame `DanielWood.df`. This will be created in the problem
file as required.

There are often several sets of starting parameters for given test problems, as well as
the possibility of generated pseudo-random vectors, especially for those problems which have
a variable number of parameters. These can be provided in a matrix called `starts` which has
named columns, since the nonlinear least squares solvers using a model formula require 
parameter names. When the problem has no model formula, we can and do provide parameter names
as `p1, p2, ..., pn`. (?? not done yet, but it is in opm() already). We expect the 
problem file to provide the `starts` matrix.

Note that the `.res` function could be used to specify nonlinear equations problems if the 
solution has all the residuals at zero. At the time of writing, we have not yet included
this possibility.


### Restrictions on the problems

We do not wish (at the moment) to include general optimization 
problems with general constraints.
In fact, we will limit our attention to at most bounds constrained nonlinear optimization and
nonlinear least squares problems. There are, however, a lot of 
these. By considering lower and
upper bounds that are equal, we could extend the constraints to fixed or **masked** 
parameters, but at the time of writing have not included such tests.

## A sample design

Building on the ideas of the above section, we create the example **problem file** 
`DanielWood.prb`. In this case, we provide all the elements above. Thus a problem file
yields a
**set** of problems, all of which can be accessed through a single structure. Through 
some experiments, detailed in *Appendix A: Use of a data frame*, we found that one could,
if desired, store such problems in a data frame, but this has since been discarded
as overly complicated.
Our flat, plain text files, one per named problem, should suffice. Some
problems can be altered by including bounds. To simplify an already rich structure, 
we will name the unconstrained and bounded problems separately since they may have
different optima.

Other structures for problem files are, of course, equally possible. However, we 
will experiment with the current approach until we discover it cannot support our requirements. 

### The **DanielWood** problem

Let us implement, in **R**, a possible file for the problem of @danwood1980.

```{r cache=FALSE, echo=FALSE}
require(knitr)
# read chunk (does not run code)
read_chunk('../inst/extdata/DanielWood.prb')
# We use this approach to avoid having differences between file here and in inst/extdata/
```

```{r}
#- We need to have 'counters' environment before we load problem file
counters <- new.env()
```


```{r}
<<DanielWood.prb>>
```

### Challenges in the design

There are several weaknesses in the design. They are unlikely fatal, but do need to be addressed.

- There is provision for multiple sets of starting parameters. To account for the possibility
of different sizes of problems, we need to be able to parameterize the starts. Moreover, there
could be different starting sets within each size. To allow for this, we use a character string
index to specify which member of a **family** of problems is to be used. The starts are then
created by a function which has the character index as its argument. Thus, for example, we could
start the Extended Rosenbrock test function (??ref) with a set of 10 parameters each having 
value `pi` by using an index "10:pi". 


## Running test problems

There are a number of ways in which we want to run the test problems. We consider three
main streams:

 1) running a specific test on a specific solver with specific conditions
 such as the gradient choice or particular control settings;
 
 2) running a specific test problem on all available solvers;

 3) running all available tests on an updated or new solver.

Some challenges in accomplishing these tasks arise from the following situations:

- Problems that can be specified as sum of squares functions (possibly with Jacobian functions)
are very easily transformed into functions for function minimization with associated gradient
functions. However, there may be ways to express the function to be minimized in ways that are
more direct and do not explicitly involve the residuals.

- If a function minimization problem has been specified via the sum of squares of residuals,
the gradient may be specified by means of the (possibly approximated) Jacobian, or it may be
directly approximated using function values.

- We may wish to attempt the test using all available approximations.

- The specification of a problem by means of a formula is translated to the computation of
residuals and Jacobian by nonlinear least squares methods. In the case of package **nlmrt**
(this will become package **nlsr** shortly), the Jacobian is computed 
analytically if possible, and the residual
and Jacobian functions are explicitly created. These may or may not be equivalent computationally
to the functions that may be present in the problem file. 

Overall, the challenges are combinatoric, and generate a very large number of cases, since there
will be 

- P problem files
 
- potentially a formula, a residual, and a function presentation of each problem. Bounded
  problems will be separately itemized in the problem files. 

- multiple gradient options (at the time of writing these are analytic, forward, backward,
  central and **numDeriv** as well as a "default" approximation in some methods)

- MF nonlinear least squares methods using a formula, MS using a residual, and MO optimization 
  methods. Some reduction in the possibilities occurs when we have bounds. 
  
- Several cases if there are choices such as the three update formulas for optim::CG, or the
  "plinear" and "port" options for `nls()`.
  
- An unspecified number of extra cases if we set limits on the function evaluations etc., or 
the controls such as various "convergence" tolerances.

**Proposal**: To reduce the number of cases, it is suggested that any level of effort limitations
be put into the problem files. That is, we propose standardizing the test and consider an effort
to have failed if it uses more counts than specified. Further, it is suggested that methods be
primarily tested giving priority to 

- the default settings of any algorithms
- the default choices of convergence and termination tests
- analytic gradients if available, else the default (internal) gradient approximation, though
it may be sensible to also test with the central finite difference approximation. This latter
choice is a modest compromise between the simple forward approximation and the expensive but
more accurate **numDeriv** approximation. Note that the complex step derivative (@Martins2003) would
be a choice if it is known to be applicable. 
  
## Reporting experience running problems

### The computing environment

The computing environment is an important aspect of computational tests. However, it is
often reported in summary and incomplete form. If we are to be thorough we should report

- the computing hardware and particular settings (clock rates, extra features, etc.)

- the operating system version and any add-ons

- the choice of floating point and related computational libraries used

- the version of the computing language processor (in our case, the **R** version)

- the versions of any packages used, and packages loaded at the time of the test

- how storage hardware is connected. While buffering may take care of this, writing to
external storage, or sending information over communications links (these may be equivalent)
may alter timings or other performance measures

- any parallelism that could alter performance

- other running processes that could alter performance.

Note that some of the above environmental factors should not alter the computed optima, nor
the counts that report the computational effort e.g., the number of function or gradient
evaluations. However, some choices will affect both these computed values and the timings to
arrive at them.

In the past, the computational environment has been reported largely as a commentary to 
published articles and reports. However, we believe that -- as far as possible -- the 
information should be gathered automatically and included in log files of test runs. 
**R** already has a useful `sessionInfo()` command that reports quite well at the level
of the programming language. 

In the last couple of years, the possibility of test variability due to changes in the 
underlying computational environment has been addressed more explicitly. Some workers have 
suggested the capture of the entire environment in a virtual machine. A somewhat
less onerous variant of this is **Rocker** (see http://dirk.eddelbuettel.com/blog/2014/10/23/).
At the time of writing, we are not considering such measures for testing optimization
tools, but could do so in future.


### Reproducible tests

It is well-known that timing measures for real-world computers have variability. 
There are also concerns about how timings are conducted (@Dolan2006). The differences
between different timings  
may be surprisingly large depending on the particular context within which test computations
are performed. It is also possible that there may be non-timing variability if optimization
methods use "random" search. We will presume (possibly heroically) that efforts have been
made to eliminate factors that could result in differences in counts and other measures
of performance excepting time. 

We also believe that for some tests it is useful to employ tools such as **microbenchmark**
to try to gauge the degree of variability in performance. This tool is almost certainly a
key choice in measuring the impact of any instrumentation code we embed in our tests, such
as the function, gradient, residual or jacobian counts mentioned below.

### Measures of computational effort

Running the function `opm()` with `method = "ALL"` very quickly reveals that 
different solvers
return different measures of effort. `opm()` is set up to report counts of function and 
gradient evaluations. For some methods, these are not returned. Sometimes other measures, 
such as "iterations" are reported. This makes it difficult to compare reported outcomes across
methods.

In response to this issue, we could insert counts into the various test function components.
This will, of course, have an effect on reported timings, so some measure of that effort 
should be taken from time to time. The effect should be small. It may be sensitive, however,
to the computational environment of the previous section. At the time of writing, we have
assigned a low priority to investigating the effect of instrumentation code on the overall
performance, as we believe it is relatively small.

How should this be implemented? In animating the optimization of the largest small hexagon
problem (see @Grah75a), Greg Snow suggested using an R6class (@R6class), but here we
can use the much simpler mechanism of creating and using a named **R** environment `counters`.
A test is provided in Appendix B. 


### What information should be saved?

Ideally, a test run should record very complete information. This includes:

- the hardware computational environment

- the operating environment, that is, operating system and relevane computational libraries

- the R version and package versions and other packages loaded

- the test function and derivative calculations used

- the solver and any settings or tolerances

- measures of effort from the solver

- measures of effort by direct count

- the trajectory of the computations, including all evaluations of the function used
for derivatives (sometimes the parameter inputs to derivative computations violate constraints,
giving failures for which the reported errors may be difficult to interpret)

- the reported solution or termination point

### What information should be reported?

The above information will be far too voluminous to report. We therefore need tools to 
render summaries at various levels. 

For many situations, it will be sufficient and desirable to have a "yes/no" or "red/yellow/green"
indicator of success or failure for a specified method and given test problem. This may, in
fact, be the most common report.

In the numerical analysis literature, it has become quite popular to present **performance 
profiles** of various types. 

### Automated collection from diverse sources

Assuming there is an established set of test functions along with a similarly prescribed 
testing code, it is desirable to allow diverse workers to carry out computations and submit
reports to a (likely centralized) data repository. This requires:

- a standardized report format, which hopefully is embedded in the tool that runs the
test computations

- a secure method to accept reports into the data repository;

- a streamlined approach to allow new contributors to join the group submitting test results;

- a mechanism to test the submissions for acceptable quality;

- tools to extract results, and organize, analyze, summarize or visualize them; and

- tools to automatically report unexpected or unwanted results. For example we would like
to know if a new version of a package causes failures on some test functions.

### Managing the test output

The previous sub-section has listed some desiderata for handling test output. Details that
will be the burden of the work remain. What are key obstacles to success?

First, we will need to deal with volume of data, particularly if trajectories are important.
There are, of course, many choices for how to store the data. A database could be used, but
imposes the choice of software to run it, as well as potential operating system issues if the
overall project needs to be replicated or moved. A simpler arrangement is a tree structure
of files. As a suggestion, we could use

```   optimization_tests
         probname1
             probname1.prb - the problem file
             probname1.runlist - a list of test runs pointing to files in the runs directory
             runs
                 probname1.instance1 - output of a give run
                 ...         
         probname2
             probname2.prb - the problem file
             probname2.runlist - a list of test runs pointing to files in the runs directory
             runs
                 probname2.instance1 - output of a give run
                 ...

```

Given that the instance files will be plain text and quite large, as well as unlikely to 
be accessed frequently, we could compress them using one of the standard tools.


## Tools for preparing and using problem files

### Access to problem files

In order to use problem files, we need a way to list them, display their content in
both summary and detail, and attempt to use them with various nonlinear least squares
and/or function minimization software.

#### Command line file lister and display tool

We need a tool that 

- sets the directory where problems are found

- lists the files, possibly flagging what resources are available for each problem

- allows one file to be selected

- displays information in a structured way (possibly according to some profile)

- allows the profile for display to be edited somehow

?? code here

#### GUI file lister and display tool

If there is a command-line tool for listing and displaying problem files, it 
should be relatively straightforward to provide a graphical user interface (GUI)
version. This will likely use the code of the command line tools, executing them 
in response to mouse or pointer clicks. ?? Check existing GUI tools to see what is 
possible.

?? code here

### Building problem files

Can we set up tools that help us build the problem files? The files are structured.
It could be easier to simply copy and edit existing files.

## Running problems

`runoptprob` -- ?? how it should be used -- access via the Rd file.

- provide 

    -- filename (at least the root)

    -- which tool to use (optimr, nls, nlmrt, nlsr tools, minpack.LM tools)

    -- choice of gradient function or approximation (gr= (gr, "grfwd", etc.)) ?? optional?

    -- controls -- as per the control list in programs

    -- other arguments ?? how to provide? dotargs as below?

    -- xdata or dotargs (how to specify might be interesting)
    
    -- timing control (e.g., microbenchmark or simple timing)

- read output control profile (initially just use sink())

- read the file and execute it (make sure it has **R** commands so we can
actually source() it)

- analyze the call to runprob and do the appropriate call

- format output and extract and store summaries

  -- this may be multilayerd and take a lot of work
  
  -- start with no formatting, and gradually add features
  
  -- need to save conditions
  
  -- make sure we have time/date stamp on all runs
  
Issues to address?? :
 - sink() will save information, but there is a cost in time and space
 - how to do ok/not or red/yellow/green nicely | Do we want on params and value?
 - dealing with multiple solutions -- linear combination defined
 - where to put setting of FUZZ, reporting of deviations
  

```{r cache=FALSE, echo=FALSE}
require(knitr)
# read chunk (does not run code)
read_chunk('../R/runoptprob.R')
```

```{r}
require(nlmrt, quietly=TRUE)
require(optimrx, quietly=TRUE)
<<runoptprob.R>>
```

## Running multiple problems

- prepare "problem list", like a play list

- run them in sequence

- try to figure out how to save the output 
    
    -- sink
    
    -- summaries
    
    -- test against expected output
    
    -- timings
    
    -- measures of success


For the moment, let us just call runoptprob(). Note that it creates a sinkfile for each
call.

```{r}
pfname <- "DanielWood"
# try it
test1 <- runoptprob(pfilename=pfname, minmeth="nls")
test1
tmp <- readline("continue")

test2 <- runoptprob(pfilename=pfname, minmeth="nlxb")
test2
tmp <- readline("continue")

test3 <- runoptprob(pfilename=pfname, minmeth="optimr", submeth="L-BFGS-B")
test3
```


## Appendix A: Use of a data frame

The following script shows that we can store problem materials, including functions,
**INSIDE** an **R** data frame.

```{r, eval=FALSE}
require(NISTO)
ls(package:NISTO)
NISTO::DanielWood.res
NISTO:::DanielWood.res
f1 <- y ~ (b1+x*(b2+x*(b3+b4*x))) / (1+x*(b5+x*(b6+x*b7)))
str(f1)
?NISTO
?DanielWood
data(DanielWood)
DanielWood
dwprob <- list(mform = f1, mdata=DanielWood, pname="DanielWood", ptype="nls")
dwprob
data()
f2 <- y ~ b1 / (1+exp(b2-b3*x))
r2data <-Ratkowsky2
r2data
r2prob <- list(mform = f2, mdata=Ratkowsky2, pname="Ratkowsky2", ptype="nls")
r2prob
myprobs <- rbind(dwprob, r2prob)
myprobs
str(myprobs)
myprobs.d <- as.data.frame(myprobs)
str(myprobs.d)
f3 <- y ~ exp(-b1*x)/(b2+b3*x)
c1data <- Chwirut1
c1name <- "Chwirut1"
c1prob <- list(mform = f3, mdata=Chwirut1, pname=c1name, ptype="nls")
myprobs.d[3,] <- c1prob
myprobs.d <- as.dataframe(rbind(myprobs.d, c1prob))
myprobs.d <- as.data.frame(rbind(myprobs.d, c1prob))
myprobs.d <- as.data.frame(rbind(myprobs, c1prob))
myprobs.d
str(myprobs.d)
myprobs.d[,5] <- NA
str(myprobs.d)
colnames(myprobs.d[,5] <- "resfn")
colnames(myprobs.d[,5]) <- "resfn"
colnames(myprobs.d)[5] <- "resfn"
myprobs.d
DanielWood.res <- function(b) {
   xx<-DanielWood$x # case !!
   yy<-DanielWood$y
   res <- rep(NA, length(xx))
   b1<-b[1]
   b2<-b[2]
   res<-b1*(xx**b2) - yy
   return(res)
}
myprobs.d[1,5] <- DanielWood.res
myprobs.d[1,5] <- quote(DanielWood.res)
dwreschr <- "DanielWood.res <- function(b) {;xx<-DanielWood$x; yy<-DanielWood$y; res <- rep(NA, length(xx)); b1<-b[1];  b2<-b[2];   res<-b1*(xx**b2) - yy;   return(res);}"
myprobs.d[1,5] <- dwreschr
ls()
rm DanielWood.res
rm( DanielWood.res)
ls
ls()
source("myprobs.d[1,5])
"
)
source("myprobs.d[1,5]")
source(text="myprobs.d[1,5]")
?source
parse(text=myprobs.d[1,5])
ls()
eval(parse(text=myprobs.d[1,5]))
ls()
savehistory("NISTOx1.txt")
```

```{r, eval=FALSE}
# NISTOx2.txt
test <- "
# Chwirut1 - Jacobian
Chwirut1.jac <- function(b) {
   xx<-Chwirut1$x
   yy<-Chwirut1$y
   n<-length(b)
   m<-length(xx)
   b1<-b[1]
   b2<-b[2]
   b3<-b[3]
   J<-matrix(0,m,n) # define the size of the Jacobian
   expr3 <- exp(-b1 * xx)
   expr5 <- b2 + b3 * xx
   expr7 <- expr3 * xx
   expr10 <- expr5*expr5
   value <- expr3/expr5
   J[,1] <- -(expr7/expr5)
   J[,2] <- -(expr3/expr10)
   J[,3] <- -(expr7/expr10)
   return(J)
}
"
ftest <- eval(parse(test))
ftest <- eval(parse(text=test))
bstart <- c(1,1,1)
print(ftest(bstart))
savehistory("NISTOx2.txt")
```

## Appendix B: Test of counters inside functions.

```{r cache=FALSE, echo=FALSE}
require(knitr)
# read chunk (does not run code)
read_chunk('../inst/Rfiles/counttest.R')
```

```{r}
<<counttest.R>>
```
