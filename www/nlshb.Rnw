\documentclass{article}

%\VignetteIndexEntry{nlshandbook Tutorial}
%\VignetteDepends{}
%\VignetteKeywords{nonlinear least squares, Levenberg-Marquardt method}
%\VignettePackage{nlmrt}

%% from Ross Ihaka doc.

\newcommand{\R}{{\sf R\ }}

\newcommand{\B}[1]{{\bf#1\rm}}
\newcommand{\code}[1]{{\tt #1}}
\newcommand{\pkg}[1]{\bf{\tt #1}\rm }

\title{nls handbook}
\author{John C. Nash}
\usepackage{Sweave}
\usepackage{fancyvrb}
\usepackage{chicago}

\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=1em}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=1em}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=1em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}
 
%%% \DefineVerbatimEnvironment{Soutput}{Verbatim}{fontsize=\small,fontshape=n}

\begin{document}
\maketitle

<<setup, echo=FALSE, cache=FALSE>>=
# size takes valid value of LaTeX font sizes like small, big, huge, ...
opts_chunk$set(size = 'scriptsize')
@

\section*{Background}

Based on the nlmrt-vignette, this document is intended to show the various
commands (and some failures) for different \R functions that deal with 
nonlinear least squares problems. It is NOT aimed at being pretty, but 
a collection of notes to assist in developing other documents more quickly.

Essentially this is an annotated version of extended versions of the examples
provided with different packages in the \R repositories and elsewhere.

Comparisons between ways of doing things always force some thinking about
which approaches are "best". In writing this I (JCN) caution that "best" is
always within a particular context. I believe \code{nls()} was developed
within a group of active researchers to allow them to conduct calculations
that involved extended nonlinear regressions. Many of the present users of
\R may have totally different expectations and needs. While I would like to
see \code{nls()} in a form that allows more transparent understanding of 
how it works, it is nonetheless a very powerful tool but a product of 
its time and place of creation as is all software. 


\section{nls}

\code{nls()} is the base installation nonlinear least squares tool. It is coded
in C with an \R wrapper. I find it very difficult to comprehend. However, it
does seem to work most of the time, though it has some weaknesses for certain
types of problems. 

Following are the examples in the nls.Rd file from the distribution (this one is
from R-2.15.1). I have split the examples to provide comments.

\subsection{A straightforward example}

The first example, chunk nlsex1, uses the built-in data set DNase.

<<nlsex1, echo=TRUE, cache=TRUE>>=
od <- options(digits=5) # include in case needed
require(graphics)

DNase1 <- subset(DNase, Run == 1)

## using a selfStart model
fm1DNase1 <- nls(density ~ SSlogis(log(conc), Asym, xmid, scal), DNase1)
summary(fm1DNase1)
## the coefficients only:
coef(fm1DNase1)
## including their SE, etc:
coef(summary(fm1DNase1))

## using conditional linearity
fm2DNase1 <- nls(density ~ 1/(1 + exp((xmid - log(conc))/scal)),
                 data = DNase1,
                 start = list(xmid = 0, scal = 1),
                 algorithm = "plinear")
summary(fm2DNase1)

## without conditional linearity
fm3DNase1 <- nls(density ~ Asym/(1 + exp((xmid - log(conc))/scal)),
                 data = DNase1,
                 start = list(Asym = 3, xmid = 0, scal = 1))
summary(fm3DNase1)

## using Port's nl2sol algorithm
fm4DNase1 <- nls(density ~ Asym/(1 + exp((xmid - log(conc))/scal)),
                 data = DNase1,
                 start = list(Asym = 3, xmid = 0, scal = 1),
                 algorithm = "port")
summary(fm4DNase1)
@

\subsection{A problem with a computationally singular Jacobian}

\code{nls()} is fine for the problem above. But what happens when we supply a problem that
is a bit nastier, the WEEDS problem \cite[section 12.2]{jncnm79}. This problem is examined
in more detail under the section on \pkg{nlmrt}

<<nlshob01, echo=TRUE, cache=TRUE>>=
traceval <- FALSE
ydat  <-  c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443, 
          38.558, 50.156, 62.948, 75.995, 91.972) # for testing
tdat  <-  seq_along(ydat) # for testing
start1  <-  c(b1=1, b2=1, b3=1)
eunsc  <-   y ~ b1/(1+b2*exp(-b3*tt))
weeddata1  <-  data.frame(y=ydat, tt=tdat)
require(nlmrt)
## check using nlmrt function nlxb
anlxb1  <-  try(nlxb(eunsc, start=start1, trace=traceval, data=weeddata1))
print(anlxb1) # ?? need a summary function
## try nls no fancies
anls1 <-  try(nls(eunsc, start=start1, trace=traceval, data=weeddata1))
print(anls1) 
## try nls with 'port' algorithm
anls1port <- try(nls(eunsc, start=start1, trace=traceval, data=weeddata1, algorithm="port"))
print(anls1port)
## try nls with 'plinear' algorithm
eunsclin <- y ~ 1/(1+b2*exp(-b3*tt))
start1lin  <-  c(b2=1, b3=1)
anls1plin <- try(nls(eunsclin, start=start1lin, trace=traceval, data=weeddata1, algorithm="plinear"))
print(anls1plin)
@

For the WEEDS problem, the "port" algorithm using the 'nl2sol' code of \cite{DenSchnab83}
finds the solutionm, though the running output prints the sum of squares divided by 2. 
The "plinear" method goes to a point where the Jacobian is
essentially singular. Package \pkg{nlmrt} is helpful here to check this.

<<nlshob01a, echo=TRUE, cache=TRUE>>=
weedss<-model2ssfun(eunsc, start1)
y<-weeddata1$y
tt<-weeddata1$tt
print(weedss(c(1.8021E3, 6.038966E1, 4.119948e-2), y=y, tt=tt))
weedjac<-model2jacfun(eunsc, start1)
JJ<-(weedjac(c(1.8021E3, 6.038966E1, 4.119948e-2), y=y, tt=tt))
svd(JJ)$d
@

\subsection{Weighted nonlinear regression}

As of 2012-8-17, package \pkg{nlmrt} does not provide for weighting,
though it would not be difficult to add. (The code is all in \R.)

<<nlsex2, echo=TRUE, cache=TRUE>>=
## weighted nonlinear regression
Treated <- Puromycin[Puromycin$state == "treated", ]
weighted.MM <- function(resp, conc, Vm, K)
{
    ## Purpose: exactly as white book p. 451 -- RHS for nls()
    ##  Weighted version of Michaelis-Menten model
    ## ----------------------------------------------------------
    ## Arguments: 'y', 'x' and the two parameters (see book)
    ## ----------------------------------------------------------
    ## Author: Martin Maechler, Date: 23 Mar 2001

    pred <- (Vm * conc)/(K + conc)
    (resp - pred) / sqrt(pred)
}

Pur.wt <- nls( ~ weighted.MM(rate, conc, Vm, K), data = Treated,
              start = list(Vm = 200, K = 0.1))
summary(Pur.wt)
@

This structure does not carry over to \code{nlxb()} from \pkg{nlmrt}, 
which is using rather more pedestrian code. Nor can we call 
\code{weighted.MM} in \code{nlfb()}. What we can do is define a new
function \code{wMMx}, where we have changed the response name \code{resp}
to match the name \code{rate} in the date frame \code{Treated}. These
changes are a bit of an annoyance, and suggestions of how to make the
routines more equivalent are welcome (to J Nash).

<<nlsex2a, echo=TRUE, cache=TRUE>>=
wMMx <- function(x, rate, conc)
{
    Vm <- x[[1]]
    K <- x[[2]]
    pred <- (Vm * conc)/(K + conc)
    (rate - pred) / sqrt(pred)
}
anlfb2 <- nlfb(start = list(Vm = 200, K = 0.1), wMMx, jacfn=NULL, 
       rate=Treated$rate, conc=Treated$conc)
@

\subsection{A different passing mechanism}

Why is this useful / important??

<<nlsex3, echo=TRUE, cache=TRUE>>=
## Passing arguments using a list that can not be coerced to a data.frame
lisTreat <- with(Treated,
                 list(conc1 = conc[1], conc.1 = conc[-1], rate = rate))

weighted.MM1 <- function(resp, conc1, conc.1, Vm, K)
{
     conc <- c(conc1, conc.1)
     pred <- (Vm * conc)/(K + conc)
    (resp - pred) / sqrt(pred)
}
Pur.wt1 <- nls( ~ weighted.MM1(rate, conc1, conc.1, Vm, K),
               data = lisTreat, start = list(Vm = 200, K = 0.1))
stopifnot(all.equal(coef(Pur.wt), coef(Pur.wt1)))
@

\subsection{Putting in a Jacobian}

Unfortunately, for reasons that do not seem clear to me (JCN), \R
in the \code{nls()} function uses the term "gradient" for the \B{matrix}
that is, arguably more commonly, called the \B{Jacobian}. 

<<nlsex4, echo=TRUE, cache=TRUE>>=
## Chambers and Hastie (1992) Statistical Models in S  (p. 537):
## If the value of the right side [of formula] has an attribute called
## 'gradient' this should be a matrix with the number of rows equal
## to the length of the response and one column for each parameter.

weighted.MM.grad <- function(resp, conc1, conc.1, Vm, K)
{
  conc <- c(conc1, conc.1)

  K.conc <- K+conc
  dy.dV <- conc/K.conc
  dy.dK <- -Vm*dy.dV/K.conc
  pred <- Vm*dy.dV
  pred.5 <- sqrt(pred)
  dev <- (resp - pred) / pred.5
  Ddev <- -0.5*(resp+pred)/(pred.5*pred)
  attr(dev, "gradient") <- Ddev * cbind(Vm = dy.dV, K = dy.dK)
  dev
}

Pur.wt.grad <- nls( ~ weighted.MM.grad(rate, conc1, conc.1, Vm, K),
                   data = lisTreat, start = list(Vm = 200, K = 0.1))

rbind(coef(Pur.wt), coef(Pur.wt1), coef(Pur.wt.grad))

## In this example, there seems no advantage to providing the gradient.
## In other cases, there might be.
@

\subsection{Zero or small residual problems}

Zero residual problems give difficulty to \code{nls()} for reasons that
appear to be related to the choice of termination criteria. After all, they
are in some ways "perfect" problems. 

<<nlsex5, echo=TRUE, cache=TRUE>>=
## The two examples below show that you can fit a model to
## artificial data with noise but not to artificial data
## without noise.
x <- 1:10
y <- 2*x + 3                            # perfect fit
yeps <- y + rnorm(length(y), sd = 0.01) # added noise
test1<-try(nls(yeps ~ a + b*x, start = list(a = 0.12345, b = 0.54321)))
print(test1)

## terminates in an error, because convergence cannot be confirmed:
err1<-try(nls(y ~ a + b*x, start = list(a = 0.12345, b = 0.54321)))
test1port<-try(nls(y ~ a + b*x, start = list(a = 0.12345, b = 0.54321), algorithm="port"))
print(test1port)
test1plinear<-try(nls(y ~ a + b*x, start = list(a = 0.12345, b = 0.54321), algorithm="plinear"))
print(test1plinear)

## Try nlmrt routine nlxb()
mydf<-data.frame(x = x, y = y, yeps = yeps)
test2<-try(nlxb(y ~ a + b*x, start = list(a = 0.12345, b = 0.54321), data=mydf))
test2
test2eps<-try(nlxb(yeps ~ a + b*x, start = list(a = 0.12345, b = 0.54321), data=mydf))
test2eps
@


\section{A note on starting values}

The examples in the .Rd file for \code{nls()} suggests that the internal "guess"
in nls() can often work. I (JCN) have generally found that a Marquardt approach
is very robust even to quite extreme starts, but that Gauss-Newton ones are much
more temperamental. 

<<nlsstrt1, echo=TRUE, cache=TRUE>>=
## the nls() internal cheap guess for starting values can be sufficient:
x <- -(1:100)/10
y <- 100 + 10 * exp(x / 2) + rnorm(x)/10
nlmod <- nls(y ~  Const + A * exp(B * x))

plot(x,y, main = "nls(*), data, true function and fit, n=100")
curve(100 + 10 * exp(x / 2), col=4, add = TRUE)
lines(x, predict(nlmod), col=2)
@

\section{A more complicated model}


<<nlsmusc1, echo=TRUE, cache=TRUE>>=
## The muscle dataset in MASS is from an experiment on muscle
## contraction on 21 animals.  The observed variables are Strip
## (identifier of muscle), Conc (Cacl concentration) and Length
## (resulting length of muscle section).
utils::data(muscle, package = "MASS")

## The non linear model considered is
##       Length = alpha + beta*exp(-Conc/theta) + error
## where theta is constant but alpha and beta may vary with Strip.

with(muscle, table(Strip)) # 2,3 or 4 obs per strip

## We first use the plinear algorithm to fit an overall model,
## ignoring that alpha and beta might vary with Strip.

musc.1 <- nls(Length ~ cbind(1, exp(-Conc/th)), muscle,
              start = list(th=1), algorithm="plinear")
summary(musc.1)

## Then we use nls' indexing feature for parameters in non-linear
## models to use the conventional algorithm to fit a model in which
## alpha and beta vary with Strip.  The starting values are provided
## by the previously fitted model.
## Note that with indexed parameters, the starting values must be
## given in a list (with names):
b <- coef(musc.1)
musc.2 <- nls(Length ~ a[Strip] + b[Strip]*exp(-Conc/th),
              muscle,
              start = list(a=rep(b[2],21), b=rep(b[3],21), th=b[1]))
summary(musc.2)
@

\section{nls2 - Gabor Grothendieck}

The CRAN package \pkg{nls2} is intended to assist in finding solutions when
\code{nls()} has difficulties. It does this by offering multiple starts. 
As with \code{nlxb()} of \pkg{nlmrt} there are some minor differences in 
the syntax that may make it awkward to "just change the name", but overall
this is a useful tool. ?? need to put in the example from nls2 and try with
nlmrt??

\subsection{\code{nls2} examples}



<<nls2x0, echo=TRUE, cache=TRUE>>=
require(nls2)
y <- c(44,36,31,39,38,26,37,33,34,48,25,22,44,5,9,13,17,15,21,10,16,22,
13,20,9,15,14,21,23,23,32,29,20,26,31,4,20,25,24,32,23,33,34,23,28,30,10,29,
40,10,8,12,13,14,56,47,44,37,27,17,32,31,26,23,31,34,37,32,26,37,28,38,35,27,
34,35,32,27,22,23,13,28,13,22,45,33,46,37,21,28,38,21,18,21,18,24,18,23,22,
38,40,52,31,38,15,21)

x <- c(26.22,20.45,128.68,117.24,19.61,295.21,31.83,30.36,13.57,60.47,
205.30,40.21,7.99,1.18,5.40,13.37,4.51,36.61,7.56,10.30,7.29,9.54,6.93,12.60,
2.43,18.89,15.03,14.49,28.46,36.03,38.52,45.16,58.27,67.13,92.33,1.17,
29.52,84.38,87.57,109.08,72.28,66.15,142.27,76.41,105.76,73.47,1.71,305.75,
325.78,3.71,6.48,19.26,3.69,6.27,1689.67,95.23,13.47,8.60,96.00,436.97,
472.78,441.01,467.24,1169.11,1309.10,1905.16,135.92,438.25,526.68,88.88,31.43,
21.22,640.88,14.09,28.91,103.38,178.99,120.76,161.15,137.38,158.31,179.36,
214.36,187.05,140.92,258.42,85.86,47.70,44.09,18.04,127.84,1694.32,34.27,
75.19,54.39,79.88,63.84,82.24,88.23,202.66,148.93,641.76,20.45,145.31,
27.52,30.70)
@


<<nls2x1, echo=TRUE, cache=TRUE>>=
## Example 1
## brute force followed by nls optimization

fo <- y ~ Const + B * (x ^ A)

# pass our own set of starting values
# returning result of brute force search as nls object
st1 <- expand.grid(Const = seq(-100, 100, len = 4), 
	B = seq(-100, 100, len = 4), A = seq(-1, 1, len = 4))
mod1 <- nls2(fo, start = st1, algorithm = "brute-force")
mod1
# use nls object mod1 just calculated as starting value for 
# nls optimization.  Same as: nls(fo, start = coef(mod1))
nls2(fo, start = mod1) 

<<nls2x2, echo=TRUE, cache=TRUE>>=
## Example 2

# pass a 2-row data frame and let nls2 calculate grid
st2 <- data.frame(Const = c(-100, 100), B = c(-100, 100), A = c(-1, 1))
mod2 <- nls2(fo, start = st2, algorithm = "brute-force")
mod2
# use nls object mod1 just calculated as starting value for 
# nls optimization.  Same as: nls(fo, start = coef(mod2))
nls2(fo, start = mod2)

<<nls2x3, echo=TRUE, cache=TRUE>>=
## Example 3

# Create same starting values as in Example 2
# running an nls optimization from each one and picking best.
# This one does an nls optimization for every random point
# generated whereas Example 2 only does a single nls optimization
nls2(fo, start = st2, control = nls.control(warnOnly = TRUE))

<<nls2x4, echo=TRUE, cache=TRUE>>=
## Example 4

# Investigate singular gradient.
# Note that this cannot be done with nls since the singular gradient at
#  the initial conditions would stop it with an error.

DF1 <- data.frame(y=1:9, one=rep(1,9))
xx <- nls2(y~(a+2*b)*one, DF1, start = c(a=1, b=1), algorithm = "brute-force")
svd(xx$m$Rmat())[-2]

<<nls2x5, echo=TRUE, cache=TRUE>>=
## Example 5

# Use plinear algorithm to reduce a 4 parameter model to a model with 
# 2 linear and 2 nonlinear parameters

## Fixed spelling error in example that is "don't run"
## data(Ratkowsky, package = "NISTnls") # Ratkowsky2 data set
data(Ratkowsky2, package = "NISTnls") # Ratkowsky2 data set
# fo corresponds to the model on page 13 of Huet et al.
fo <- y ~ cbind(rep(1, 9), exp(- exp(p3+p4*log(x))))
st <- data.frame(p3 = c(-100,100), p4 = c(-100, 100))
## Fixed spelling error in example that is "don't run"
## rat.nls <- nls2(fo, Ratkwosky2, start = st,
##	control = nls.control(maxiter = 200), algorithm = "plinear")
rat.nls <- nls2(fo, Ratkowsky2, start = st,
	control = nls.control(maxiter = 200), algorithm = "plinear")
rat.nls
rat2.nls <- nls2(fo, Ratkowsky2, start = rat.nls, algorithm = "plinear")
rat2.nls
@


\subsection{\code{as.lm.nls}}

<<aslmnls01, echo=TRUE, cache=TRUE>>=
# data is from ?nls
DNase1 <- subset(DNase, Run == 1)
fm1DNase1 <- nls(density ~ SSlogis(log(conc), Asym, xmid, scal), DNase1)

# these give same result
vcov(fm1DNase1)
## NOTE: had to change as.lm to as.lm.nls 
vcov(as.lm.nls(fm1DNase1))

# nls confidence and prediction intervals based on asymptotic approximation
# are same as as.lm confidence intervals.
## NOTE: had to change as.lm to as.lm.nls 
predict(as.lm.nls(fm1DNase1), interval = "confidence")
## NOTE: had to change as.lm to as.lm.nls 
predict(as.lm.nls(fm1DNase1), interval = "prediction")
@

\section{nlmrt}

For package \pkg{nlmrt}, let us consider a slightly different problem, 
called WEEDS. Here the objective
is to model a set of 12 data points (density $y$ of weeds at annual time points $tt$)
versus the time index. (A minor note: use of \code{t} rather than \code{tt} in \R
may encourage confusion with the transpose function \code{t()}, so I tend to 
avoid plain \code{t}.) The model suggested was a 3-parameter logistic function,

$  y_{model}  =  b_1/(1 + b_2 exp(-b_3 tt) ) $

and while it is possible to use this formulation, a scaled version gives slightly
better results

$  y_{model} =  100  b_1/(1 + 10 b_2 exp(-0.1 b_3 tt) ) $


\subsection{Problems using a model formula -- \code{nlxb()}}

First, we will set up the problem to use a model formula. We also set up 
the data and a variety of starting vectors.

<<nlmrt00, echo=TRUE, cache=TRUE>>=
rm(list=ls())
library(nlmrt)
# traceval set TRUE to debug or give full history
traceval  <-  FALSE
# Data for Hobbs problem
ydat  <-  c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443, 
          38.558, 50.156, 62.948, 75.995, 91.972) # for testing
y  <-  ydat  # for testing
tdat  <-  seq_along(ydat) # for testing
# WARNING -- using T can get confusion with TRUE
tt  <-  tdat
eunsc  <-   y ~ b1/(1+b2*exp(-b3*tt))
escal  <-   y ~ 100*b1/(1+10*b2*exp(-0.1*b3*tt))
# set up data in data frames
weeddata1  <-  data.frame(y=ydat, tt=tdat)
weeddata2  <-  data.frame(y=1.5*ydat, tt=tdat)
# starting vectors -- must have named parameters for nlxb, nls, wrapnls.
start1  <-  c(b1=1, b2=1, b3=1)
startf1  <-  c(b1=1, b2=1, b3=.1)
suneasy  <-  c(b1=200, b2=50, b3=0.3)
ssceasy  <-  c(b1=2, b2=5, b3=3)
st1scal  <-  c(b1=100, b2=10, b3=0.1)
@

\pkg{nlmrt} is not intended to be used with global data i.e., data in the
environment in which the user is working. (??should this be changed??) So
the calls here should fail.

<<nlmrt01, echo=TRUE, cache=TRUE>>=
cat("GLOBAL DATA -- nls -- SHOULD WORK\n")
anls1g  <-  try(nls(eunsc, start=start1, trace=traceval))
print(anls1g)
cat("GLOBAL DATA -- nlxb -- SHOULD FAIL\n")
anlxb1g  <-  try(nlxb(eunsc, start=start1, trace=traceval))
print(anlxb1g)
rm(y)
rm(tt)
@

<<nlmrt02, echo=TRUE, cache=TRUE>>=
cat("LOCAL DATA IN DATA FRAMES\n")
anlxb1  <-  try(nlxb(eunsc, start=start1, trace=traceval, data=weeddata1))
print(anlxb1)
anlxb2  <-  try(nlxb(eunsc, start=start1, trace=traceval, data=weeddata2))
print(anlxb2)
@

<<nlmrt03, echo=TRUE, cache=TRUE>>=
## With BOUNDS
anlxb1  <-  try(nlxb(eunsc, start=startf1, lower=c(b1=0, b2=0, b3=0), 
      upper=c(b1=500, b2=100, b3=5), trace=traceval, data=weeddata1))
print(anlxb1)
# Check nls too
anlsb1  <-  try(nls(eunsc, start=start1, lower=c(b1=0, b2=0, b3=0), 
     upper=c(b1=500, b2=100, b3=5), trace=traceval, data=weeddata1, algorithm='port'))
print(anlsb1)
cat("Another case -- hard upper bound\n")
anlxb2  <-  try(nlxb(eunsc, start=start1, lower=c(b1=0, b2=0, b3=0), upper=c(b1=500, b2=100, b3=.25), trace=traceval, data=weeddata1))
print(anlxb2)
anlsb2  <-  try(nls(eunsc, start=start1, lower=c(b1=0, b2=0, b3=0), upper=c(b1=500, b2=100, b3=.25), trace=traceval, data=weeddata1, algorithm='port'))
print(anlsb2)
@


<<nlmrt04, echo=TRUE, cache=TRUE>>=
cat("TEST MASKS\n")
anlsmnqm  <-  try(nlxb(eunsc, start=start1, lower=c(b1=0, b2=0, b3=0), 
   upper=c(b1=500, b2=100, b3=5), masked=c("b2"), trace=traceval, data=weeddata1))
print(anlsmnqm)

cat("UNCONSTRAINED\n")
an1q  <-  try(nlxb(eunsc, start=start1, trace=traceval, data=weeddata1))
print(an1q)

cat("MASKED\n")
an1qm3  <-  try(nlxb(eunsc, start=start1, trace=traceval, data=weeddata1, masked=c("b3")))
print(an1qm3)

# Note that the parameters are put in out of order to test code.
an1qm123  <-  try(nlxb(eunsc, start=start1, trace=traceval, data=weeddata1, masked=c("b2","b1","b3")))
print(an1qm123)
@


<<nlmrt05, echo=TRUE, cache=TRUE>>=
cat("BOUNDS")
start2  <-  c(b1=100, b2=10, b3=0.1)
an1qb1  <-  try(nlxb(eunsc, start=start2, trace=traceval, data=weeddata1, lower=c(0,0,0), upper=c(200, 60, .3)))
print(an1qb1)

cat("BOUNDS and MASK")
an1qbm2  <-  try(nlxb(eunsc, start=start2, trace=traceval, data=weeddata1, lower=c(0,0,0), upper=c(200, 60, .3), masked=c("b2")))
print(an1qbm2)
@

<<nlmrt06, echo=TRUE, cache=TRUE>>=

cat("Try with scaled model\n")


cat("EASY start -- unscaled")
anls01  <-  try(nls(eunsc, start=suneasy, trace=traceval, data=weeddata1))
print(anls01)
anlmrt01  <-  try(nlxb(eunsc, start=ssceasy, trace=traceval, data=weeddata1))
print(anlmrt01)

cat("All 1s start -- unscaled")
anls02  <-  try(nls(eunsc, start=start1, trace=traceval, data=weeddata1))
if (class(anls02) == "try-error") {
   cat("FAILED:")
   print(anls02)
} else {
   print(anls02)
}
anlmrt02  <-  nlxb(eunsc, start=start1, trace=traceval, data=weeddata1)
print(anlmrt02)

cat("ones start -- scaled")
anls03  <-  try(nls(escal, start=start1, trace=traceval, data=weeddata1))
print(anls03)
anlmrt03  <-  nlxb(escal, start=start1, trace=traceval, data=weeddata1)
print(anlmrt03)

cat("HARD start -- scaled")
anls04  <-  try(nls(escal, start=st1scal, trace=traceval, data=weeddata1))
print(anls04)
anlmrt04  <-  nlxb(escal, start=st1scal, trace=traceval, data=weeddata1)
print(anlmrt04)

cat("EASY start -- scaled")
anls05  <-  try(nls(escal, start=ssceasy, trace=traceval, data=weeddata1))
print(anls05)
anlmrt05  <-  nlxb(escal, start=ssceasy, trace=traceval, data=weeddata1)
print(anlmrt03)
@

\subsection{Problems using an objective or residual function -- \code{nlfb()}}

The residuals for the scaled WEEDS problem (in form "model" minus "data") are coded in 
\R in the following code chunk in the function \code{shobbs.res}. We have also
coded the Jacobian for this model as \code{shobbs.jac}


<<nlmrtf00, echo=TRUE, cache=TRUE>>=
shobbs.res <- function(x){ # scaled Hobbs weeds problem -- residual
# This variant uses looping
    if(length(x) != 3) stop("hobbs.res -- parameter vector n!=3")
    y <- c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443, 38.558, 50.156, 62.948,
         75.995, 91.972)
    tt <- 1:12
    res <- 100.0*x[1]/(1+x[2]*10.*exp(-0.1*x[3]*tt)) - y
}
 
shobbs.jac <- function(x) { # scaled Hobbs weeds problem -- Jacobian
    jj <- matrix(0.0, 12, 3)
    tt <- 1:12
    yy <- exp(-0.1*x[3]*tt) # We don't need data for the Jacobian
    zz <- 100.0/(1+10.*x[2]*yy)
    jj[tt,1]  <-  zz
    jj[tt,2]  <-  -0.1*x[1]*zz*zz*yy
    jj[tt,3]  <-  0.01*x[1]*zz*zz*yy*x[2]*tt
    return(jj)
}
traceval <- FALSE
@

With package \code{nlmrt}, function \code{nlfb} can be used to estimate the 
parameters of the WEEDS problem as follows, where we use the naive starting 
point where all parameters are 1.

<<nlmrtf01, echo=TRUE, cache=TRUE>>=
st <- c(b1=1, b2=1, b3=1)
cat("try nlfb\n")
low  <-  -Inf
up <- Inf
ans1 <- nlfb(st, shobbs.res, shobbs.jac, trace=traceval)
print(ans1)
@

This works very well, with almost identical iterates as given by \code{nlxb}.
(Since the algorithms are the same, this should be the case.) Note that we 
turn off the \code{trace} output. There is also the possibility of interrupting
the iterations to \code{watch} the progress. Changing the value of \code{watch} in 
the call to \code{nlfb} below allows this. In this code chunk, we use an internal
numerical approximation to the Jacobian. 

<<nlmrtf01b, echo=TRUE, cache=TRUE>>=
cat("No jacobian function -- use internal approximation\n")
ans1n <- nlfb(st, shobbs.res, trace=FALSE, control=list(watch=FALSE)) # NO jacfn
print(ans1n)
@

Note that we could also form the sum of squares function and the gradient and
use a function minimization code. The next code block shows how this is done,
creating the sum of squares function and its gradient, then using the \code{optimx}
package to call a number of minimizers simultaneously.

<<nlmrtss01, echo=TRUE, cache=TRUE>>=
shobbs.f <- function(x){
   res <- shobbs.res(x)
   as.numeric(crossprod(res))
}
shobbs.g <- function(x){
   res <- shobbs.res(x) # This is NOT efficient -- we generally have res already calculated
   JJ <- shobbs.jac(x)
   2.0*as.vector(crossprod(JJ,res))
}
require(optimx)
aopx <- optimx(st, shobbs.f, shobbs.g, control=list(all.methods=TRUE))
optansout(aopx, NULL) # no file output
cat("\nNow with numerical gradient approximation or derivative free methods\n")
aopxn <- optimx(st, shobbs.f, control=list(all.methods=TRUE))
optansout(aopxn, NULL) # no file output
@

The functional variant of the Marquardt code also supports bounds and
masks. 

<<nlmrtf02, echo=TRUE, cache=TRUE>>=
cat("BOUNDS")
time2 <- system.time(ans2 <- nlfb(st, shobbs.res, shobbs.jac, upper=c(2,2,2), trace=traceval))
ans2
time2
st2s <- c(b1=1, b2=1, b3=1)

an1qb1 <- try(nlxb(escal, start=st2s, trace=traceval, data=weeddata1, 
  lower=c(0,0,0), upper=c(2, 6, 3), control=list(watch=FALSE)))
print(an1qb1)

ans2 <- nlfb(st2s,shobbs.res, shobbs.jac, lower=c(0,0,0), upper=c(2, 6, 3), 
   trace=traceval, control=list(watch=FALSE))
print(ans2)

cat("BUT ... nls() seems to do better from the TRACE information\n")
anlsb <- nls(escal, start=st2s, trace=traceval, data=weeddata1, lower=c(0,0,0),
     upper=c(2,6,3), algorithm='port')
cat("However, let us check the answer\n")
print(anlsb)
cat("HOWEVER...crossprod(resid(anlsb))=",crossprod(resid(anlsb)),"\n")
@

\subsection{Obtaining a result with \code{nls} structure}

The structure of the output of the function \code{nlfb} is NOT the same as
that of \code{nls()}. We can, however, obtain this easily if the solution 
found by \code{nlfb()} is stable under the Gauss-Newton method of \code{nls()}
which is generally the case. We simply use the \code{wrapnls()} function of
\pkg{nlmrt} that first uses \code{nlxb()} then calls \code{nls()} and returns
the answer from that function. 

<<nlmrtf03, echo=TRUE, cache=TRUE>>=
cat("Try wrapnls\n")
traceval <- FALSE
# Data for Hobbs problem
ydat <- c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443, 
          38.558, 50.156, 62.948, 75.995, 91.972) # for testing
tdat <- seq_along(ydat) # for testing
start1 <- c(b1=1, b2=1, b3=1)
escal <-  y ~ 100*b1/(1+10*b2*exp(-0.1*b3*tt))
up1 <- c(2,6,3)
up2 <- c(1, 5, 9)
## weeddata1 <- data.frame(y=ydat, tt=tdat)
an1w <- try(wrapnls(escal, start=start1, trace=traceval, data=weeddata1))
print(an1w)

cat("BOUNDED wrapnls\n")
an1wb <- try(wrapnls(escal, start=start1, trace=traceval, data=weeddata1, upper=up1))
print(an1wb)

cat("BOUNDED wrapnls\n")
an2wb <- try(wrapnls(escal, start=start1, trace=traceval, data=weeddata1, upper=up2))
print(an2wb)

cat("TRY MASKS ONLY\n")
an1xm3 <- try(nlxb(escal, start1, trace=traceval, data=weeddata1, masked=c("b3")))
print(an1xm3)
an1fm3 <- try(nlfb(start1, shobbs.res, shobbs.jac, trace=traceval, data=weeddata1, maskidx=c(3)))
print(an1fm3)

an1xm1 <- try(nlxb(escal, start1, trace=traceval, data=weeddata1, masked=c("b1")))
print(an1xm1)
an1fm1 <- try(nlfb(start1, shobbs.res, shobbs.jac, trace=traceval, data=weeddata1, maskidx=c(1)))
print(an1fm1)

# Need to check when all parameters masked.??
@

\subsection{Transforming a model formula to objective function form}

<<nlmrtk01, echo=TRUE, cache=TRUE>>=

cat("\n\n Now check conversion of expression to function\n\n")
cat("K Vandepoel function\n")

x <- c(1,3,5,7) # data
y <- c(37.98,11.68,3.65,3.93)
penetrationks28 <- data.frame(x=x,y=y)

cat("Try nls() -- note the try() function!\n")

fit0  <-  try(nls(y ~ (a+b*exp(1)^(-c * x)), data = penetrationks28, 
    start = c(a=0,b = 1,c=1), trace = TRUE))
print(fit0)

cat("\n\n")


fit1  <-  nlxb(y ~ (a+b*exp(-c*x)), data = penetrationks28, 
   start = c(a=0,b=1,c=1), trace = TRUE) 
print(fit1)

mexprn <- "y ~ (a+b*exp(-c*x))"
pvec <- c(a=0,b=1,c=1)
bnew <- c(a=10,b=3,c=4)

k.r <- model2resfun(mexprn , pvec)
k.j <- model2jacfun(mexprn , pvec)
k.f <- model2ssfun(mexprn , pvec)
k.g <- model2grfun(mexprn , pvec)


cat("At pvec:")
print(pvec)
rp <- k.r(pvec, x=x, y=y)
cat(" rp=")
print(rp)
rf <- k.f(pvec, x=x, y=y)
cat(" rf=")
print(rf)
rj <- k.j(pvec, x=x, y=y)
cat(" rj=")
print(rj)
rg <- k.g(pvec, x=x, y=y)
cat(" rg=")
print(rg)
cat("modss at pvec gives ")
print(modss(pvec, k.r, x=x, y=y))
cat("modgr at pvec gives ")
print(modgr(pvec, k.r, k.j, x=x, y=y))
cat("\n\n")



cat("At bnew:")
print(bnew)
rb <- k.r(bnew, x=x, y=y)
cat(" rb=")
print(rb)
rf <- k.f(bnew, x=x, y=y)
cat(" rf=")
print(rf)
rj <- k.j(bnew, x=x, y=y)
cat(" rj=")
print(rj)
rg <- k.g(bnew, x=x, y=y)
cat(" rg=")
print(rg)
cat("modss at bnew gives ")
print(modss(bnew, k.r, x=x, y=y))
cat("modgr at bnew gives ")
print(modgr(bnew, k.r, k.j, x=x, y=y))
cat("\n\n")
@

\subsection{nlmrt TODOS}

  weightings (data or function call?? -- try to match nls)

  print method(s)

  issue of character vs expression

  return a class??

  guessed starting values



\section{minpack.lm}

Package \pkg{minpack.lm} \cite{minpacklm12} provides for the minimization of nonlinear sums of
squares expressed in residual function form. It is an interfacing of \R to the
Fortran software called \B{minpack} \cite{more80}.


\subsection{Brief example of \code{minpack.lm}}

Recently Kate Mullen provided some capability for the package \pkg{minpack.lm} to
include bounds constraints. I am particularly happy that this effort is proceeding,
as there are significant differences in how \pkg{minpack.lm} and \pkg{nlmrt} are
built and implemented. They can be expected to have different performance 
characteristics on different problems. A lively dialogue between developers, and
the opportunity to compare and check results can only improve the tools.

The examples below are a very quick attempt to show how to run the Ratkowsky-Huet 
problem with \code{nls.lm} from \pkg{minpack.lm}.

<<chunk17, echo=TRUE, cache=TRUE>>=
require(minpack.lm)
anlslm <- nls.lm(ones, lower=rep(-1000,4), upper=rep(1000,4), jres, jjac, yield=pastured$yield, time=pastured$time)
cat("anlslm from ones\n")
print(strwrap(anlslm))
anlslmh <- nls.lm(huetstart, lower=rep(-1000,4), upper=rep(1000,4), jres, jjac, yield=pastured$yield, time=pastured$time)
cat("anlslmh from huetstart\n")
print(strwrap(anlslmh))
@

?? include minpack.lm failure example and explain why things go wrong

\subsection{Examples \code{nls.lm}}


<<nls.lm01, echo=TRUE, cache=TRUE>>=
###### example 1

## values over which to simulate data 
x <- seq(0,5,length=100)

## model based on a list of parameters 
getPred <- function(parS, xx) parS$a * exp(xx * parS$b) + parS$c 

## parameter values used to simulate data
pp <- list(a=9,b=-1, c=6) 

## simulated data, with noise  
simDNoisy <- getPred(pp,x) + rnorm(length(x),sd=.1)
 
## plot data
plot(x,simDNoisy, main="data")

## residual function 
residFun <- function(p, observed, xx) observed - getPred(p,xx)

## starting values for parameters  
parStart <- list(a=3,b=-.001, c=1)

## perform fit 
nls.out <- nls.lm(par=parStart, fn = residFun, observed = simDNoisy,
xx = x, control = nls.lm.control(nprint=1))

## plot model evaluated at final parameter estimates  
lines(x,getPred(as.list(coef(nls.out)), x), col=2, lwd=2)

## summary information on parameter estimates
summary(nls.out) 

###### example 2 

## function to simulate data 
f <- function(TT, tau, N0, a, f0) {
    expr <- expression(N0*exp(-TT/tau)*(1 + a*cos(f0*TT)))
    eval(expr)
}

## helper function for an analytical gradient 
j <- function(TT, tau, N0, a, f0) {
    expr <- expression(N0*exp(-TT/tau)*(1 + a*cos(f0*TT)))
    c(eval(D(expr, "tau")), eval(D(expr, "N0" )),
      eval(D(expr, "a"  )), eval(D(expr, "f0" )))
}

## values over which to simulate data 
TT <- seq(0, 8, length=501)

## parameter values underlying simulated data  
p <- c(tau = 2.2, N0 = 1000, a = 0.25, f0 = 8)

## get data 
Ndet <- do.call("f", c(list(TT = TT), as.list(p)))
## with noise
N <- Ndet +  rnorm(length(Ndet), mean=Ndet, sd=.01*max(Ndet))

## plot the data to fit
par(mfrow=c(2,1), mar = c(3,5,2,1))  
plot(TT, N, bg = "black", cex = 0.5, main="data")

## define a residual function 
fcn     <- function(p, TT, N, fcall, jcall)
    (N - do.call("fcall", c(list(TT = TT), as.list(p))))

## define analytical expression for the gradient 
fcn.jac <- function(p, TT, N, fcall, jcall) 
    -do.call("jcall", c(list(TT = TT), as.list(p)))

## starting values 
guess <- c(tau = 2.2, N0 = 1500, a = 0.25, f0 = 10)

## to use an analytical expression for the gradient found in fcn.jac
## uncomment jac = fcn.jac
out <- nls.lm(par = guess, fn = fcn, jac = fcn.jac,
              fcall = f, jcall = j,
              TT = TT, N = N, control = nls.lm.control(nprint=1))

## get the fitted values 
N1 <- do.call("f", c(list(TT = TT), out$par))   

## add a blue line representing the fitting values to the plot of data 
lines(TT, N1, col="blue", lwd=2)

## add a plot of the log residual sum of squares as it is made to
## decrease each iteration; note that the RSS at the starting parameter
## values is also stored
plot(1:(out$niter+1), log(out$rsstrace), type="b",
main="log residual sum of squares vs. iteration number",
xlab="iteration", ylab="log residual sum of squares", pch=21,bg=2) 

## get information regarding standard errors
summary(out) 
@

\subsection{Examples for \code{nlsLM}}

<<nlsLM11, echo=TRUE, cache=TRUE>>=
### Examples from 'nls' doc ###
DNase1 <- subset(DNase, Run == 1)
## using a selfStart model
fm1DNase1 <- nlsLM(density ~ SSlogis(log(conc), Asym, xmid, scal), DNase1)
## using logistic formula
fm2DNase1 <- nlsLM(density ~ Asym/(1 + exp((xmid - log(conc))/scal)),
                 data = DNase1, 
                 start = list(Asym = 3, xmid = 0, scal = 1))

## all generics are applicable
coef(fm1DNase1)
confint(fm1DNase1)
deviance(fm1DNase1)
df.residual(fm1DNase1)
fitted(fm1DNase1)
formula(fm1DNase1)
logLik(fm1DNase1)
predict(fm1DNase1)
print(fm1DNase1)
profile(fm1DNase1)
residuals(fm1DNase1)
summary(fm1DNase1)
update(fm1DNase1)
vcov(fm1DNase1)
weights(fm1DNase1)

## weighted nonlinear regression using 
## inverse squared variance of the response
## gives same results as original 'nls' function
Treated <- Puromycin[Puromycin$state == "treated", ]
var.Treated <- tapply(Treated$rate, Treated$conc, var)
var.Treated <- rep(var.Treated, each = 2)
Pur.wt1 <- nls(rate ~ (Vm * conc)/(K + conc), data = Treated, 
               start = list(Vm = 200, K = 0.1), weights = 1/var.Treated^2)
Pur.wt2 <- nlsLM(rate ~ (Vm * conc)/(K + conc), data = Treated, 
               start = list(Vm = 200, K = 0.1), weights = 1/var.Treated^2)
all.equal(coef(Pur.wt1), coef(Pur.wt2))

## 'nlsLM' can fit zero-noise data
## in contrast to 'nls'
x <- 1:10
y <- 2*x + 3

try(nls(y ~ a + b * x, start = list(a = 0.12345, b = 0.54321)))

nlsLM(y ~ a + b * x, start = list(a = 0.12345, b = 0.54321))

### Examples from 'nls.lm' doc
## values over which to simulate data 
x <- seq(0,5, length = 100)
## model based on a list of parameters 
getPred <- function(parS, xx) parS$a * exp(xx * parS$b) + parS$c 
## parameter values used to simulate data
pp <- list(a = 9,b = -1, c = 6) 
## simulated data with noise  
simDNoisy <- getPred(pp, x) + rnorm(length(x), sd = .1)
## make model
mod <- nlsLM(simDNoisy ~ a * exp(b * x) + c, 
             start = c(a = 3, b = -0.001, c = 1), 
             trace = TRUE)     
## plot data
plot(x, simDNoisy, main = "data")
## plot fitted values
lines(x, fitted(mod), col = 2, lwd = 2)

## create declining cosine
## with noise
TT <- seq(0, 8, length = 501)
tau <- 2.2
N0 <- 1000
a <- 0.25
f0 <- 8
Ndet <- N0 * exp(-TT/tau) * (1 + a * cos(f0 * TT))
N <- Ndet +  rnorm(length(Ndet), mean = Ndet, sd = .01 * max(Ndet))
## make model
mod <- nlsLM(N ~ N0 * exp(-TT/tau) * (1 + a * cos(f0 * TT)), 
             start = c(tau = 2.2, N0 = 1500, a = 0.25, f0 = 10), 
             trace = TRUE)  

## plot data
plot(TT, N, main = "data")
## plot fitted values
lines(TT, fitted(mod), col = 2, lwd = 2)
@

\section{nls2 - INRIA}

There are some other tools for \R that aim to solve nonlinear least 
squares problems. We have not yet been able to successfully use the INRA package 
\code{nls2}  \cite{Huet1996}. This 
is a quite complicated package and is not installable as a regular \R package 
using \code{install.packages()}. Note that there is a very different package 
by the same name on CRAN by Gabor Grothendieck. 

August 15, 2012 -- was not able to figure out the INSTALL script. 
?? Should suggest a debian and/or Ubuntu package if this is possible, or else some 
improvement of INSTALL for mere mortals. 

\section{nlstools}



\section{Self-starting models}

\R provides for so-called "self-starting models". ?? starting values.





\bibliography{nlpd}
\bibliographystyle{chicago}

\end{document}

