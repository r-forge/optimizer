\name{Rcgdesc}
\alias{Rcgdesc}
\encoding{UTF-8}
\title{An R wrapper to the C version of the nonlinear conjugate gradient
package CG_DESCENT by Hager and Zhang.
}
\usage{
   Rcgdesc(par, fn, gr, control =
                 ControlParams(), \dots)
}
\description{
  The purpose of \code{Rcgdesc} is to minimize
  an unconstrained function
  of many parameters by a nonlinear conjugate gradients method.
}
\arguments{
 \item{par}{A numeric vector of starting estimates.}
 \item{fn}{A function that returns the value of the objective at the
   supplied set of parameters \code{par} using auxiliary data in \dots.
   The first argument of \code{fn} must be \code{par}. }
 \item{gr}{A function that returns the gradient of the objective at the
   supplied set of parameters \code{par} using auxiliary data in \dots.
   The first argument of \code{fn} must be \code{par}. This function 
   returns the gradient as a numeric vector.

   If \code{gr} is not provided or is NULL, then the simple forward 
   gradient code \code{grfwd} from package \code{optextras} is used. However, 
   we recommend
   carefully coded and checked analytic derivatives for Rcgdesc.

   The use of numerical gradients for Rcgdesc is discouraged.
   First, the termination
   test uses a size measure on the gradient, and numerical gradient 
   approximations can sometimes give results that are too large. Second,
   if there are bounds constraints, the step(s) taken to calculate the
   approximation to the derivative are NOT checked to see if they are
   out of bounds, and the function may be undefined at the evaluation
   point. 

   There is also the option of using the routines \code{grfwd}, \code{grback}, 
   \code{grcentral} or \code{grnd} from package \code{optextras}. The last 
   of these calls the \code{grad()} function from package numDeriv. These 
   are called by putting the name of the (numerical) gradient function in 
   quotation marks, e.g.,

      gr="grfwd"

   to use the standard forward difference numerical approximation.

   Note that all but the \code{grnd} routine use a stepsize parameter that
   can be redefined in a special scratchpad storage variable \code{deps}.
   See package \code{optextras}. The default is \code{deps = 1e-07}. 
   However, redefining this is discouraged unless you understand what
   you are doing. 
}
 \item{control}{
    An optional list of control settings.  
 }
 \item{\dots}{Further arguments to be passed to \code{fn}.}
}
\details{
   Functions \code{fn} must return a numeric value.

   The \code{control} argument is a list. ?? Note that these will need 
      correction when package is further advanced.
   \describe{
   \item{maxit}{A limit on the number of iterations (default 500). Note that this is 
      used to compute a quantity \code{maxfeval}<-round(sqrt(n+1)*maxit) where n is the
      number of parameters to be minimized.}
   \item{trace}{Set 0 (default) for no output, >0 for trace output
      (larger values imply more output).}
   \item{eps}{Tolerance used to calculate numerical gradients. Default is 1.0E-7.
    See source code for \code{Rcgdesc} for details of application.}
   \item{\code{dowarn}}{= TRUE if we want warnings generated by optimx. Default is 
     TRUE.}
   \item{\code{tol}}{Tolerance used in testing the size of the square of the gradient.
     Default is 0 on input, which uses a value of tolgr = n*n*.Machine$double.eps,
     where n is the number of parameters,
     in testing if crossprod(g) <= tolgr * (abs(fmin) + offset). If the user supplies
     a value for \code{tol} that is non-zero, then that value is used for tolgr.

     offset=100 is only alterable by changing the code. fmin is the current best
     value found for the function minimum value. 

     Note that the scale of the gradient means that tests for a small gradient can
     easily be mismatched to a given problem. The defaults in Rcgdesc are a "best 
     guess".}
   \item{\code{checkgrad}}{= TRUE if we want gradient function checked against 
	numerical approximations. Default is FALSE.}
   \item{\code{checkbounds}}{= TRUE if we want bounds verified. Default is 
     TRUE.}
   \item{}{The source code \code{Rcgdesc} for R is likely to remain a work in progress 
    for some time, so users should watch the console output.}
  }

}
\value{
  A list with components:
  \item{par}{The best set of parameters found.}
  \item{value}{The value of the objective at the best set of parameters found.}
  \item{counts}{A two-element integer vector giving the number of calls to
          'fn' and 'gr' respectively. This excludes those calls needed
          to compute the Hessian, if requested, and any calls to 'fn'
          to compute a finite-difference approximation to the gradient.}
  \item{convergence}{An integer code. 
	 '0' indicates successful convergence.
         '1' indicates that the function evaluation count 'maxfeval' was reached.
         '2' indicates initial point is infeasible.}
  \item{message}{A character string giving any additional information returned
          by the optimizer, or 'NULL'.}
}
\references{

       Hager W. W. and H. Zhang, A new conjugate gradient method
       with guaranteed descent and an efficient line search,
       SIAM Journal on Optimization, 16 (2005), 170-192.

       Hager W. W. and H. Zhang, Algorithm 851: CG_DESCENT,
       A conjugate gradient method with guaranteed descent,
       ACM Transactions on Mathematical Software, 32 (2006), 113-137.

       Hager W. W. and H. Zhang, A survey of nonlinear conjugate
       gradient methods, Pacific Journal of Optimization,
       2 (2006), pp. 35-58.

       Hager W. W. and H. Zhang, Limited memory conjugate gradients,
       www.math.ufl.edu/~hager/papers/CG/lcg.pdf

       Nash J. C. (1979). Compact Numerical Methods for Computers: Linear 
       Algebra and Function Minimisation. Adam Hilger, Bristol. Second 
       Edition, 1990, Bristol: Institute of Physics Publications.

       Nash, J. C. and M. Walker-Smith (1987). Nonlinear Parameter 
       Estimation: An Integrated System in BASIC. New York: Marcel Dekker. 
       See http://www.nashinfo.com/nlpe.htm for a downloadable version 
       of this plus some extras.

}
\seealso{\code{\link{optim}}}
\examples{
#####################
require(numDeriv)
require(Rcgdescent)
## Rosenbrock Banana function
fr <- function(x) {
    x1 <- x[1]
    x2 <- x[2]
    100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}

grr <- function(x) { ## Gradient of 'fr'
    x1 <- x[1]
    x2 <- x[2]
    c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
       200 *      (x2 - x1 * x1))
}

grn<-function(x){
    gg<-grad(fr, x)
}  

# myctrl <- ControlParams()
ansrosenbrock0 <- Rcgdesc(par=c(-1.2,1), fn=fr, gr=grr)
print(ansrosenbrock0) # use print to allow copy to separate file that 
#    can be called using source()
#####################
genrose.f<- function(x, gs=NULL){ # objective function
## One generalization of the Rosenbrock banana valley function (n parameters)
	n <- length(x)
        if(is.null(gs)) { gs=100.0 }
	fval<-1.0 + sum (gs*(x[1:(n-1)]^2 - x[2:n])^2 + (x[2:n] - 1)^2)
        return(fval)
}
genrose.g <- function(x, gs=NULL){
# vectorized gradient for genrose.f
# Ravi Varadhan 2009-04-03
	n <- length(x)
        if(is.null(gs)) { gs=100.0 }
	gg <- as.vector(rep(0, n))
	tn <- 2:n
	tn1 <- tn - 1
	z1 <- x[tn] - x[tn1]^2
	z2 <- 1 - x[tn]
	gg[tn] <- 2 * (gs * z1 - z2)
	gg[tn1] <- gg[tn1] - 4 * gs * x[tn1] * z1
	gg
}

# analytic gradient test
xx<-rep(pi,10)
lower<-NULL
upper<-NULL
bdmsk<-NULL
genrosea<-Rcgdesc(par=xx, fn=genrose.f, gr=genrose.g, gs=10)
# The following will FAIL -- we MUST provide a gradient
# genrosenn1<-Rcgdesc(xx,genrose.f, gs=10) # use local numerical gradient
# genrosenn<-Rcgdesc(xx,genrose.f, "grfwd", gs=10) # use forward approximation gradient
cat("genrosea uses analytic gradient\n")
print(genrosea)
# cat("genrosenn uses default gradient approximation\n")
#print(genrosenn)

######### One dimension test
nn<-1

sqtst<-function(xx) {
   res<-sum((xx-2)*(xx-2))
}

gsqtst<-function(xx) {
    gg<-2*(xx-2)
}


startx<-rep(0,nn)
onepar<-Rcgdesc(par=startx, fn=sqtst,  gr=gsqtst) 
print(onepar)

# cat("Suppress warnings\n")
# oneparnw<-Rcgdesc(startx,sqtst,  gr=gsqtst, control=list(dowarn=FALSE,trace=1)) 
# print(oneparnw)

}

\keyword{nonlinear}
\keyword{optimize}

